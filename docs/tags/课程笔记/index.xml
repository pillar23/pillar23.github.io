<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>课程笔记 on π1l4r_のblog</title>
        <link>https://blog2.pillar.fun/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/</link>
        <description>Recent content in 课程笔记 on π1l4r_のblog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>pill4r</copyright>
        <lastBuildDate>Wed, 27 Dec 2023 19:07:10 +0800</lastBuildDate><atom:link href="https://blog2.pillar.fun/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>网络认证技术笔记</title>
        <link>https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 27 Dec 2023 19:07:10 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/img/placeholder.jpeg" alt="Featured image of post 网络认证技术笔记" /&gt;&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;说是开课以来从未有过挂科选手，但是想得不错的分数还是要努努力，进自己脑子的知识才是最好的知识&lt;/p&gt;
&lt;h1 id=&#34;笔记&#34;&gt;笔记&lt;/h1&gt;
&lt;p&gt;网络认证技术 ≈ 密码学+计算机网络&lt;/p&gt;
&lt;p&gt;网络认证：在信息系统/网络环境中，实现身份的确认。目标：在不可信的网络环境中确认主体是谁，有什么属性、权限、能力&lt;/p&gt;
&lt;p&gt;身份确认的主体：人、设备、软件服务……&lt;/p&gt;
&lt;p&gt;PKI：公钥基础设施&lt;/p&gt;
&lt;p&gt;CA：认证中心，生成数字证书&lt;/p&gt;
&lt;p&gt;CA是PKI的核心组成成分，但是在很多地方把CA和PKI混用了。&lt;/p&gt;
&lt;h1 id=&#34;考试用&#34;&gt;考试用&lt;/h1&gt;
&lt;p&gt;两个半小时&lt;/p&gt;
&lt;p&gt;题型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 2分&lt;/li&gt;
&lt;li&gt;简答 5-8分
&lt;ol&gt;
&lt;li&gt;建议不要空这&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;关键是原理之类的要记住的&lt;/p&gt;
&lt;h2 id=&#34;01-导言意义不大&#34;&gt;01 导言（意义不大）&lt;/h2&gt;
&lt;p&gt;相关概念&lt;/p&gt;
&lt;h2 id=&#34;02-密码学基础会涉及题目需要复习复习&#34;&gt;02 密码学基础（会涉及题目，需要复习复习）&lt;/h2&gt;
&lt;p&gt;对称：加解密&lt;/p&gt;
&lt;p&gt;非对称：签名&lt;/p&gt;
&lt;p&gt;光看密钥长度不能知道强度，RSA1024bits=ECC160bits。短密钥可以达到高强度&lt;/p&gt;
&lt;p&gt;哈希：验证&lt;/p&gt;
&lt;p&gt;消息鉴别码：MAC=C(K,M)，K为密钥M为消息，把密钥跟着一块哈希了&lt;/p&gt;
&lt;p&gt;可鉴别加密CCM、GCM、AEAD（简单了解）&lt;/p&gt;
&lt;p&gt;国外的密码基本原理不细说了&lt;/p&gt;
&lt;p&gt;国产的了解一下&lt;/p&gt;
&lt;p&gt;SM2 非对称 ECC &amp;mdash; 椭圆曲线 知道基于椭圆曲线域上的离散对数困难问题。 替换RSA&lt;/p&gt;
&lt;p&gt;SM3 哈希 256bit和sha256差不多 分组长度512bit，摘要值长度256bit&lt;/p&gt;
&lt;p&gt;SM4 分组工作模式&lt;/p&gt;
&lt;p&gt;ECB：对每个块独立加密：明文同样的块会加密成同样的密文&lt;/p&gt;
&lt;p&gt;CBC：明文先与上一个密文异或在加密，需要初始化向量&lt;/p&gt;
&lt;p&gt;OFB：将块密码转为流密码，生成密钥流的块&lt;/p&gt;
&lt;p&gt;CTR（ICM、SIC）：将块密码变为流密码，通过递增加密计数器产生密钥流&lt;/p&gt;
&lt;p&gt;ZUC 流密码&lt;/p&gt;
&lt;p&gt;128位的初始密钥key和128位的初始向量iv来作为输入。每个时钟周期能生成32bit&lt;/p&gt;
&lt;p&gt;共享密钥问题-&amp;gt;为什么要有非对称的原因-&amp;gt;数字签名&lt;/p&gt;
&lt;h2 id=&#34;03-口令鉴别&#34;&gt;03 口令鉴别&lt;/h2&gt;
&lt;p&gt;client 用复杂口令，不要告诉别人，次数限制&lt;/p&gt;
&lt;p&gt;传输 使用已被验证的安全信道&lt;/p&gt;
&lt;p&gt;server 存储，验证&lt;/p&gt;
&lt;h2 id=&#34;04-基于密码技术的鉴别&#34;&gt;04 基于密码技术的鉴别&lt;/h2&gt;
&lt;p&gt;两大类：&lt;/p&gt;
&lt;p&gt;对称： 有没有密钥&lt;/p&gt;
&lt;p&gt;提一个协议框架，让你看有没有什么错，一些参数有什么用【用什么方式可以抵抗什么攻击】&lt;/p&gt;
&lt;p&gt;replay attack（重放攻击）：通过加一个nonce抵抗&lt;/p&gt;
&lt;p&gt;oracle session attack（就是攻击者使另一方帮自己来计算）：让u和v不同。比如u为加密，v为解密，被挑战方只能加密，就不能被当成解密服务器了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705143946346.png&#34;
	width=&#34;498&#34;
	height=&#34;316&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705143946346_hu5f537607399d9aa9ddbc596a569fcea5_6625_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705143946346_hu5f537607399d9aa9ddbc596a569fcea5_6625_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705143946346&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Parallel Session attack：p(), q()与方向有关。从而攻击者不能利用服务器的计算。比如发起者会加一个xor，被挑战者会加一个左移&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144131279.png&#34;
	width=&#34;466&#34;
	height=&#34;301&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144131279_hue97dd130eeb6e4ef037f71fe1a48cdf0_6737_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144131279_hue97dd130eeb6e4ef037f71fe1a48cdf0_6737_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144131279&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;371px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;offset attack：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144243068.png&#34;
	width=&#34;1240&#34;
	height=&#34;995&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144243068_huf359cceab508d617bf281d2845489199_163567_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144243068_huf359cceab508d617bf281d2845489199_163567_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144243068&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;299px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;把返回的东西改为E(f()#E(g))&lt;/p&gt;
&lt;p&gt;可信第三方，kobras&lt;/p&gt;
&lt;p&gt;非对称：数字签名和验证&lt;/p&gt;
&lt;p&gt;单向（带一个时间戳之类的约定好的东西）、双向（A发给B后B还要发给A）&lt;/p&gt;
&lt;p&gt;PPT标红好好看看&lt;/p&gt;
&lt;h2 id=&#34;0506-pki技术&#34;&gt;05+06 PKI技术&lt;/h2&gt;
&lt;p&gt;CA：认证机构，权威第三方，公钥（证书）可信发布[根CA、子CA]&lt;/p&gt;
&lt;p&gt;RA：注册机构，审查信息，防止CA职能太多导致一个出问题导出都出问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144615935.png&#34;
	width=&#34;586&#34;
	height=&#34;391&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144615935_hu7fb4400745004ff61f9c7507c6cbbc25_105984_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144615935_hu7fb4400745004ff61f9c7507c6cbbc25_105984_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144615935&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;repository（存数据的吧）&lt;/p&gt;
&lt;p&gt;CRL（Certificate Revocation List，证书撤销列表）&lt;/p&gt;
&lt;p&gt;Online Certificate Status Protocol（OCSP）一种通信协议，专门用于检查证书是否已经被撤销 相应的服务器称为OCSP Server-&amp;gt;（证书有三种状态）Good、Revoked、Unknown ：未撤销、已经撤销、未知&lt;/p&gt;
&lt;p&gt;ASN.1-基本数据类型-DER编码-sequence-implicit/explicit tag  稍微看一下&lt;/p&gt;
&lt;h2 id=&#34;07-证书拓展&#34;&gt;07 证书拓展&lt;/h2&gt;
&lt;p&gt;证书基本域&lt;/p&gt;
&lt;p&gt;证书扩展域 X.509版本3 18种，了解功能即可&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1704713547641.png&#34;
	width=&#34;1121&#34;
	height=&#34;619&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1704713547641_hu8752b02efbe366c5151fb9273f61cd53_41598_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1704713547641_hu8752b02efbe366c5151fb9273f61cd53_41598_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704713547641&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;434px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;拓展有关键和非关键，如果关键出了错（识别不出来），直接认定证书非法。非关键出错则忽略拓展&lt;/p&gt;
&lt;p&gt;Basic Constraints：区分是否是CA证书（能否签发其他证书）以及路径的深度（说明CA可以有多少层次的下级）&lt;/p&gt;
&lt;p&gt;Authority Key Identifier：证书链中可能有多个公钥，这个确定哪个是用来验证证书的颁发者（CA）的公钥&lt;/p&gt;
&lt;p&gt;Subject Key Identifier：证书链中可能有多个公钥，确定哪个是证书自己的公钥&lt;/p&gt;
&lt;p&gt;Key Usage：密钥的用途。7种+2种辅助用途&lt;/p&gt;
&lt;p&gt;Private Key Usage Period：给出证书有效的开始到结束的时间&lt;/p&gt;
&lt;p&gt;Issuer Alternative Name：放置签发者（CA）的消息（DN存放CA信息，子CN没法用DN，就用这来放）&lt;/p&gt;
&lt;p&gt;Subject Alternative Name：放置证书拥有者的消息&lt;/p&gt;
&lt;p&gt;Subject Directory Attributes：可加入任何与Subject有关的信息，例如，民族、生日等&lt;/p&gt;
&lt;p&gt;Name Constraints：限制下级CA所能够签发证书的订户的名字空间（只在下级CA中有用）&lt;/p&gt;
&lt;p&gt;Certificate Policies（CP）：区分不同证书的安全等级&lt;/p&gt;
&lt;p&gt;Inhibit Any-Policy：（CP的Any-Policy指对于该CA所签发的订户证书的CP没有限制），值是整数N，表示：在证书路径中，本证书之下的N个证书可带有Any-Policy的证书&lt;/p&gt;
&lt;p&gt;Policy Mappings：说明了不同CA域之间的CP等级的相互映射关系&lt;/p&gt;
&lt;p&gt;Policy Constraints：对于证书认证路径的策略映射过程中，有关CP的处理，进行限制。N：在N个证书后，不允许再进行策略映射；M：在M个证书后，就必须要有认识的、明确的CP&lt;/p&gt;
&lt;p&gt;Extended Key Usage：证书/密钥可用的用途（拓展）&lt;/p&gt;
&lt;p&gt;CRL Distribution Points：和应用系统约定在哪儿获取CRL&lt;/p&gt;
&lt;p&gt;Freshest CRL：增量CRL情况下，获取最新的增量CRL的地址&lt;/p&gt;
&lt;p&gt;Authority Information Access：如何在Internet上面，访问一些CA的信息（目前只有 1、上级CA的情况 2、OCSP服务器的情况两个信息）&lt;/p&gt;
&lt;p&gt;Subject Information Access： l如何在Internet上面，访问一些用户的信息 （目前只有 1、资料库的地址（针对CA）2、TSA服务地址（针对TSA服务器））&lt;/p&gt;
&lt;h2 id=&#34;08-pki信任体系&#34;&gt;08 PKI信任体系&lt;/h2&gt;
&lt;p&gt;信任模型&lt;/p&gt;
&lt;p&gt;单根CA&lt;/p&gt;
&lt;p&gt;多根CA 根之间要互相通信-&lt;strong&gt;CTL&lt;/strong&gt;（用户自主+权威发布）&lt;em&gt;&lt;strong&gt;沟通方式、原理、优缺点，应用&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CTL（信任锚）由权威机构统一地发布1个可信的信任锚列表（Certificate Trust List）包括多个根CA证书文件的HASH结果和受信任CA对其签名&lt;/p&gt;
&lt;p&gt;【信任锚里有根CA证书的hash、其他CA证书、CRL、信任策略和规则等。然后由一个我信任的CA对CTL签名，一般不用CTL里信任的CA来签名。】&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705156671337.png&#34;
	width=&#34;1011&#34;
	height=&#34;569&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705156671337_huf204a7db48b14019f3e7ff701993fade_104698_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705156671337_huf204a7db48b14019f3e7ff701993fade_104698_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705156671337&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;方式：1、不同PKI域用同一个CTL 2、加一个ACA的信任锚说明哪些根CA是可以信任的&lt;/p&gt;
&lt;p&gt;交叉认证-网状mesh-&amp;gt;桥CA&lt;/p&gt;
&lt;p&gt;相当于将对方CA认作是我的子CA。&lt;/p&gt;
&lt;p&gt;mesh-&amp;gt;信任链变成信任网&lt;/p&gt;
&lt;p&gt;桥CA-&amp;gt;不同域之间的证书传递&lt;/p&gt;
&lt;h2 id=&#34;09-证书撤销&#34;&gt;09 证书撤销&lt;/h2&gt;
&lt;p&gt;验证签名-验证有效期-验证撤销状态&lt;/p&gt;
&lt;p&gt;撤销状态 CRL、OCSP、CRT 原理&lt;/p&gt;
&lt;p&gt;CA/CRL Issuer定期地签发CRL CRL，certificate revocation list&lt;/p&gt;
&lt;p&gt;完全CRL－Complete CRL：所有CRL信息一次发布&lt;/p&gt;
&lt;p&gt;增量CRL－Delta CRL：发布新增的CRL信息发布&lt;/p&gt;
&lt;p&gt;直接CRL－Direct CRL：证书签发者签发CRL&lt;/p&gt;
&lt;p&gt;间接CRL－Indirect CRL：使用CRL issuer签发CRL&lt;/p&gt;
&lt;p&gt;OCSP在线证书状态协议 Online Certificate Status Protocol 在线服务器&lt;/p&gt;
&lt;p&gt;CRT：证书撤销树，对于各证书序列号进行一定的结构化，形成了HASH链&lt;/p&gt;
&lt;p&gt;使用了merkle hash tree【区块链信任算法】&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705164608101.png&#34;
	width=&#34;531&#34;
	height=&#34;419&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705164608101_hu094195101f352d3e8b0136c8fe47aa93_54646_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705164608101_hu094195101f352d3e8b0136c8fe47aa93_54646_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705164608101&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;304px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;拿加粗的子哈希算哈希，就可以推出根hash，验证起来需要更少的那啥&lt;/p&gt;
&lt;h2 id=&#34;10-tls&#34;&gt;10 TLS&lt;/h2&gt;
&lt;p&gt;handshake怎么shake的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705163142817.png&#34;
	width=&#34;1215&#34;
	height=&#34;589&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705163142817_hudee5f9630e8ff38e091e3cd37159fc9d_96886_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705163142817_hudee5f9630e8ff38e091e3cd37159fc9d_96886_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705163142817&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;495px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;增加一个server对client的鉴别&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208493971.png&#34;
	width=&#34;1195&#34;
	height=&#34;709&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208493971_hu21ef76a372807c1d35250971395516f5_104014_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208493971_hu21ef76a372807c1d35250971395516f5_104014_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705208493971&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果server证书只能签名不能加密，则要生成一个临时公钥，签名后发给client【ServerKeyExchange】&lt;/p&gt;
&lt;p&gt;两张图里的消息有什么含义 1.3和1.2的区别&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208812117.png&#34;
	width=&#34;975&#34;
	height=&#34;391&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208812117_hufd9a71f0ad1209c616c683d2ac826862_31322_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208812117_hufd9a71f0ad1209c616c683d2ac826862_31322_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705208812117&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;598px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;直接在client hello中发了选择算法的key（用server 公钥加密）&lt;/p&gt;
&lt;h2 id=&#34;105wifi认证&#34;&gt;10.5wifi认证&lt;/h2&gt;
&lt;p&gt;WPA-PSK共享口令 （路由器上做）&lt;/p&gt;
&lt;p&gt;WPA-802.1X 基于账号的身份鉴别 （身份鉴别server）&lt;/p&gt;
&lt;p&gt;客户端或网页 （微信、短信）&lt;/p&gt;
&lt;h2 id=&#34;11-不考&#34;&gt;11 不考&lt;/h2&gt;
&lt;h2 id=&#34;12-pki安全增强&#34;&gt;12 PKI安全增强&lt;/h2&gt;
&lt;h3 id=&#34;入侵容忍-解决了什么问题怎么解决的-原理&#34;&gt;入侵容忍 解决了什么问题？怎么解决的 原理&lt;/h3&gt;
&lt;p&gt;解决了在入侵场景下的高可用。黑客侵入了其中一个PKI节点无法获利，同时PKI系统任然保持可用性&lt;/p&gt;
&lt;p&gt;【门限密码学：把密钥分成L份，当有其中f+1份时可以解密，否则解密不了】&lt;/p&gt;
&lt;p&gt;eg. Shamir&amp;rsquo;s Secret Sharing 基于拉格朗日插值法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065190379.png&#34;
	width=&#34;1071&#34;
	height=&#34;392&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065190379_hua39e73ea9fc336d14ec1faec1969a0ab_52151_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065190379_hua39e73ea9fc336d14ec1faec1969a0ab_52151_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065190379&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;eg2. ITTC 基于离散对数的子密钥分配&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065815902.png&#34;
	width=&#34;1315&#34;
	height=&#34;752&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065815902_hu51c6a1dee3f64d5d03ccef3af807317c_69505_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065815902_hu51c6a1dee3f64d5d03ccef3af807317c_69505_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065815902&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065831688.png&#34;
	width=&#34;701&#34;
	height=&#34;180&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065831688_hu768a57c0d10172b668620d96e435a889_6011_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065831688_hu768a57c0d10172b668620d96e435a889_6011_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065831688&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;389&#34;
		data-flex-basis=&#34;934px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;server上用密码算，CA来整合&lt;/p&gt;
&lt;p&gt;也就是说黑客就算攻入了一个节点，他仍然无法获取PKI用来签名证书的私钥，同时其他节点还能继续工作。&lt;/p&gt;
&lt;h3 id=&#34;信任增强-解决方式原理&#34;&gt;信任增强 解决方式原理&lt;/h3&gt;
&lt;p&gt;信任机制基本假设：1、CA行为不会出错，证书中的信息不会出错【只有可能是错误操作导致的签发给错误的人】 2、无限制权利&lt;/p&gt;
&lt;p&gt;三个思路：&lt;/p&gt;
&lt;p&gt;1、 浏览器端实施检测：&lt;/p&gt;
&lt;p&gt;（1）浏览器维护证书信息&lt;/p&gt;
&lt;p&gt;（2）多个会话之间互相比较&lt;/p&gt;
&lt;p&gt;2、限制CA权利&lt;/p&gt;
&lt;p&gt;（1）假定server只会向同一个国家的CA申请证书&lt;/p&gt;
&lt;p&gt;（2）限定CA能签发的顶级域名范围&lt;/p&gt;
&lt;p&gt;（3）域名拥有者可以控制哪个CA给他签发证书&lt;/p&gt;
&lt;p&gt;（4）server再次确认机制：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705067041133.png&#34;
	width=&#34;780&#34;
	height=&#34;480&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705067041133_hu74fbd29ad53f14f4c356bac7c9b4c016_48502_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705067041133_hu74fbd29ad53f14f4c356bac7c9b4c016_48502_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705067041133&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;server在多一个sovereign Key的公私钥对挂在timeline上，浏览器看到timeline上有sovereign Key，会要求server再次拿sovereign Key私钥签名，黑客控制了CA，却无法获取server的sovereign Key私钥，因此仍然无法伪造身份&lt;/p&gt;
&lt;p&gt;3、证书透明化：&lt;/p&gt;
&lt;p&gt;假定CA也会出错，审计CA&lt;/p&gt;
&lt;h2 id=&#34;13-证书透明化&#34;&gt;13 证书透明化&lt;/h2&gt;
&lt;p&gt;虚假证书：证书可以被严格验证通过，但是证书对应的私钥并不被证书主体拥有，而是被其他人拥有（CA被人黑了，一顿乱发）&lt;/p&gt;
&lt;p&gt;透明化增加哪些步骤SCT相关特点弄清楚一点&lt;/p&gt;
&lt;p&gt;增加&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;公开日志服务器（Public Log Server）：保存和维护记录证书的公开日志（Public Log）&lt;/p&gt;
&lt;p&gt;收到证书并验证通过后，公开日志服务器会向提交者返回一个凭据（SCT）Signed Certificate Timestamp。（有可能多个公开日志服务器，就会返回多个SCT）用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705068113028.png&#34;
	width=&#34;1456&#34;
	height=&#34;630&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705068113028_hu14e7dd89cac93ed80bceb5ca58f540bd_164693_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705068113028_hu14e7dd89cac93ed80bceb5ca58f540bd_164693_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705068113028&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;231&#34;
		data-flex-basis=&#34;554px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）

怎么获得SCT呢？

1.从X.509证书扩展项获得SCT

2.从连接建立时TLS扩展项获得SCT -&amp;gt;TLS客户端要支持

3.从OCSP stapling的扩展项获得SCT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面不重要&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;监视员（Monitor）：周期性的访问公开日志服务器，寻找和发现可疑的证书&lt;/li&gt;
&lt;li&gt;审计员（Auditor）：审计公开日志的行为&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;14-隐式证书&#34;&gt;14 隐式证书&lt;/h2&gt;
&lt;p&gt;传统和隐式的结构和使用的区别&lt;/p&gt;
&lt;p&gt;在带宽、计算能力、存储资源有限制的环境下，隐式证书是传统X.509证书的一种高效替代&lt;/p&gt;
&lt;p&gt;X.509证书基本内容：订户身份信息+公钥数据+CA数字签名&lt;/p&gt;
&lt;p&gt;隐式证书基本内容：中间公钥数据$P_U$  + 订户身份标识。 最终公钥 P=$P_{CA}$+$P_U$以及身份信息也有关  $P_{CA}$：CA证书公钥&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705069930310.png&#34;
	width=&#34;1030&#34;
	height=&#34;827&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705069930310_hucbbedfed951625a980cf4babac2156b1_135222_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705069930310_hucbbedfed951625a980cf4babac2156b1_135222_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705069930310&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;298px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070190100.png&#34;
	width=&#34;1092&#34;
	height=&#34;391&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070190100_hu35f1d41185397330c55c04a477f161e3_56406_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070190100_hu35f1d41185397330c55c04a477f161e3_56406_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705070190100&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;279&#34;
		data-flex-basis=&#34;670px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070203597.png&#34;
	width=&#34;1049&#34;
	height=&#34;351&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070203597_hu978f7796b10bc659af20d259d50c5cd2_43230_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070203597_hu978f7796b10bc659af20d259d50c5cd2_43230_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705070203597&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;298&#34;
		data-flex-basis=&#34;717px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;X.509： 需要对订户证书进行CA签名的验证&lt;/p&gt;
&lt;p&gt;隐式证书：需要重构出订户公钥，在对消息的验签时同时完成对证书本身的验证&lt;/p&gt;
&lt;p&gt;隐式证书中，没有对CA数字签名的验证，取而代之的是，重构公钥的计算，后者的计算量较小。&lt;/p&gt;
&lt;p&gt;假名证书不考&lt;/p&gt;
&lt;h2 id=&#34;15-kerberos&#34;&gt;15 kerberos&lt;/h2&gt;
&lt;p&gt;可信第三方TTP，基于对称密码，也支持在某些过程使用非对称&lt;/p&gt;
&lt;p&gt;获得一个TGT，用TGT和要访问的目，请求问kerberos服务器，来获取访问目标的票据（不是TGT，TGT只是告诉kerberos我已经被验证过了）&lt;/p&gt;
&lt;p&gt;kerberos票据流程&lt;/p&gt;
&lt;p&gt;长期密钥（主密钥）Long-term Key/Master Key： 长期保持不变的密钥。被长期密钥（主密钥）加密的数据尽量不在网络上传输。（防止暴力破解、分析）&lt;/p&gt;
&lt;p&gt;短期密钥（会话密钥）Short-term Key/Session Key： 加密需要进行网络传输的数据。只在一段时间内有效，即使被加密的数据包被黑客截获并破解成功后，这个Key早就已经过期了。&lt;/p&gt;
&lt;p&gt;KDC（Key Distribution Center）：kerberos server作为可信第三方，维护所有帐户（client、server）的注册信息、用户名、口令、用户主密钥、服务器主密钥&lt;/p&gt;
&lt;p&gt;Server 与Client之间基于共享秘密短期密钥key实现身份鉴别&lt;/p&gt;
&lt;p&gt;KDC仅仅是允许进入应用系统，至于有什么权限、由应用系统自主决定&lt;/p&gt;
&lt;p&gt;获取TGT：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080938952.png&#34;
	width=&#34;1269&#34;
	height=&#34;425&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080938952_hu749a8a9ea3b3cb0610ea690fd3c1d955_39559_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080938952_hu749a8a9ea3b3cb0610ea690fd3c1d955_39559_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705080938952&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;298&#34;
		data-flex-basis=&#34;716px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;client发请求，KDC用client的master key加密一个会话密钥$S_{KDC-Client}$，用KDC的master key加密TGT，TGT里包含会话密钥和client信息（让client 鉴别KDC是KDC而非被伪造）&lt;/p&gt;
&lt;p&gt;获取ST：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705139628762.png&#34;
	width=&#34;790&#34;
	height=&#34;324&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705139628762_hu18769472a60cb76d7bf8353fa3fe6283_61984_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705139628762_hu18769472a60cb76d7bf8353fa3fe6283_61984_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705139628762&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;585px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;这个图有问题，KDC还给client的不是用clinet的master key，而是用session key。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当client要访问server的时候，给KDC自己的TGT和要访问的server。&lt;/p&gt;
&lt;p&gt;KDC根据TGT来对client进行认证，生成$S_{Server-Client}$和ST(session ticket)&lt;/p&gt;
&lt;p&gt;$S_{Server-Client}$：用client的主密钥加密一个会话密钥，&lt;/p&gt;
&lt;p&gt;ST：用server的主密钥加密，ST包含会话密钥和client的信息。&lt;/p&gt;
&lt;p&gt;将这两个被加密的Copy一并发送给Client&lt;/p&gt;
&lt;p&gt;client得到会话密钥后，用session key解密，创建Authenticator（Client Info + Timestamp）并用会话密钥加密&lt;/p&gt;
&lt;p&gt;client将ST和Authenticator访问server，server用自己的主密钥解密ST得到会话密钥，在用会话密钥解密Authenticator，比较Authenticator里的client info和ST里的client info来确定client就是client&lt;/p&gt;
&lt;p&gt;那如果TGT没过期，session key过期了呢？可以用TGT再申请一个，因为TGT用KDC的master key加密，KDC可以得到旧的session key和client info，进而再发一个session key。由于session key是TGT的一部分，这其实也就相当于重新申请了TGT&lt;/p&gt;
&lt;p&gt;client鉴别server（双向鉴别）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080767000.png&#34;
	width=&#34;1165&#34;
	height=&#34;397&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080767000_hu461c64cc558dc3baf036d2ae29d69c28_45767_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080767000_hu461c64cc558dc3baf036d2ae29d69c28_45767_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705080767000&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;293&#34;
		data-flex-basis=&#34;704px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在Authenticator里在加一个flag要求server自证。&lt;/p&gt;
&lt;p&gt;server看到后，用ST里得到的会话密钥解密Authenticator，把里面的timestamp用会话密钥加密发给client&lt;/p&gt;
&lt;h2 id=&#34;16-oauthoidc&#34;&gt;16 OAuth&amp;amp;OIDC&lt;/h2&gt;
&lt;p&gt;单点登录(Single Sign on)在某个地方认证了之后，在整个域里都不用再认证了。&lt;/p&gt;
&lt;p&gt;SSO 口令记录器-&amp;gt;保存在edge/chrome&lt;/p&gt;
&lt;p&gt;OAuth 协议流程图，理解认证的流程，有那几个角色，分别做了什么&lt;/p&gt;
&lt;p&gt;OIDC 协议流程图，理解认证的流程&lt;/p&gt;
&lt;h2 id=&#34;17-fido&#34;&gt;17 FIDO&lt;/h2&gt;
&lt;p&gt;在服务器端将用户与移动终端的可信环境进行身份绑定
将用户与服务器之间的直接鉴别转变为两段式鉴别
1 移动终端鉴别用户主要是靠生物特征
2 服务器端鉴别移动终端主要是靠数字签名&lt;/p&gt;
</description>
        </item>
        <item>
        <title>自然语言处理笔记</title>
        <link>https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 27 Dec 2023 19:07:00 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp.png" alt="Featured image of post 自然语言处理笔记" /&gt;&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;期末全是开放问题，因此弄清楚各种模型的优劣非常有必要。&lt;/p&gt;
&lt;h1 id=&#34;笔记&#34;&gt;笔记&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp.png&#34;
	width=&#34;1780&#34;
	height=&#34;1273&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp_hudb173f042648ce2e35cf0194087f2335_506623_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp_hudb173f042648ce2e35cf0194087f2335_506623_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;nlp.png&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;335px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;model&lt;/h2&gt;
&lt;p&gt;常见的模型有DNN、CNN、RNN、GNN、LSTM、&lt;/p&gt;
&lt;h2 id=&#34;task&#34;&gt;task&lt;/h2&gt;
&lt;p&gt;NLP的经典问题有&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704274435391.png&#34;
	width=&#34;1080&#34;
	height=&#34;885&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704274435391_hu25258c87febe04012aa7f4a2a0e0d764_159198_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704274435391_hu25258c87febe04012aa7f4a2a0e0d764_159198_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704274435391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;122&#34;
		data-flex-basis=&#34;292px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在课程中，我们主要学习了&lt;/p&gt;
&lt;h4 id=&#34;属性抽取ae&#34;&gt;属性抽取（AE）&lt;/h4&gt;
&lt;p&gt;opinion target和aspect的区别：opinion target是被评价对象，aspect是对象的属性&lt;/p&gt;
&lt;p&gt;eg&amp;quot;这个手机的摄像头很出色，但电池寿命较短。&amp;ldquo;手机是opinion target，而摄像头和电池寿命是手机的两个aspect。&lt;/p&gt;
&lt;p&gt;目标：抽取对象。eg：华为技术遥遥领先！-&amp;gt; 抽取出“华为”&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/51189078&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Aspect Term Extraction 论文阅读（一） - 知乎 (zhihu.com)&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704948448294.png&#34;
	width=&#34;1843&#34;
	height=&#34;1053&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704948448294_hufc3e4fc27f395c813af2b2f990a86bdd_405379_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704948448294_hufc3e4fc27f395c813af2b2f990a86bdd_405379_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704948448294&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;THA是用了attention机制的LSTM，用于获得上文（单向）已经标注过的aspect的信息，来指导当前aspect标注。&lt;/p&gt;
&lt;p&gt;STN是LSTM，用于获得opinion的摘要信息。首先，STN单元获得基于给定aspect的opinion的表示，接下来利用attention机制来获得基于全局的opinion的表示。自此就可以获得基于当前aspect的opinion摘要。将aspect的表示和opinion摘要拼接作为特征，用于标注。&lt;/p&gt;
&lt;p&gt;（表示就是一个框里三个圆圆）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704954212132.png&#34;
	width=&#34;1811&#34;
	height=&#34;1203&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704954212132_hu27c9f3e4b068acfa2a66d07c6f55407e_231068_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704954212132_hu27c9f3e4b068acfa2a66d07c6f55407e_231068_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704954212132&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;将ATE形式化为一个seq2seq的学习任务。在这个任务中，源序列和目标序列分别由单词和标签组成。为了使Seq2Seq学习更适合ATE,作者设计了门控单元网络和位置感知注意力机制。门控单元网络用于将相应的单词表示融入解码器，而位置感知注意力机制则用于更多地关注目标词的相邻词。&lt;/p&gt;
&lt;p&gt;decoder包含一个门控单元，用于控制编码器和解码器产生的隐状态。当解码标签时，这个门控单元可以自动的整合来自编码器和解码器隐状态的信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704955929276.png&#34;
	width=&#34;1851&#34;
	height=&#34;1163&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704955929276_huf66787a5171c86d7538ba142f6dfa2f4_254054_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704955929276_huf66787a5171c86d7538ba142f6dfa2f4_254054_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704955929276&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;381px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;masked seq2seq。首先，对输入句子的连续几个词进行掩码处理。然后，encoder接收部分掩码的句子及其标签序列作为输入，decoder尝试根据编码上下文和标签信息重建句子原文。要求保持opinion target位置不变&lt;/p&gt;
&lt;h4 id=&#34;观点抽取oe&#34;&gt;观点抽取（OE）&lt;/h4&gt;
&lt;p&gt;一般都是先抽取aspect，在对aspect进行情感预测的流水线方式&lt;/p&gt;
&lt;p&gt;IMN使用非流水线方式。与传统的多任务学习方法依赖于学习不同任务的共同特征不同，IMN引入了一种消息传递体系结构，通过一组共享的潜在变量将信息迭代地传递给不同的任务&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704957048177.png&#34;
	width=&#34;1889&#34;
	height=&#34;961&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704957048177_hu5e6f9b41d2c4d023d0cece8758c2cbe8_205570_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704957048177_hu5e6f9b41d2c4d023d0cece8758c2cbe8_205570_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704957048177&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;196&#34;
		data-flex-basis=&#34;471px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;它接受一系列tokens{x1，…，xn}作为特征提取组件fθs的输入，该组件在所有任务之间共享。该组件由单词嵌入层和几个特征提取层（好多个CNN）组成。输出所有任务共享的潜在向量{hs1，hs2，…，hsn}的序列。该潜在向量序列会根据来自不同任务组件传播来的信息来更新。&lt;/p&gt;
&lt;p&gt;$hi^{s(T)}$ 表示为t轮消息传递后Xi对应的共享潜在向量的值。&lt;/p&gt;
&lt;p&gt;共享潜在向量序列用作不同任务特定组件的输入。每个特定于任务的组件都有自己的潜在变量和输出变量集。输出变量对应于序列标签任务中的标签序列；在AE中，我们为每个令牌分配一个标签，表明它是否属于任何aspect或opinion，而在AS中，我们为每个单词加上它的情感标签。在分类任务中，输出对应于输入实例的标签：情感分类任务(DS)的文档的情感，以及领域分类任务(DD)的文档域。在每次迭代中，适当的信息被传递回共享的潜在向量以进行组合；这可以是输出变量的值，也可以是潜在变量的值，具体取决于任务。 此外，我们还允许在每次迭代中在组件之间传递消息（opinion transmission）。&lt;/p&gt;
&lt;p&gt;感觉有点训练词向量的感觉，像是预处理一下得到向量序列来方便其他任务。&lt;/p&gt;
&lt;p&gt;【超，好像这些都不是考试重点】&lt;/p&gt;
&lt;h2 id=&#34;属性级情感分类&#34;&gt;属性级情感分类&lt;/h2&gt;
&lt;h1 id=&#34;for-exam&#34;&gt;For exam&lt;/h1&gt;
&lt;p&gt;试卷题型：简答题 40 分（5*8）好多个问号（内容为胡老师讲的基础部分）+ 综合题 60 分（内容为曹老师讲的核心应用部分）&lt;/p&gt;
&lt;p&gt;简答题重点章节：&lt;/p&gt;
&lt;p&gt;什么是语言模型、神经网络语言模型、几种、特点（优点）&lt;/p&gt;
&lt;p&gt;概念性的简答题， 不难+&lt;/p&gt;
&lt;p&gt;第4章 语言模型+词向量 （要求掌握：语言模型概念，神经网络语言模型 ）&lt;/p&gt;
&lt;p&gt;第 5章 NLP中的注意力机制 （全部要求掌握）概念、用处&lt;/p&gt;
&lt;p&gt;第 7 章 预训练语言模型（全部要求掌握）[主要掌握GPT，BERT 是 怎么训练的，与下游任务是如何对接的]prompt，inconcert learning，思维链【建模的几种范式】&lt;/p&gt;
&lt;p&gt;主观题重点章节： 设计东西&lt;/p&gt;
&lt;p&gt;第9章 情感分析（要求掌握：方面级情感分析基本方法原理）&lt;/p&gt;
&lt;p&gt;第10章 信息抽取 （要求掌握：实体和关系联合抽取基本方法原理）&lt;/p&gt;
&lt;p&gt;第 11章 问答系统（要求掌握：检索式问答系统基本方法原理）&lt;/p&gt;
&lt;h2 id=&#34;语言模型概念&#34;&gt;语言模型概念&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349075208.png&#34;
	width=&#34;1880&#34;
	height=&#34;1373&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349075208_hu0c1ca89050c448333fc36378f9fbfccb_260188_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349075208_hu0c1ca89050c448333fc36378f9fbfccb_260188_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349075208&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;328px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349189011.png&#34;
	width=&#34;1915&#34;
	height=&#34;1431&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349189011_huc8ea206b6bb16ee526b95676d1b53cbe_168180_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349189011_huc8ea206b6bb16ee526b95676d1b53cbe_168180_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349189011&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;321px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349255995.png&#34;
	width=&#34;1917&#34;
	height=&#34;1436&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349255995_hu12f6c968581a6571f907361cab13f8ee_216633_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349255995_hu12f6c968581a6571f907361cab13f8ee_216633_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349255995&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型&lt;/h2&gt;
&lt;p&gt;统计的方法使用最大似然估计，需要数据平滑否则会出现0概率问题。&lt;/p&gt;
&lt;p&gt;神经网络使用DNN和RNN&lt;/p&gt;
&lt;p&gt;利用RNN 语言模型可以解决以上概率语言模型问题，在神经网络一般用RNN语言模型&lt;/p&gt;
&lt;h3 id=&#34;一些我不会的背景知识&#34;&gt;一些（我不会的）背景知识&lt;/h3&gt;
&lt;h4 id=&#34;梯度下降算法&#34;&gt;梯度下降算法&lt;/h4&gt;
&lt;p&gt;梯度下降法是一种常用的优化算法，主要用于找到函数的局部最小值。它的基本思想是：在每一步迭代过程中，选择函数在当前点的负梯度（即函数在该点下降最快的方向）作为搜索方向，然后按照一定的步长向该方向更新当前点，不断迭代，直到满足停止准则。&lt;/p&gt;
&lt;p&gt;具体来说，假设我们要最小化一个可微函数$f(x)$，我们首先随机选择一个初始点$x_0$，然后按照以下规则更新$x$：&lt;/p&gt;
&lt;p&gt;$$
x_{n+1} = x_n - \alpha \nabla f(x_n)
$$&lt;/p&gt;
&lt;p&gt;其中，$\nabla f(x_n)$是函数$f$在点$x_n$处的梯度，$\alpha$是步长（也称为学习率），控制着每一步更新的幅度。&lt;/p&gt;
&lt;p&gt;梯度下降法只能保证找到局部最小值&lt;/p&gt;
&lt;h4 id=&#34;双曲正切函数&#34;&gt;双曲正切函数&lt;/h4&gt;
&lt;p&gt;它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在&lt;/p&gt;
&lt;p&gt;$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$&lt;/p&gt;
&lt;h4 id=&#34;bp算法&#34;&gt;BP算法&lt;/h4&gt;
&lt;p&gt;反向传播算法，当正向传播得到的结果和预期不符，则反向传播，修改权重&lt;/p&gt;
&lt;h4 id=&#34;bptt算法&#34;&gt;BPTT算法&lt;/h4&gt;
&lt;p&gt;是BP算法的拓展，可以处理具有时间序列结构的数据，用于训练RNN&lt;/p&gt;
&lt;p&gt;BPTT的工作原理如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;正向传播&lt;/strong&gt; ：在每个时间步，网络会读取输入并计算输出。这个过程会持续进行，直到处理完所有的输入序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt; ：一旦完成所有的正向传播步骤，网络就会计算最后一个时间步的误差（即网络的预测与实际值之间的差距），然后将这个误差反向传播到前一个时间步。这个过程会持续进行，直到误差被传播回第一个时间步。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;参数更新&lt;/strong&gt; ：在误差反向传播的过程中，网络会计算误差关于每个参数的梯度。然后，这些梯度会被用来更新网络的参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;one-hot编码&#34;&gt;one-hot编码&lt;/h4&gt;
&lt;p&gt;独热编码是一种将离散的分类标签转换为二进制向量的方法&lt;/p&gt;
&lt;p&gt;假设我们要做一个分类任务，总共有3个类别，分别是猫、狗、人。那这三个类别就是一种离散的分类：它们之间互相独立，不存在谁比谁大、谁比谁先、谁比谁后的关系。&lt;/p&gt;
&lt;p&gt;在神经网络中，需要一种数学的表示方法，来表示猫、狗、人的分类。最容易想到的，便是以 0 代表猫，以 1 代表狗，以 2 代表人这种简单粗暴的方式。但这样会导致分类标签之间出现了不对等的情况。（2比1大……）&lt;/p&gt;
&lt;p&gt;而进行如下的编码的话就可以解决这个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;猫：[1, 0, 0]&lt;/li&gt;
&lt;li&gt;狗：[0, 1, 0]&lt;/li&gt;
&lt;li&gt;人：[0, 0, 1]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这就是独热码&lt;/p&gt;
&lt;h4 id=&#34;pairwise&#34;&gt;pairwise&lt;/h4&gt;
&lt;p&gt;&amp;ldquo;Pairwise&amp;quot;是一种常用于排序和推荐系统的方法。它的主要思想是将排序问题转换为二元分类问题。每次取一对样本，预估这一对样本的先后顺序，不断重复预估一对对样本，从而得到某条查询下完整的排序。如果文档A的相关性高于文档B，则赋值+1，反之则赋值-1。这样，我们就得到了二元分类器训练所需的训练样本&lt;/p&gt;
&lt;p&gt;Pairwise方法也有其缺点。例如，它只考虑了两篇文档的相对顺序，没有考虑他们出现在搜索结果列表中的位置。&lt;/p&gt;
&lt;p&gt;除了Pairwise，还有其他的方法如Pointwise和Listwise。Pointwise方法每次仅仅考虑一个样本，预估的是每一条和查询的相关性，基于此进行排序。而Listwise方法则同时考虑多个样本，找到最优顺序。这些方法各有优缺点，选择哪种方法取决于具体的应用场景和需求。&lt;/p&gt;
&lt;h4 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h4&gt;
&lt;p&gt;&amp;ldquo;Zero-shot learning&amp;rdquo;（零样本学习）是一种机器学习范式，它允许模型在没有先前训练过相关数据集的情况下，对不包含在训练数据中的类别或任务进行准确的预测或推断。这种能力是由先进的深度学习模型和迁移学习方法得以实现的。&lt;/p&gt;
&lt;p&gt;举个例子，假设我们的模型已经能够识别马，老虎和熊猫了，现在需要该模型也识别斑马，那么我们需要告诉模型，怎样的对象才是斑马，但是并不能直接让模型看见斑马。所以模型需要知道的信息是马的样本、老虎的样本、熊猫的样本和样本的标签，以及关于前三种动物和斑马的描述。&lt;/p&gt;
&lt;p&gt;这种方法的优点是可以极大地节省标注量。不需要增加样本，只需要增加描述即可。&lt;/p&gt;
&lt;h4 id=&#34;ppo&#34;&gt;PPO&lt;/h4&gt;
&lt;p&gt;PPO（Proximal Policy Optimization，近端策略优化）是一种强化学习算法，由OpenAI在2017年提出。PPO算法的目标是解决深度强化学习中策略优化的问题。&lt;/p&gt;
&lt;p&gt;PPO算法的核心在于更新策略梯度，主流方法有两种，分别是KL散度做penalty，另一种是Clip剪裁，它们的主要作用都是限制策略梯度更新的幅度，从而推导出不同的神经网络参数更新方式。&lt;/p&gt;
&lt;p&gt;PPO算法具备Policy Gradient、TRPO的部分优点，采样数据和使用随机梯度上升方法优化代替目标函数之间交替进行，虽然标准的策略梯度方法对每个数据样本执行一次梯度更新，但PPO提出新目标函数，可以实现小批量更新。&lt;/p&gt;
&lt;h3 id=&#34;dnnnnlm&#34;&gt;DNN（NNLM）&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704359720351.png&#34;
	width=&#34;583&#34;
	height=&#34;489&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704359720351_hu1a1e5f58fd6df4800c855f239c4e8368_49965_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704359720351_hu1a1e5f58fd6df4800c855f239c4e8368_49965_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704359720351&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;286px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-grambigram&#34;&gt;2-gram（bigram）&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364780556.png&#34;
	width=&#34;1383&#34;
	height=&#34;965&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364780556_hu760ecb3db27206cf53920759113a8d8c_82901_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364780556_hu760ecb3db27206cf53920759113a8d8c_82901_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704364780556&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;343px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364894689.png&#34;
	width=&#34;1747&#34;
	height=&#34;1247&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364894689_hu1584a8d9b5c853130d6fa493ed5e84cb_319682_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364894689_hu1584a8d9b5c853130d6fa493ed5e84cb_319682_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704364894689&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;336px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中$\theta$就是训练过程中要学习的参数，有了这些参数就可以直接的到 $p(w_i|w_{i-1})$， 找到一组足够好的参数，就能让得到的$p(w_i|w_{i-1})$最接近训练语料库&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704365950174.png&#34;
	width=&#34;1037&#34;
	height=&#34;963&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704365950174_huce4441bc6e09012c7fc01bdc7974a721_116580_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704365950174_huce4441bc6e09012c7fc01bdc7974a721_116580_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704365950174&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;107&#34;
		data-flex-basis=&#34;258px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这里的最大化就是损失函数最小（最接近0），因为P永远小于1，所以log永远是负数，他们加起来永远小于0，让log最大，也就是让log最接近0&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366274757.png&#34;
	width=&#34;1703&#34;
	height=&#34;1093&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366274757_hu46b344b03803c7c0fed3c178313bbde2_115064_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366274757_hu46b344b03803c7c0fed3c178313bbde2_115064_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366274757&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;373px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;n-gram&#34;&gt;n-gram&lt;/h4&gt;
&lt;p&gt;拓展一下罢了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366342703.png&#34;
	width=&#34;1463&#34;
	height=&#34;973&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366342703_huf72800e8235c138cf4fd59fa50bbbeb2_91381_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366342703_huf72800e8235c138cf4fd59fa50bbbeb2_91381_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366342703&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366354123.png&#34;
	width=&#34;1753&#34;
	height=&#34;1131&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366354123_hud6de40a4b485f08e9cfde6fed9681f09_133344_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366354123_hud6de40a4b485f08e9cfde6fed9681f09_133344_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366354123&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;371px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366370789.png&#34;
	width=&#34;1699&#34;
	height=&#34;1149&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366370789_hu26fb6493ab425d95d30f20774f3e50a4_177932_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366370789_hu26fb6493ab425d95d30f20774f3e50a4_177932_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366370789&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;354px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;rnnrnnlm&#34;&gt;RNN（RNNLM）&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367534089.png&#34;
	width=&#34;1053&#34;
	height=&#34;603&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367534089_hub368771156f39db0dc5aaadcd704118b_165183_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367534089_hub368771156f39db0dc5aaadcd704118b_165183_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367534089&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367561457.png&#34;
	width=&#34;1913&#34;
	height=&#34;1245&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367561457_huab66d923a3650b61264f1017d4637585_146551_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367561457_huab66d923a3650b61264f1017d4637585_146551_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367561457&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;153&#34;
		data-flex-basis=&#34;368px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367582922.png&#34;
	width=&#34;1853&#34;
	height=&#34;1329&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367582922_huafade5146f75aefbdf5d3dc9453d7d0a_267697_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367582922_huafade5146f75aefbdf5d3dc9453d7d0a_267697_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367582922&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;334px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704368432414.png&#34;
	width=&#34;2015&#34;
	height=&#34;1253&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704368432414_hu21b8c7a7e8fa0d8b9590afe211386631_154349_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704368432414_hu21b8c7a7e8fa0d8b9590afe211386631_154349_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704368432414&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;385px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;词向量&#34;&gt;词向量&lt;/h2&gt;
&lt;p&gt;自然语言问题要用计算机处理时，第一步要找一种方法把这些符号数字化，成为计算机方便处理的形式化表示&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NNLM模型词向量&lt;/li&gt;
&lt;li&gt;RNNLM模型词向量&lt;/li&gt;
&lt;li&gt;C&amp;amp;W 模型词向量&lt;/li&gt;
&lt;li&gt;CBOW 模型词向量&lt;/li&gt;
&lt;li&gt;Skip-gram模型词向量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不同模型的词向量之间的主要区别在于它们捕获和编码词义和上下文信息的方式。以下是一些常见模型的词向量特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;神经网络语言模型（NNLM）&lt;/strong&gt; ：NNLM通过学习预测下一个词的任务来生成词向量。这种方法可以捕获词义和词之间的关系，但是它通常无法捕获长距离的依赖关系，因为它只考虑了固定大小的上下文。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;循环神经网络语言模型（RNNLM）&lt;/strong&gt; ：RNNLM使用循环神经网络结构，可以处理变长的输入序列，并能捕获长距离的依赖关系。因此，RNNLM生成的词向量可以包含更丰富的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt; ：Word2Vec是一种预训练词向量的方法，它包括两种模型：Skip-gram和CBOW。Skip-gram模型通过一个词预测其上下文，而CBOW模型则通过上下文预测一个词。Word2Vec生成的词向量可以捕获词义和词之间的各种关系，如同义词、反义词、类比关系等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GloVe&lt;/strong&gt; ：GloVe（Global Vectors for Word Representation）是另一种预训练词向量的方法，它通过对词-词共现矩阵进行分解来生成词向量。GloVe生成的词向量可以捕获词义和词之间的线性关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; ：BERT（Bidirectional Encoder Representations from Transformers）使用Transformer模型结构，并通过预训练任务（如Masked Language Model和Next Sentence Prediction）来生成词向量。BERT生成的词向量是上下文相关的，也就是说，同一个词在不同的上下文中可能有不同的词向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，不同模型的词向量之间的区别主要在于它们捕获和编码词义和上下文信息的方式。选择哪种词向量取决于具体的任务需求和计算资源。&lt;/p&gt;
&lt;h3 id=&#34;nnlm的词向量&#34;&gt;NNLM的词向量&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369375532.png&#34;
	width=&#34;1459&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369375532_hu832eda59757e840b1bbf4646543a00cb_80143_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369375532_hu832eda59757e840b1bbf4646543a00cb_80143_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369375532&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;326px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;解决办法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369411606.png&#34;
	width=&#34;1977&#34;
	height=&#34;1085&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369411606_hua7c014e2889ce3df3191c7a50bd72fa1_161595_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369411606_hua7c014e2889ce3df3191c7a50bd72fa1_161595_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369411606&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;437px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;通过一个|D| * |V|的矩阵，额可以将one-shot的编码转为D维的稠密的词向量，所以管他叫lookup表&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369517877.png&#34;
	width=&#34;1845&#34;
	height=&#34;1273&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369517877_hu786818deba51ac724b8d09afb69879a6_211910_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369517877_hu786818deba51ac724b8d09afb69879a6_211910_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369517877&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369568799.png&#34;
	width=&#34;1783&#34;
	height=&#34;1209&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369568799_hu7721cda882a2a39fe07f475340edf74e_215449_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369568799_hu7721cda882a2a39fe07f475340edf74e_215449_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369568799&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;NNLM 语言模型在训练语言模型同时也训练了词向量&lt;/p&gt;
&lt;h3 id=&#34;rnnlm的词向量&#34;&gt;RNNLM的词向量&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369951464.png&#34;
	width=&#34;1795&#34;
	height=&#34;1075&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369951464_hu87c6ec9fcf04d9407be3e10dc98aa4a8_152073_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369951464_hu87c6ec9fcf04d9407be3e10dc98aa4a8_152073_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369951464&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704370827646.png&#34;
	width=&#34;1955&#34;
	height=&#34;1039&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704370827646_hu26c39f59ffb4c0f9f30b14263e3fe320_129520_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704370827646_hu26c39f59ffb4c0f9f30b14263e3fe320_129520_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704370827646&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;cw&#34;&gt;C&amp;amp;W&lt;/h3&gt;
&lt;p&gt;C&amp;amp;W模型是靠两边猜中间的一种模型，输入层是wi上下文的词向量&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704371807694.png&#34;
	width=&#34;1935&#34;
	height=&#34;1207&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704371807694_hufc986403cc4fc5c6afb6539138fa18f8_222743_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704371807694_hufc986403cc4fc5c6afb6539138fa18f8_222743_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704371807694&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;score是wi中间这个word在这个位置有多合理，越高越合理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372099728.png&#34;
	width=&#34;1669&#34;
	height=&#34;1173&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372099728_hu69c8c5e8754ef3d413c6210305f69216_137542_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372099728_hu69c8c5e8754ef3d413c6210305f69216_137542_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372099728&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;正样本通常是指在实际语料库中出现过的词语及其上下文。负样本则是人为构造的，通常是将一个词与一个随机的上下文配对。&lt;/p&gt;
&lt;p&gt;Pairwise方法在训练C&amp;amp;W词向量时，主要是通过比较一对词的上下文来进行训练的。具体来说，对于每一对词（一个正样本和一个负样本），我们都会计算它们的词向量，并通过比较这两个词向量的相似度来更新我们的模型。&lt;/p&gt;
&lt;p&gt;在训练过程中，我们首先需要选择一个损失函数，这里是修改后的HingeLoss&lt;/p&gt;
&lt;p&gt;然后，我们会使用一种优化算法来最小化这个损失函数，这里是梯度下降，在每一次迭代中，我们都会根据当前的损失来更新我们的词向量。&lt;/p&gt;
&lt;p&gt;训练的目标是在正样本中的score高，负样本的score低，然后score差的越大效果越好&lt;/p&gt;
&lt;h3 id=&#34;cbow&#34;&gt;CBOW&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372201199.png&#34;
	width=&#34;1851&#34;
	height=&#34;1247&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372201199_hu799ddf5409049cc214b2706c3b2e0521_248117_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372201199_hu799ddf5409049cc214b2706c3b2e0521_248117_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372201199&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;356px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;CBOW也是靠两边猜中间，输入层是wi上下文词向量的平均值，目标是最小化（最收敛与0）上下文词的平均与目标词之间的距离。输出是&lt;/p&gt;
&lt;h3 id=&#34;skip-gram&#34;&gt;skip-gram&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372268757.png&#34;
	width=&#34;1903&#34;
	height=&#34;1265&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372268757_hu20c9fa8cfa5b334a829835e73add6121_225661_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372268757_hu20c9fa8cfa5b334a829835e73add6121_225661_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372268757&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;skip-gram是知道中间猜两边，训练最小化（最收敛于0）目标词与上下文词之间的距离。&lt;/p&gt;
&lt;h2 id=&#34;注意力机制&#34;&gt;注意力机制&lt;/h2&gt;
&lt;h3 id=&#34;概述&#34;&gt;概述&lt;/h3&gt;
&lt;p&gt;在注意力机制中，Q、K、V 分别代表查询（Query）、键（Key）和值（Value）。&lt;/p&gt;
&lt;p&gt;注意力机制的工作过程可以简单概括为：对于每一个查询，计算它与所有键的匹配程度（通常使用点积），然后对这些匹配程度进行归一化（通常使用 softmax 函数），得到每个键对应的&lt;strong&gt;权重&lt;/strong&gt;。最后，用这些权重对所有的值进行加权求和，得到最终的输出。&lt;/p&gt;
&lt;p&gt;这种机制允许模型在处理一个元素时，考虑到其他相关元素的信息，从而捕捉输入元素之间的依赖关系。在自然语言处理、计算机视觉等领域，注意力机制已经被广泛应用，并取得了显著的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978816700.png&#34;
	width=&#34;1335&#34;
	height=&#34;435&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978816700_hucd19b4ee5a4afc0633709fd168f378ea_110878_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978816700_hucd19b4ee5a4afc0633709fd168f378ea_110878_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978816700&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;306&#34;
		data-flex-basis=&#34;736px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978826803.png&#34;
	width=&#34;1409&#34;
	height=&#34;681&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978826803_hu55909e70fd2eea21c0ef3b39a089306c_154712_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978826803_hu55909e70fd2eea21c0ef3b39a089306c_154712_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978826803&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;496px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978837242.png&#34;
	width=&#34;1409&#34;
	height=&#34;689&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978837242_hu5fed35e3283668992fbef7317ca675cc_240700_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978837242_hu5fed35e3283668992fbef7317ca675cc_240700_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978837242&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;204&#34;
		data-flex-basis=&#34;490px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;W是权重，都是学来的。&lt;/p&gt;
&lt;p&gt;参考&lt;/p&gt;
&lt;p&gt;KQV矩阵： &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;amp;vd_source=2fbfeabcc6cdd857dcd6247eb0154d83&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;vd_source=2fbfeabcc6cdd857dcd6247eb0154d83&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    &lt;/p&gt;
&lt;p&gt;Attention机制： &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1YA411G7Ep&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1YA411G7Ep&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455161185.png&#34;
	width=&#34;1261&#34;
	height=&#34;844&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455161185_hud02f354ce847a0460c1751bcd5356e00_95026_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455161185_hud02f354ce847a0460c1751bcd5356e00_95026_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704455161185&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;358px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455261000.png&#34;
	width=&#34;1334&#34;
	height=&#34;886&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455261000_hu9b5dd8e475366b83ee7feb2a27f6ee07_315684_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455261000_hu9b5dd8e475366b83ee7feb2a27f6ee07_315684_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704455261000&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;K、V都是经过线性变换的词向量集合（矩阵）&lt;/p&gt;
&lt;p&gt;Q是隐藏状态（隐藏向量）&lt;/p&gt;
&lt;p&gt;A是一个注意力值，就是我们设置的这个字的注意力值&lt;/p&gt;
&lt;p&gt;通过attention的学习，可以得到a1、a2……，这些就是K中各个向量对Q的权重&lt;/p&gt;
&lt;p&gt;步骤1：计算 f ( Q ,Ki )&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467445387.png&#34;
	width=&#34;1222&#34;
	height=&#34;329&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467445387_huf6a66801c22a84097970dc8f17d3cce6_60883_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467445387_huf6a66801c22a84097970dc8f17d3cce6_60883_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467445387&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;371&#34;
		data-flex-basis=&#34;891px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;步骤2：计算对于Q 各个 Ki 的权重&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467647363.png&#34;
	width=&#34;1220&#34;
	height=&#34;839&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467647363_huafa347c01c0e5ef24a433771922fb344_279695_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467647363_huafa347c01c0e5ef24a433771922fb344_279695_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467647363&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;步骤3：计算输出 Att-V值（各 Ki 乘以自己的权重，然后求和 ）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467679320.png&#34;
	width=&#34;1175&#34;
	height=&#34;770&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467679320_hu245bc1ed024d01fb98cbe3a532bdc531_268416_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467679320_hu245bc1ed024d01fb98cbe3a532bdc531_268416_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467679320&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;366px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;举例1-seq2seqrnn2rnn的机器翻译中&#34;&gt;举例1， seq2seq（RNN2RNN）的机器翻译中&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704543763032.png&#34;
	width=&#34;2310&#34;
	height=&#34;1297&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704543763032_hua0bc05f75798fba85c66e244a5194c41_559871_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704543763032_hua0bc05f75798fba85c66e244a5194c41_559871_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704543763032&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;427px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;seq2seq做机器翻译的过程，需要大量的两种语言的平行语料，就是意思相同的语言的一一对应的关系。&lt;/p&gt;
&lt;p&gt;其中x为词向量，A为权重矩阵，h为隐藏状态（隐藏向量）。&lt;/p&gt;
&lt;p&gt;RNN是用预训练的词向量，然后通过学习权重矩阵A来微调，得到隐藏状态，可以理解为隐藏状态是带了上下文的更加符合RNN的词的向量表示。&lt;/p&gt;
&lt;p&gt;每一个时间步中，A都被微调， 因此x1、x2、x3的A可能都是不一样的。在大量预料的训练下会获得表现比较好的A和A&#39;&lt;/p&gt;
&lt;p&gt;h可以表示为$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$其中$h_t$ 是当前时间步的隐藏状态，$h_{t-1}$ 是前一时间步的隐藏状态，$x_t$ 是当前时间步的输入，$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b_h$ 是偏置项，$f$ 是激活函数&lt;/p&gt;
&lt;p&gt;不加注意力机智的seq2seq模型，encoder是RNN，decoder也是RNN，在encoder接受了$x_1$到$x_m$的词向量序列后，得到最终的隐藏状态$h_m$， 也就是$s_0$，作为decoder的初始状态。&lt;/p&gt;
&lt;p&gt;如果不加注意力机制，decoder那边也就是靠隐藏状态、x和参数（A矩阵，偏置值b）来继续进行RNN的步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550430429.png&#34;
	width=&#34;1957&#34;
	height=&#34;1270&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550430429_hu5bd76466549592210c32b0ec81a8c6ce_420808_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550430429_hu5bd76466549592210c32b0ec81a8c6ce_420808_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704550430429&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;369px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;现在我们引入注意力机制，也就是图上的$c_0$，权重的计算按照上面所说的KQV计算方法，这里K是词向量集合x1,x2&amp;hellip; Q是隐藏状态。也就是对于每一个隐藏状态，都可以求一个关于词向量序列的权重值$\alpha$。&lt;/p&gt;
&lt;p&gt;通过求出这一系列的$\alpha$，就可以加权求出上下文矩阵$c$，c知道当前隐藏状态和词向量矩阵的全部关系。&lt;/p&gt;
&lt;p&gt;加了注意力机制之后，decoder的各个隐藏状态求解过程就会向之前提到的那样变得更复杂&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551152595.png&#34;
	width=&#34;810&#34;
	height=&#34;274&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551152595_hu0a77244b867e70bdac507a1efbb0979e_93382_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551152595_hu0a77244b867e70bdac507a1efbb0979e_93382_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704551152595&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;295&#34;
		data-flex-basis=&#34;709px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;而每一个步骤的c都不一样，比如&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550965949.png&#34;
	width=&#34;2303&#34;
	height=&#34;1289&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550965949_hubb4f0266f0e8c1f8b52196bcfda15244_479226_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550965949_hubb4f0266f0e8c1f8b52196bcfda15244_479226_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704550965949&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;428px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;c0是s0对于x1,x2&amp;hellip;的att-V，也就是hm对于x1,x2&amp;hellip;的att-V；c1是隐藏状态s1对于x1,x2&amp;hellip;的att-V，c2是隐藏状态s2对于x1,x2&amp;hellip;的att-V这些c都需要花算力来算&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551282267.png&#34;
	width=&#34;686&#34;
	height=&#34;415&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551282267_hud01555d935e07390dfdc00e0199932ee_94242_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551282267_hud01555d935e07390dfdc00e0199932ee_94242_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704551282267&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;396px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;注意力机制的问题是时间复杂度太大了。如果是简单的RNN2RNN，如果encoder词向量矩阵大小为m，decoder词向量矩阵大小为n，所需的时间复杂度为O(m+n)，而使用注意力机制之后就会变成O(mn)，还是打分函数比较简单的情况下。&lt;/p&gt;
&lt;h3 id=&#34;注意力编码机制&#34;&gt;注意力编码机制&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470189498.png&#34;
	width=&#34;1231&#34;
	height=&#34;851&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470189498_hub8e60ab929906ef66b5419a3f8766e33_255484_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470189498_hub8e60ab929906ef66b5419a3f8766e33_255484_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470189498&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470407421.png&#34;
	width=&#34;1216&#34;
	height=&#34;871&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470407421_huabaf6bf0ae4174704a7bb8c6578ed4c7_110600_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470407421_huabaf6bf0ae4174704a7bb8c6578ed4c7_110600_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470407421&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;335px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470394991.png&#34;
	width=&#34;1266&#34;
	height=&#34;806&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470394991_hu2fa536fd23e0067fad9b8d1c7df89fc1_89880_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470394991_hu2fa536fd23e0067fad9b8d1c7df89fc1_89880_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470394991&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;376px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;attention机制还可以将不同序列融合编码（将多个序列经过某种处理或嵌入方式，转换为一个固定长度的向量或表示形式。）&lt;/p&gt;
&lt;p&gt;就是给每个词向量乘个权重加起来，被称作注意力池化（Attention Pooling）或加权求和（Weighted Sum）。这个操作的含义是将注意力权重分配给输入序列中的不同部分，从而形成一个汇聚了注意力的向量表示。&lt;/p&gt;
&lt;p&gt;这个操作的效果是聚焦于输入序列中具有更高注意力权重的部分，形成一个综合的表示，其中对于重要的部分有更大的贡献。这对于处理序列数据中的上下文信息，关注重要元素，以及实现对不同部分不同程度的关注都非常有用，特别是在自然语言处理中的任务中。&lt;/p&gt;
&lt;h2 id=&#34;预训练语言模型&#34;&gt;预训练语言模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705278999.png&#34;
	width=&#34;2297&#34;
	height=&#34;1405&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705278999_hud4622f7d90fed570e77657ded2a3cd9c_354027_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705278999_hud4622f7d90fed570e77657ded2a3cd9c_354027_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705278999&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;163&#34;
		data-flex-basis=&#34;392px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;迁移学习&#34;&gt;迁移学习&lt;/h4&gt;
&lt;p&gt;迁移学习（Transfer Learning）是一种机器学习方法，其核心思想是利用已有的知识来辅助学习新的知识。例如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#。&lt;/p&gt;
&lt;p&gt;迁移学习通常会关注有一个源域（源任务） $D_ {s}$ 和一个目标域（目标任务） $D_ {t}$ 的情况.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705772371.png&#34;
	width=&#34;2055&#34;
	height=&#34;1303&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705772371_hu952cf0c1c166f5c036a8794c6ac9f45c_301052_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705772371_hu952cf0c1c166f5c036a8794c6ac9f45c_301052_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705772371&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;迁移方式分为两种&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705819455.png&#34;
	width=&#34;1753&#34;
	height=&#34;1187&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705819455_hu6149cf4641a3b58627bfdae581715e2c_279569_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705819455_hu6149cf4641a3b58627bfdae581715e2c_279569_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705819455&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;354px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;几个范式&#34;&gt;几个范式&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782526810.png&#34;
	width=&#34;991&#34;
	height=&#34;363&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782526810_huf78a8faa670031a1b1d10cefe3bbc82b_68463_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782526810_huf78a8faa670031a1b1d10cefe3bbc82b_68463_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704782526810&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;第三范式预训练-精调范式&#34;&gt;第三范式：预训练-精调范式&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782261195.png&#34;
	width=&#34;1617&#34;
	height=&#34;1219&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782261195_hu457ea54a8c6c1b67e863bb3c12d05a13_184214_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782261195_hu457ea54a8c6c1b67e863bb3c12d05a13_184214_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704782261195&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;318px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704785636077.png&#34;
	width=&#34;1575&#34;
	height=&#34;1011&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704785636077_huc3b84d882377334c59bff61ed0c845ca_155159_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704785636077_huc3b84d882377334c59bff61ed0c845ca_155159_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704785636077&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;373px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;自回归：预测序列的下一个或者上一个&lt;/p&gt;
&lt;p&gt;自编码：预测序列中的某一个或某几个&lt;/p&gt;
&lt;p&gt;广义自回归：和自回归主要区别在于他们处理输入数据的方式。自回归预训练语言模型在生成序列时，会一个接一个地生成新的词，每个新词都依赖于前面的词。如GPT，而广义自回归预训练语言模型则更为灵活，它们可以在生成序列时考虑更多的上下文信息，模型不仅可以查看前面的词，还可以查看后面的词或者整个序列。如XLNet&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704787153210.png&#34;
	width=&#34;1881&#34;
	height=&#34;1367&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704787153210_hue088a08a54bd4f6558737409f39d3643_308771_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704787153210_hue088a08a54bd4f6558737409f39d3643_308771_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704787153210&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;137&#34;
		data-flex-basis=&#34;330px&#34;
	
&gt;&lt;/p&gt;
&lt;h6 id=&#34;gpt训练和对接&#34;&gt;GPT训练和对接&lt;/h6&gt;
&lt;p&gt;GPT 采用了 Transformer 的 Decoder 部分，并且每个子层只有一个 Masked Multi Self-Attention（768 维向量和 12 个 Attention Head）和一个FeedForward （无普通transformer解码器层的编码器-解码器注意力子层），模型共叠加使用了 12 层的 Decoder。使用了从左向右的单向注意力机制&lt;/p&gt;
&lt;p&gt;Masked Multi Self-Attention的768维向量和12个attention head： 意思是12个独立的attention组件，每个组件的参数都独立，然后每个attention的Q向量都是768维，也可以理解为一个词在模型中的向量（或者说词嵌入）是768维]&lt;/p&gt;
&lt;p&gt;feedforward： 作用是提取更深层次的特征。在每个序列的位置单独应用一个全连接前馈网络，由两个线性层和一个激活函数组成。线性层将每个位置的表示扩展，为学习更复杂的特征提供可能性，激活函数帮助模型学习更复杂的非线性特征，第二个线性层将每个位置的表示压缩回原始维度。这样，位置特征敏感的部分就会被表达出来，提供给后续网络学习。&lt;/p&gt;
&lt;p&gt;就是十二个下图这样的小东西&lt;/p&gt;
&lt;p&gt;transformer输入有token embedding和position embedding&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704860957530.png&#34;
	width=&#34;1705&#34;
	height=&#34;653&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704860957530_hue934b2eb189b67c8657644a9da984850_167026_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704860957530_hue934b2eb189b67c8657644a9da984850_167026_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704860957530&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;261&#34;
		data-flex-basis=&#34;626px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对比一下transformer，transformer的decoder是6个右边的，少了一层multi-head attention的encoder-decoder注意力子层（cross-attention的那个子模块）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704861830326.png&#34;
	width=&#34;1745&#34;
	height=&#34;1215&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704861830326_hu049414e348bbe04fee23ac5e74f1154e_253300_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704861830326_hu049414e348bbe04fee23ac5e74f1154e_253300_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704861830326&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;344px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;6层attention堆叠就是六个encoder就是个小的encoder，每个encoder里都有attention机制，上图N=6的意思。&lt;/p&gt;
&lt;p&gt;训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865198754.png&#34;
	width=&#34;1627&#34;
	height=&#34;1151&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865198754_huc9f901670f75975da6ab25768f162f67_198156_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865198754_huc9f901670f75975da6ab25768f162f67_198156_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865198754&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;maximize负数=近0最小化&lt;/p&gt;
&lt;p&gt;与下游任务对接：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865491544.png&#34;
	width=&#34;1715&#34;
	height=&#34;1093&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865491544_hu1bcafc58ab919685f50c44042844f975_270185_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865491544_hu1bcafc58ab919685f50c44042844f975_270185_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865491544&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;376px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;把多序列通过一些特定的规则拼成一个单序列。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865594321.png&#34;
	width=&#34;1645&#34;
	height=&#34;1097&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865594321_hue0d73b81ba9d0d293957bd147d0afd38_275837_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865594321_hue0d73b81ba9d0d293957bd147d0afd38_275837_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865594321&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;微调：&lt;/p&gt;
&lt;p&gt;任务微调有2种方式 ：① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务&lt;/p&gt;
&lt;p&gt;举例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866344121.png&#34;
	width=&#34;1631&#34;
	height=&#34;777&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866344121_hu296dbf4b80b31124e7c737407c8a6e22_192747_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866344121_hu296dbf4b80b31124e7c737407c8a6e22_192747_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866344121&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;503px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866727437.png&#34;
	width=&#34;1667&#34;
	height=&#34;1263&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866727437_hu249ea5bcdb35cd3728011052098ccf33_286514_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866727437_hu249ea5bcdb35cd3728011052098ccf33_286514_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866727437&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;131&#34;
		data-flex-basis=&#34;316px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这儿的$L_1(C)$是上面提到的预训练过程中的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866802266.png&#34;
	width=&#34;1043&#34;
	height=&#34;247&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866802266_hu6532fe188807fa299274ec1ed5d71977_25845_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866802266_hu6532fe188807fa299274ec1ed5d71977_25845_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866802266&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;422&#34;
		data-flex-basis=&#34;1013px&#34;
	
&gt;&lt;/p&gt;
&lt;h6 id=&#34;bert训练和对接&#34;&gt;BERT训练和对接&lt;/h6&gt;
&lt;p&gt;用了transformer的encoder再加FFN（前馈神经网络，FFN 层有助于学习序列中的非线性关系和模式）层&lt;/p&gt;
&lt;p&gt;【但是transformer的encoder不是带FeedForward吗？】FFN仅在MLM过程中有用，而BERT的最终输出是模型在整个预训练过程中学到的表示的某种组合。这些表示在后续的任务中可以进一步微调或者用作特征。（BYD，原来只是训练过程中的一个b东西）&lt;/p&gt;
&lt;p&gt;下图中一个trm是一个子层，&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867651617.png&#34;
	width=&#34;1699&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867651617_hua7527c2df1238a886b444e812f0c1e96_204334_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867651617_hua7527c2df1238a886b444e812f0c1e96_204334_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704867651617&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867919696.png&#34;
	width=&#34;1743&#34;
	height=&#34;1183&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867919696_hu2cc5f6a6b601a6084d8eb8148c67d6d0_599721_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867919696_hu2cc5f6a6b601a6084d8eb8148c67d6d0_599721_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704867919696&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在BERT模型中，输入的每个单词都会通过三种嵌入（embedding）进行编码&lt;/p&gt;
&lt;p&gt;Token Embedding：是将每个单词或者词片映射到一个向量，这个向量能够捕获该单词的语义信息。在BERT中，使用了WordPiece标记化，其中输入句子的每个单词都被分解成子词标记。这些标记的嵌入是随机初始化的，然后通过梯度下降进行训练。&lt;/p&gt;
&lt;p&gt;Segment Embedding：是用来区分不同的句子的。在处理两个句子的任务（如自然语言推理）时，BERT需要知道每个单词属于哪个句子。&lt;/p&gt;
&lt;p&gt;Position Embedding：由于Transformer模型并没有像循环神经网络那样的顺序性，因此需要显式地向模型添加位置信息，以保留句子中单词的顺序信息&lt;/p&gt;
&lt;p&gt;训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871798829.png&#34;
	width=&#34;1501&#34;
	height=&#34;1293&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871798829_hu485778d7108148b664f10e4f5b6fd15e_297142_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871798829_hu485778d7108148b664f10e4f5b6fd15e_297142_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704871798829&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;116&#34;
		data-flex-basis=&#34;278px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;MLM：把一个序列的几个word给mask了让模型猜的训练方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880632493.png&#34;
	width=&#34;1733&#34;
	height=&#34;1355&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880632493_hu492af94dad6d71d7ce6bc4e88c43c315_597930_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880632493_hu492af94dad6d71d7ce6bc4e88c43c315_597930_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880632493&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;(2).句子顺序模型训练&lt;/p&gt;
&lt;p&gt;凑一些下一句不是下一句的负样本来训练预训练模型对句子顺序的敏感。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871959170.png&#34;
	width=&#34;1653&#34;
	height=&#34;1143&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871959170_hue79886b94b13aa62f610264793d6000d_344052_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871959170_hue79886b94b13aa62f610264793d6000d_344052_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704871959170&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对接：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704872111654.png&#34;
	width=&#34;1641&#34;
	height=&#34;1135&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704872111654_hu78424a4ce723d893d68accfa1231a5da_722981_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704872111654_hu78424a4ce723d893d68accfa1231a5da_722981_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704872111654&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;微调同样有两种① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务&lt;/p&gt;
&lt;h6 id=&#34;其他&#34;&gt;其他&lt;/h6&gt;
&lt;p&gt;RoBERTa：把BERT使用Adam默认的参数改为使用更大的batches，训练时把静态mask改为动态mask。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704873163103.png&#34;
	width=&#34;1581&#34;
	height=&#34;451&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704873163103_hu1ed9f4552d0e22436fd67784347304c8_145739_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704873163103_hu1ed9f4552d0e22436fd67784347304c8_145739_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704873163103&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;350&#34;
		data-flex-basis=&#34;841px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;BART：GPT只用了transformer的decoder，BERT只用了transformer的encoder。导致&lt;/p&gt;
&lt;p&gt;BERT具备双向语言理解能力的却不具备做生成任务的能力。GPT拥有自回归特性的却不能更好的从双向理解语言.&lt;/p&gt;
&lt;p&gt;（模型的&amp;quot;自回归&amp;quot;特性指的是，当前的观察值是过去观察值的加权平均和一个随机项）&lt;/p&gt;
&lt;p&gt;BART使用标准的Transformer结构为基础，吸纳BERT和GPT的优点，使用&lt;strong&gt;多种噪声破坏原文本&lt;/strong&gt;，再将残缺文本通过序列到序列的任务重新复原（降噪自监督）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880759669.png&#34;
	width=&#34;1643&#34;
	height=&#34;937&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880759669_hu6d8063b693fecb36b2439b592325dd92_191132_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880759669_hu6d8063b693fecb36b2439b592325dd92_191132_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880759669&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;BERT在预测时加了额外的FFN, 而BART没使用FFN.&lt;/p&gt;
&lt;p&gt;（还记得这个Beyond吗）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880695071.png&#34;
	width=&#34;783&#34;
	height=&#34;565&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880695071_huc22aad66c4fcd1dea86fed7ef841b8d4_115785_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880695071_huc22aad66c4fcd1dea86fed7ef841b8d4_115785_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880695071&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;T5&lt;/p&gt;
&lt;p&gt;给整个 NLP 预训练模型领域提供了一个通用框架，把所有NLP任务都转化成一种形式(Text-to-Text)，通过这样的方式可以用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务。以后的各种NLP任务，只需针对一个超大预训练模型，考虑怎么把任转换成合适的文本输入输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881664053.png&#34;
	width=&#34;1737&#34;
	height=&#34;1085&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881664053_hu80eb1ed700ec8ec7266cc6c56c7669e3_472562_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881664053_hu80eb1ed700ec8ec7266cc6c56c7669e3_472562_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881664053&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881694268.png&#34;
	width=&#34;885&#34;
	height=&#34;581&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881694268_huc9d0ed5830b97c4772f19726a530ab61_60117_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881694268_huc9d0ed5830b97c4772f19726a530ab61_60117_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881694268&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881712622.png&#34;
	width=&#34;1377&#34;
	height=&#34;1167&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881712622_hudb13584b37659e21a5afc68a18f9e371_435694_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881712622_hudb13584b37659e21a5afc68a18f9e371_435694_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881712622&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;283px&#34;
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;第四范式预训练提示预测范式pre-trainpromptpredict&#34;&gt;第四范式：预训练，提示，预测范式（Pre-train,Prompt,Predict）&lt;/h5&gt;
&lt;p&gt;prompt挖掘工程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881803249.png&#34;
	width=&#34;1213&#34;
	height=&#34;457&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881803249_huec70659cc7294e1f2a848b7a828059b7_128342_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881803249_huec70659cc7294e1f2a848b7a828059b7_128342_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881803249&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;265&#34;
		data-flex-basis=&#34;637px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;特点：不通过目标工程使预训练的语言模型（LM）适应下游任务，而是将下游任务建模的方式重新定义（Reformulate），通过利用合适prompt实现不对预训练语言模型改动太多，尽量在原始 LM上解决任务的问题。&lt;/p&gt;
&lt;p&gt;实现方法eg：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882346953.png&#34;
	width=&#34;1153&#34;
	height=&#34;903&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882346953_hu6b81862353a609877186ebe4d37ec8a9_117407_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882346953_hu6b81862353a609877186ebe4d37ec8a9_117407_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882346953&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882361576.png&#34;
	width=&#34;1551&#34;
	height=&#34;1067&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882361576_hub8a52e1444284fe4dbf9d3b4b945295b_295097_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882361576_hub8a52e1444284fe4dbf9d3b4b945295b_295097_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882361576&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882378196.png&#34;
	width=&#34;1743&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882378196_hu06bd0fe6ede19bf12ad2b9eb8d89a22e_242114_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882378196_hu06bd0fe6ede19bf12ad2b9eb8d89a22e_242114_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882378196&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882674432.png&#34;
	width=&#34;1663&#34;
	height=&#34;1101&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882674432_hu3176c64ab64e05146ed26b8c2de7791d_304025_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882674432_hu3176c64ab64e05146ed26b8c2de7791d_304025_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882674432&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;362px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882723263.png&#34;
	width=&#34;1499&#34;
	height=&#34;1165&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882723263_hu64fbe9a5af865fb2c7fe45a55cbf1f0d_175531_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882723263_hu64fbe9a5af865fb2c7fe45a55cbf1f0d_175531_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882723263&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;308px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;要素：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882803241.png&#34;
	width=&#34;1605&#34;
	height=&#34;1131&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882803241_hud2c32c41a9ce27f2a002d6db6328b951_173622_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882803241_hud2c32c41a9ce27f2a002d6db6328b951_173622_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882803241&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;输入端&lt;/p&gt;
&lt;p&gt;prompt工程&lt;/p&gt;
&lt;p&gt;完形填空和前缀提示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882845802.png&#34;
	width=&#34;1663&#34;
	height=&#34;1033&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882845802_huc30f2bfc4926a5fa64376289a2c313f6_191031_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882845802_huc30f2bfc4926a5fa64376289a2c313f6_191031_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882845802&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;386px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882862203.png&#34;
	width=&#34;1575&#34;
	height=&#34;935&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882862203_huf6fe022d19ac2f5b2a150aa283ee65ef_155740_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882862203_huf6fe022d19ac2f5b2a150aa283ee65ef_155740_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882862203&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;模板创建&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883318434.png&#34;
	width=&#34;1649&#34;
	height=&#34;1057&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883318434_hud563219f773de1073ce86470336fb540_217817_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883318434_hud563219f773de1073ce86470336fb540_217817_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883318434&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;输出端&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883343584.png&#34;
	width=&#34;1501&#34;
	height=&#34;961&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883343584_hud560b9cad10d75572c2f929209bd692d_146556_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883343584_hud560b9cad10d75572c2f929209bd692d_146556_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883343584&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883381023.png&#34;
	width=&#34;1511&#34;
	height=&#34;493&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883381023_hu6020a68b41bea0676dee21906c4c1d18_74663_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883381023_hu6020a68b41bea0676dee21906c4c1d18_74663_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883381023&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;306&#34;
		data-flex-basis=&#34;735px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;微调&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883499101.png&#34;
	width=&#34;1721&#34;
	height=&#34;721&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883499101_hu6c82e1ff8c5f6b5270a03c6eeb47500f_133780_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883499101_hu6c82e1ff8c5f6b5270a03c6eeb47500f_133780_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883499101&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;572px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;生成类任务用法与第五范式相同&lt;/p&gt;
&lt;h5 id=&#34;第五范式大模型&#34;&gt;第五范式：大模型&lt;/h5&gt;
&lt;p&gt;大语言模型 (Large Language Model，LLM) 通常指由大量参数（通常数十亿个权重或更多）组成的人工神经网络预训练语言模型，使用大量的计算资源在海量数据上进行训练。&lt;/p&gt;
&lt;p&gt;大型语言模型是通用的模型，在广泛的任务（例如情感分析、命名实体识别或数学推理）中表现出色，具有与人类认证对齐的特点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890488901.png&#34;
	width=&#34;1745&#34;
	height=&#34;1177&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890488901_hu9e8c1dd2496fa51c7dbd72032c289b3d_266220_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890488901_hu9e8c1dd2496fa51c7dbd72032c289b3d_266220_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704890488901&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;355px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890734795.png&#34;
	width=&#34;511&#34;
	height=&#34;1397&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890734795_hu35e6e266a351d58373a5c4677c6feec7_314016_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890734795_hu35e6e266a351d58373a5c4677c6feec7_314016_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704890734795&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;36&#34;
		data-flex-basis=&#34;87px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891371789.png&#34;
	width=&#34;1031&#34;
	height=&#34;367&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891371789_hu3fe56bb7a76f889260c7cab23ecc3d28_35836_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891371789_hu3fe56bb7a76f889260c7cab23ecc3d28_35836_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891371789&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;280&#34;
		data-flex-basis=&#34;674px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891495297.png&#34;
	width=&#34;1289&#34;
	height=&#34;689&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891495297_hu3487599bc5b9b45eb25f8aeb4443638c_95444_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891495297_hu3487599bc5b9b45eb25f8aeb4443638c_95444_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891495297&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;187&#34;
		data-flex-basis=&#34;448px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891515801.png&#34;
	width=&#34;1071&#34;
	height=&#34;381&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891515801_hu0ebf6b2a3aac7ca1a12bc6233772836c_37895_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891515801_hu0ebf6b2a3aac7ca1a12bc6233772836c_37895_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891515801&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;281&#34;
		data-flex-basis=&#34;674px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891548168.png&#34;
	width=&#34;1689&#34;
	height=&#34;1053&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891548168_hu94b8b583905674468cabedf264193844_159786_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891548168_hu94b8b583905674468cabedf264193844_159786_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891548168&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;不需要任务模型的意思是只要有预训练就行&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892005089.png&#34;
	width=&#34;1367&#34;
	height=&#34;1053&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892005089_hu84b1da5607bb96a9b6f47c4059cf98a7_487410_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892005089_hu84b1da5607bb96a9b6f47c4059cf98a7_487410_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892005089&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;311px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892027838.png&#34;
	width=&#34;1331&#34;
	height=&#34;759&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892027838_hu9b6a2e9efeaa1db1356cc0bf2bd7fe29_223527_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892027838_hu9b6a2e9efeaa1db1356cc0bf2bd7fe29_223527_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892027838&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;（我靠，这要传统注意力算死了）&lt;/p&gt;
&lt;p&gt;学习方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892066074.png&#34;
	width=&#34;1807&#34;
	height=&#34;1279&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892066074_hu353c2ac5f3e4f633b2482bc2cfec1659_461646_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892066074_hu353c2ac5f3e4f633b2482bc2cfec1659_461646_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892066074&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因为上下文学习，在使用的时候也可以用zero-shot, one-shot和few-shot。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892306373.png&#34;
	width=&#34;1701&#34;
	height=&#34;1129&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892306373_hu1a6f3d60dc6d58156d756e6f7f59ffc3_235265_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892306373_hu1a6f3d60dc6d58156d756e6f7f59ffc3_235265_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892306373&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892321288.png&#34;
	width=&#34;1479&#34;
	height=&#34;593&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892321288_hu0c10ab9456cdc634e1208f8eede43e88_108443_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892321288_hu0c10ab9456cdc634e1208f8eede43e88_108443_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892321288&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;598px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;chain-of-thought&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892359407.png&#34;
	width=&#34;1833&#34;
	height=&#34;1243&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892359407_hua65faf25c166dadbd34f6fe788a74bb9_613820_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892359407_hua65faf25c166dadbd34f6fe788a74bb9_613820_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892359407&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892375228.png&#34;
	width=&#34;1669&#34;
	height=&#34;887&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892375228_hudc9612e91634f51d8a3336bc1086bd2e_156420_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892375228_hudc9612e91634f51d8a3336bc1086bd2e_156420_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892375228&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892402637.png&#34;
	width=&#34;1693&#34;
	height=&#34;1069&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892402637_hu9b3cc0ef739de860e2922cae685e22ef_260089_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892402637_hu9b3cc0ef739de860e2922cae685e22ef_260089_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892402637&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892464014.png&#34;
	width=&#34;1571&#34;
	height=&#34;945&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892464014_hufb347991d3d69c06e58af99e28454cec_221361_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892464014_hufb347991d3d69c06e58af99e28454cec_221361_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892464014&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;398px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892496826.png&#34;
	width=&#34;1663&#34;
	height=&#34;1063&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892496826_hu8b7c4df7a6573edc7b8c92608fe1e76a_375044_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892496826_hu8b7c4df7a6573edc7b8c92608fe1e76a_375044_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892496826&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;375px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;与人类对齐：RLHF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892569477.png&#34;
	width=&#34;1573&#34;
	height=&#34;993&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892569477_huaed993029c76faca4567edb9f03720ff_432801_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892569477_huaed993029c76faca4567edb9f03720ff_432801_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892569477&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;简而言之：1、在人工标注数据上SFT（有监督微调）模型&lt;/p&gt;
&lt;p&gt;2、多模型给标注人员做排序，用来训练奖励模型（RM）&lt;/p&gt;
&lt;p&gt;3、使用强化学习PPO算法，交互地优化模型参数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894207867.png&#34;
	width=&#34;1399&#34;
	height=&#34;759&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894207867_hua35d1d2b7fa3716ffca445794fb967e9_85028_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894207867_hua35d1d2b7fa3716ffca445794fb967e9_85028_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894207867&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;184&#34;
		data-flex-basis=&#34;442px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;文本分类在各个范式上的例子&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894268859.png&#34;
	width=&#34;1503&#34;
	height=&#34;1237&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894268859_hu2bb283b0724e143993ef29e8bb0f1f62_397643_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894268859_hu2bb283b0724e143993ef29e8bb0f1f62_397643_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894268859&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;121&#34;
		data-flex-basis=&#34;291px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894283289.png&#34;
	width=&#34;1597&#34;
	height=&#34;1147&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894283289_hu140aaa084e8d45f8bd92482b55a27a70_198404_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894283289_hu140aaa084e8d45f8bd92482b55a27a70_198404_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894283289&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;334px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894294431.png&#34;
	width=&#34;1573&#34;
	height=&#34;1239&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894294431_hud4620f7e892ea82c3fc2c75fa0ca0c3e_352702_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894294431_hud4620f7e892ea82c3fc2c75fa0ca0c3e_352702_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894294431&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;304px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894305898.png&#34;
	width=&#34;1519&#34;
	height=&#34;1217&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894305898_huc6e53b443c0f7048e5977016f29a9ec4_436441_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894305898_huc6e53b443c0f7048e5977016f29a9ec4_436441_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894305898&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;299px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894318376.png&#34;
	width=&#34;1537&#34;
	height=&#34;1197&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894318376_hu54676de5e35a5b05b14ccfd62c2f890a_452874_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894318376_hu54676de5e35a5b05b14ccfd62c2f890a_452874_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894318376&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;308px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894335472.png&#34;
	width=&#34;1569&#34;
	height=&#34;1177&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894335472_hu0209de2fd796e09a95e369fad98008ce_408080_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894335472_hu0209de2fd796e09a95e369fad98008ce_408080_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894335472&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;319px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894354621.png&#34;
	width=&#34;1707&#34;
	height=&#34;1257&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894354621_hu587cc2306dba8a43e1c8b44fd7e44435_436098_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894354621_hu587cc2306dba8a43e1c8b44fd7e44435_436098_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894354621&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;方面级情感分类&#34;&gt;方面级情感分类&lt;/h2&gt;
&lt;p&gt;方面级情感分类（Aspect-Level Sentiment Classification）是自然语言处理（NLP）中的一个任务，它的目标是识别文本中特定方面的情感倾向。例如，在产品评论中，“这款手机的电池寿命很长，但屏幕质量差。”这句话中，“电池寿命”这个方面的情感是积极的，而“屏幕质量”这个方面的情感是消极的。所以，方面级情感分类不仅要识别出文本中的各个方面，还要判断这些方面的情感倾向。这个任务在许多领域都有应用，比如产品评论分析、社交媒体监控等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946258919.png&#34;
	width=&#34;1121&#34;
	height=&#34;609&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946258919_hu721964277a2387fed13eb85f981a0d94_88087_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946258919_hu721964277a2387fed13eb85f981a0d94_88087_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946258919&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;184&#34;
		data-flex-basis=&#34;441px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946290972.png&#34;
	width=&#34;1717&#34;
	height=&#34;1197&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946290972_huf21f3cb444ca9798f670aed8a12a0923_260909_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946290972_huf21f3cb444ca9798f670aed8a12a0923_260909_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946290972&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;344px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;问题定义&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946620060.png&#34;
	width=&#34;1741&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946620060_hu00fead3232e555cefcd8ef1d46c6339a_193297_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946620060_hu00fead3232e555cefcd8ef1d46c6339a_193297_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946620060&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946709801.png&#34;
	width=&#34;1605&#34;
	height=&#34;1001&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946709801_hu276a0f4566f0d1e86e0004c37735e0cc_152701_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946709801_hu276a0f4566f0d1e86e0004c37735e0cc_152701_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946709801&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946746917.png&#34;
	width=&#34;1601&#34;
	height=&#34;1029&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946746917_hub7f71c671d488f23e426fb20fb8f94a5_117904_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946746917_hub7f71c671d488f23e426fb20fb8f94a5_117904_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946746917&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;373px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;基本方法原理&#34;&gt;基本方法、原理&lt;/h3&gt;
&lt;p&gt;子任务等：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946917658.png&#34;
	width=&#34;1655&#34;
	height=&#34;1089&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946917658_hu6d99ef69b1ccabe099f11b083138162a_204719_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946917658_hu6d99ef69b1ccabe099f11b083138162a_204719_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946917658&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;364px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity/Target&lt;/strong&gt;：评论的对象或者物品是什么，例如某个餐厅，某款手机。&amp;ldquo;Target&amp;quot;这个词用的比较模糊，其既可以被当作Entity，又可以当作Aspect Term。和在AE里提到的opinion target是一个意思。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Aspect&lt;/strong&gt;：隶属于某个Entity的属性。在这里其因为学者提出的任务类型不同，又分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Aspect Term&lt;/strong&gt;：存在在句子中的Aspect。例如例句中的”拍照“、”电池“、”外观“。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aspect Category&lt;/strong&gt;：预先给定的Aspect。例如，我们想知道评论对”华为手机“的”外观“、”售后服务“、”便携性“三个aspect的情感极性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;lstm&#34;&gt;LSTM&lt;/h4&gt;
&lt;p&gt;LSTM 方法先将所有变长的句子均表示为一种固定长度的向量，具体做法是将最后一个word对应的计算得到的 hidden vector 作为整句话的表示（sentence vector）。之后，将最后得到的这个 sentence vector 送入一个 linear layer，使其输出为一个维度为情绪种类个数。最后对 linear layer 得出的结果做 softmax 并依次为依据选出该句（同时也是 target）的情绪分类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968549919.png&#34;
	width=&#34;1903&#34;
	height=&#34;489&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968549919_hud9a73b5a536821a1b52737d98b28ef37_170701_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968549919_hud9a73b5a536821a1b52737d98b28ef37_170701_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704968549919&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;389&#34;
		data-flex-basis=&#34;933px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;td-lstm&#34;&gt;TD-LSTM&lt;/h4&gt;
&lt;p&gt;将输入的句子根据 aspect 分为两部分，两边都朝着 aspect 的方向分别同时把 words 送入两个 LSTM 中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968574703.png&#34;
	width=&#34;1915&#34;
	height=&#34;593&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968574703_hue9297df74f3d7584dae556d5b4dcd8cc_205924_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968574703_hue9297df74f3d7584dae556d5b4dcd8cc_205924_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704968574703&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;322&#34;
		data-flex-basis=&#34;775px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;tc-lstm&#34;&gt;TC-LSTM&lt;/h4&gt;
&lt;p&gt;与 TD-LSTM 唯一的不同就是在 input 时在每个 word embedding vector 后面拼接上 aspect vector（如果 aspect 中有多个 word，则取平均）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704963182156.png&#34;
	width=&#34;1881&#34;
	height=&#34;757&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704963182156_hu9a3a7fad140b26b2edbda070afd86fdb_166849_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704963182156_hu9a3a7fad140b26b2edbda070afd86fdb_166849_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;img&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;248&#34;
		data-flex-basis=&#34;596px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;at-lstm&#34;&gt;AT-LSTM&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970432154.png&#34;
	width=&#34;1589&#34;
	height=&#34;533&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970432154_hu47b031dbfd98ff7c7043ae7e9aff38b5_128490_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970432154_hu47b031dbfd98ff7c7043ae7e9aff38b5_128490_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704970432154&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;298&#34;
		data-flex-basis=&#34;715px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对隐藏状态h和aspect的词嵌入后施加attention&lt;/p&gt;
&lt;h4 id=&#34;atae-lstm&#34;&gt;ATAE-LSTM&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970484908.png&#34;
	width=&#34;1559&#34;
	height=&#34;613&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970484908_hua53715cfc1514b436ce6aef4e01435dc_164199_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970484908_hua53715cfc1514b436ce6aef4e01435dc_164199_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704970484908&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;610px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在LSTM的输入方面在concat一个aspect的词向量，说明aspect的重要性&lt;/p&gt;
&lt;h4 id=&#34;ian&#34;&gt;IAN&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704971332861.png&#34;
	width=&#34;1535&#34;
	height=&#34;1061&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704971332861_hu7943492d282972d3b8bd99e10e705f48_347349_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704971332861_hu7943492d282972d3b8bd99e10e705f48_347349_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704971332861&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;IAN 模型由两部分组成，两部分分别对 Target 和 Context 进行建模。每一部分都以词嵌入作为输入，再通过 LSTM 获取每个词的隐藏状态，最后取所有隐藏向量的平均值，用它来监督另一部分注意力向量的生成。attention学习隐藏状态和对应词向量序列的相关性。&lt;/p&gt;
&lt;p&gt;attention部分是$h_t^i$&amp;amp;$avg(h_c)$在target上做注意力，$h_c^i$&amp;amp;$avg(h_t)$在context上做注意力&lt;/p&gt;
&lt;h2 id=&#34;实体和关系联合抽取&#34;&gt;实体和关系联合抽取&lt;/h2&gt;
&lt;p&gt;信息抽取：从自然语言文本中抽取指定类型的实体、 关系、 事件等事实信息，并形成结构化数据输出的文本处理技术。一般情况下信息抽取别是知识抽取等其他任务的基础。主要在对无结构数据的抽取出现问题&lt;/p&gt;
&lt;h3 id=&#34;基本方法原理-1&#34;&gt;基本方法原理&lt;/h3&gt;
&lt;h4 id=&#34;名词解释&#34;&gt;名词解释&lt;/h4&gt;
&lt;p&gt;span：指的是文本中的一段连续的子串，这段子串对应于某个&lt;em&gt;&lt;strong&gt;实体&lt;/strong&gt;&lt;/em&gt;或者&lt;em&gt;&lt;strong&gt;关系&lt;/strong&gt;&lt;/em&gt;的具体文本表述。&lt;/p&gt;
&lt;h4 id=&#34;dygie&#34;&gt;DyGIE&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704981462026.png&#34;
	width=&#34;1761&#34;
	height=&#34;1223&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704981462026_hu5be03809e705684624ae1362282b5f39_993279_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704981462026_hu5be03809e705684624ae1362282b5f39_993279_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704981462026&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;345px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;问题定义：&lt;/p&gt;
&lt;p&gt;输入：所有句中可能的spans序列集合。&lt;/p&gt;
&lt;p&gt;输出三种信息：实体类型，关系分类（同一句），指代链接（跨句）；&lt;/p&gt;
&lt;p&gt;Token Representation Layer（Token表示层）：BiLSTM&lt;/p&gt;
&lt;p&gt;Span Representation Layer（span表示层）： 初始化来自BiLSTM输出联合起来，加入基于注意力模型。&lt;/p&gt;
&lt;p&gt;Coreference Propagation Layer（指代传播层）：N次传播处理，跨span共享上下文信息&lt;/p&gt;
&lt;p&gt;Relation Propagation Layer（关系传播层）：与指代传播层相似&lt;/p&gt;
&lt;p&gt;Final Prediction Layer（最终预测层）：去预测任务—实体任务，关系任务&lt;/p&gt;
&lt;h4 id=&#34;oneie&#34;&gt;OneIE&lt;/h4&gt;
&lt;p&gt;任务定义：给定一个输入的句子，输出一个图，图中节点(含节点类型)代表实体提及或者触发词，图中的边表示表示节点之间的关系&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982012782.png&#34;
	width=&#34;1903&#34;
	height=&#34;1313&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982012782_hu2a3e038bd7b8c5ccf62475c5760823aa_781316_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982012782_hu2a3e038bd7b8c5ccf62475c5760823aa_781316_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982012782&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982419293.png&#34;
	width=&#34;1873&#34;
	height=&#34;733&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982419293_hub52192bd546dcfcb8419574068427da6_659488_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982419293_hub52192bd546dcfcb8419574068427da6_659488_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982419293&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;613px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;条件随机场（Conditional Random Field，CRF）是一种在自然语言处理（NLP）中广泛使用的模型。CRF的主要作用是解决序列数据的标注问题，它能够考虑整个序列的上下文信息，以做出更准确的预测。&lt;/p&gt;
&lt;p&gt;Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。Beam Search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。&lt;/p&gt;
&lt;p&gt;在这里只保留最好的&lt;/p&gt;
&lt;h4 id=&#34;uie&#34;&gt;UIE&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982684822.png&#34;
	width=&#34;1471&#34;
	height=&#34;1295&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982684822_hu57e59458cbe67fce794a865f797b077e_742961_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982684822_hu57e59458cbe67fce794a865f797b077e_742961_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982684822&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;113&#34;
		data-flex-basis=&#34;272px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982702205.png&#34;
	width=&#34;1847&#34;
	height=&#34;1305&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982702205_hu1fad7a85f4248ea0ee59664a8035cd8b_804779_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982702205_hu1fad7a85f4248ea0ee59664a8035cd8b_804779_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982702205&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;uniex&#34;&gt;UniEX&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982730929.png&#34;
	width=&#34;1771&#34;
	height=&#34;1089&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982730929_huea6798c42d72796be18490fea6777b41_584928_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982730929_huea6798c42d72796be18490fea6777b41_584928_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982730929&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982749870.png&#34;
	width=&#34;1877&#34;
	height=&#34;1129&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982749870_hu22c32797ea61940160b16956019671a6_1034476_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982749870_hu22c32797ea61940160b16956019671a6_1034476_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982749870&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;399px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;检索式问答系统&#34;&gt;检索式问答系统&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210507003.png&#34;
	width=&#34;1619&#34;
	height=&#34;1114&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210507003_hue833c30ef04dde08e50b11dbe387c6f0_349495_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210507003_hue833c30ef04dde08e50b11dbe387c6f0_349495_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210507003&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;1、问题分析模块：问题分类和关键词提取&lt;/p&gt;
&lt;p&gt;问题分类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210654986.png&#34;
	width=&#34;1467&#34;
	height=&#34;755&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210654986_hu4587f1b62fc0f9d175ca7b1611d1fb1b_100041_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210654986_hu4587f1b62fc0f9d175ca7b1611d1fb1b_100041_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210654986&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;466px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;关键词提取：根据问题分类，用&lt;strong&gt;序列标注法&lt;/strong&gt;抽取相应类别的&lt;strong&gt;实体&lt;/strong&gt;做为检索关键词&lt;/p&gt;
&lt;p&gt;2、检索模块：检索问题答案所在文档与段落&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210788393.png&#34;
	width=&#34;1750&#34;
	height=&#34;819&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210788393_hu4a438cfdee76a8f47e4cef7b56b80364_201074_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210788393_hu4a438cfdee76a8f47e4cef7b56b80364_201074_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210788393&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;213&#34;
		data-flex-basis=&#34;512px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;3、 答案抽取模块：在相关片段中抽取备选答案，并对备选答案进行排序&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210839822.png&#34;
	width=&#34;1441&#34;
	height=&#34;793&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210839822_hu4272d4315748ca0554502088ffc74a19_158222_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210839822_hu4272d4315748ca0554502088ffc74a19_158222_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210839822&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;436px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;实现方法：&lt;/p&gt;
&lt;h3 id=&#34;流水线方式&#34;&gt;流水线方式&lt;/h3&gt;
&lt;p&gt;Document Retriever + Reading Comprehension Reader框架&lt;/p&gt;
&lt;h4 id=&#34;drqa&#34;&gt;DrQA&lt;/h4&gt;
&lt;p&gt;TF-IDF：（Term Frequency-Inverse Document Frequency，词频-逆文件频率）是一种用于信息检索和数据挖掘的常用加权技术。它是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210960856.png&#34;
	width=&#34;1655&#34;
	height=&#34;648&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210960856_hu5e73f05ab4873a7f2dd19e1e4708c347_149320_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210960856_hu5e73f05ab4873a7f2dd19e1e4708c347_149320_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210960856&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;612px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用TF-IDF获取与问题topK相关的文档&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210980785.png&#34;
	width=&#34;1496&#34;
	height=&#34;1016&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210980785_hued7e8a25fc1b8301b2158ef5923ecfd5_484033_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210980785_hued7e8a25fc1b8301b2158ef5923ecfd5_484033_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210980785&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然后将对topK使用抽取式阅读理解，从原文中抽取出可以回答的文本&lt;/p&gt;
&lt;h4 id=&#34;evidence-aggregation-for-answer-re-ranking-in-open-domain-question-answering&#34;&gt;Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering&lt;/h4&gt;
&lt;p&gt;有些问题需要来自不同来源的证据相结合才能正确回答。解决方法：strength-based re-ranker&amp;amp;coverage-based re-ranker&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235444754.png&#34;
	width=&#34;1872&#34;
	height=&#34;868&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235444754_huc54c788723dd89257fe3ba976f7270fa_279136_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235444754_huc54c788723dd89257fe3ba976f7270fa_279136_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235444754&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;517px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;strength-based re-ranker的基本思想是，正确的答案通常会被更多的段落反复提及&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235457636.png&#34;
	width=&#34;1883&#34;
	height=&#34;886&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235457636_hu72e696d9d407f5f2277275475d2b3445_372983_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235457636_hu72e696d9d407f5f2277275475d2b3445_372983_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235457636&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;510px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;coverage-based re-ranker考虑每个答案在覆盖不同证据方面的能力，这里用一个BiLSTM来计算答案支撑片段的相似表征【指一个答案和它的支撑片段在表征空间中的相似度】，在垮文本上的相似表征很很高说明这个答案更可靠&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235473330.png&#34;
	width=&#34;1817&#34;
	height=&#34;720&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235473330_hu4cd6f85dec4aa128f991df5d6baf15cf_243228_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235473330_hu4cd6f85dec4aa128f991df5d6baf15cf_243228_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235473330&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;605px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235487967.png&#34;
	width=&#34;1730&#34;
	height=&#34;632&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235487967_hu76e64111d28a0366ecc623a5d3111d7c_115596_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235487967_hu76e64111d28a0366ecc623a5d3111d7c_115596_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235487967&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;656px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;端到端方式&#34;&gt;端到端方式&lt;/h3&gt;
&lt;h4 id=&#34;retriever-reader的联合学习&#34;&gt;Retriever-Reader的联合学习&lt;/h4&gt;
&lt;h5 id=&#34;orqa-open-retriever-question-answering&#34;&gt;ORQA: Open-Retriever Question Answering&lt;/h5&gt;
&lt;p&gt;问题引入：&lt;/p&gt;
&lt;p&gt;1）需要具有强监督的支持证据：监督数据难以获得&lt;/p&gt;
&lt;p&gt;2）利用IR（信息检索）系统检索候选证据：QA与IR存在一定差异性，IR更关注词法或语义相似性，QA对于语言理解层次更丰富&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235654530.png&#34;
	width=&#34;1749&#34;
	height=&#34;1145&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235654530_hu69c5f4de9bbb84b5ce90b8d0f98ce12b_468767_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235654530_hu69c5f4de9bbb84b5ce90b8d0f98ce12b_468767_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235654530&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;366px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;就是一个S是打分函数。评价retrieval和评价reader是两个不同的，$s_{retr}$是评价这个block和问题的相关性的，$S_{read}$是评价块儿里的文本和q的相关性的。这个里面的bert是用来理解retrieval和question的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235732678.png&#34;
	width=&#34;1865&#34;
	height=&#34;1006&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235732678_hu37d256b2208876277f3150a21b7d502e_598581_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235732678_hu37d256b2208876277f3150a21b7d502e_598581_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235732678&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;444px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;每个块通过BERT和权重矩阵b生成隐藏向量h，问题通过BERT和权重矩阵q生成隐藏向量h，通过点积判断相关性&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235784735.png&#34;
	width=&#34;1764&#34;
	height=&#34;1138&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235784735_hud1f404df8d26173de0bd2a8f99c48222_355489_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235784735_hud1f404df8d26173de0bd2a8f99c48222_355489_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235784735&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;372px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;BERT_R+MLP生成s，给S_read来评分&lt;/p&gt;
&lt;p&gt;有监督训练，需要手标与a有关的s&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705239164034.png&#34;
	width=&#34;1751&#34;
	height=&#34;1096&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705239164034_hufebe00040d91fd128f7d8e46cf5ab368_454097_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705239164034_hufebe00040d91fd128f7d8e46cf5ab368_454097_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705239164034&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;383px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;有挑战，但是懒得管了&lt;/p&gt;
&lt;h4 id=&#34;基于预训练的retriever-free方法&#34;&gt;基于预训练的Retriever-Free方法&lt;/h4&gt;
&lt;p&gt;对预训练模型进行微调，使其能够在没有任何外部上下文或知识的情况下回答问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705240830964.png&#34;
	width=&#34;1582&#34;
	height=&#34;933&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705240830964_hu3309883ebfbb1e811fbf0b2dc0bd72b0_526872_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705240830964_hu3309883ebfbb1e811fbf0b2dc0bd72b0_526872_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705240830964&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;169&#34;
		data-flex-basis=&#34;406px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用span corruption来预训练&lt;/p&gt;
&lt;p&gt;Span Corruption是T5模型预训练任务中的一种方法。它将完整的句子根据随机的span进行掩码。例如，原句：“Thank you for inviting me to your party last week”，Span Corruption之后可能得到输入：“Thank you [X] me to your party [Y] week”，目标：“[X] for inviting [Y] last [Z]”。其中[X]等一系列辅助编码称为sentinels。&lt;/p&gt;
&lt;p&gt;这种方法的目标是让模型学习如何从被打乱或被掩码的句子中恢复出原始的句子。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705234885580.png&#34;
	width=&#34;1598&#34;
	height=&#34;737&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705234885580_huf096fb2909614810cbf5e9a81039edd1_178906_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705234885580_huf096fb2909614810cbf5e9a81039edd1_178906_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705234885580&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;LLM在问答任务上与有监督微调效果不相上下&lt;/p&gt;
&lt;p&gt;LLM在计数、多跳推理、日期、因果等类型上的性能较弱&lt;/p&gt;
&lt;h1 id=&#34;最后一节课&#34;&gt;最后一节课&lt;/h1&gt;
&lt;p&gt;讲了一节课的对话系统（不考）&lt;/p&gt;
&lt;p&gt;参考：https://blog.csdn.net/ld326/article/details/112802292&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
