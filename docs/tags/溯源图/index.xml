<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>溯源图 on π1l4r_のblog</title>
        <link>https://blog2.pillar.fun/tags/%E6%BA%AF%E6%BA%90%E5%9B%BE/</link>
        <description>Recent content in 溯源图 on π1l4r_のblog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>pill4r</copyright>
        <lastBuildDate>Wed, 24 Jul 2024 12:22:46 +0800</lastBuildDate><atom:link href="https://blog2.pillar.fun/tags/%E6%BA%AF%E6%BA%90%E5%9B%BE/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>NODLINK</title>
        <link>https://blog2.pillar.fun/p/nodlink/</link>
        <pubDate>Wed, 24 Jul 2024 12:22:46 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/nodlink/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/nodlink/index/1722260277932.png" alt="Featured image of post NODLINK" /&gt;&lt;h2 id=&#34;导语&#34;&gt;导语&lt;/h2&gt;
&lt;p&gt;NDSS24， doi is &lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.14722/ndss.2024.23204&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    &lt;/p&gt;
&lt;p&gt;这也是一篇溯源图方向的论文，主要是优化基于&lt;strong&gt;在线来源&lt;/strong&gt;的溯源图检测系统，即他的目标是让EDR能直接用上&lt;/p&gt;
&lt;h2 id=&#34;贡献&#34;&gt;贡献&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;在基于在线来源的检测系统中，将APT攻击检测过程建模为斯坦纳树问题&lt;/li&gt;
&lt;li&gt;提出了一种新颖的内存缓存设计&lt;/li&gt;
&lt;li&gt;提出一种高效的攻击筛选方法&lt;/li&gt;
&lt;li&gt;提出一种新的STP近似算法【在APT攻击检测中比传统的算法更有效，同时保持了相同的复杂度】&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;目标&#34;&gt;目标&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;降低基于溯源图的检测系统的开销&lt;/li&gt;
&lt;li&gt;增强及时性&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;基于stp的检测系统的挑战与应对方法&#34;&gt;基于STP的检测系统的挑战与应对方法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;长期攻击：STP需要提前了解整个溯源图。但是，由于数据大小的原因，不可能将所有来源数据保存在内存中，并且由于 I/O 瓶颈，将其存储在磁盘数据库中也不切实际。
&lt;ul&gt;
&lt;li&gt;提出了一种新颖的内存缓存设计，该设计具有评分方法，可以优先考虑可能导致APT攻击的事件，并捕获STP时间窗口内长时间运行的攻击。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有效地识别STP中的终端：现有的检测方法不适合在线系统
&lt;ul&gt;
&lt;li&gt;设计了一个IDF加权的三层变分自编码器（VAE）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;目前用于 STP 的近似算法对于 APT 攻击检测仍然不够有效。现有的方法[需要找到两个节点之间的最短路径，开销太高
&lt;ul&gt;
&lt;li&gt;开发了一种面向重要性的在线STP优化贪婪算法，该算法实现了低计算复杂度和有限竞争比。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;除了这些之外，NODLINK在深信服的产品中应用了并在两天捕获了7个APT攻击（这也太不P了）&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;大部分都因为日志量大、建图成本等原因只能靠日志在事后进行溯源，所以最近有直接接受系统审计事件的系统，在发现攻击后以溯源子图的形式输出警报&lt;/p&gt;
&lt;p&gt;现实世界中，在溯源APT时，攻击事件与良性事件之间的比率不平衡，精确检测攻击的难度高。&lt;/p&gt;
&lt;p&gt;现有的基于源的检测系统可分为两类：基于规则的检测系统和基于学习的系统。然而，现有的系统无法同时实现足够的节点级精度和节点级召回率。&lt;/p&gt;
&lt;h3 id=&#34;在线斯坦纳树问题&#34;&gt;在线斯坦纳树问题&lt;/h3&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://oi-wiki.org/graph/steiner-tree/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;斯坦纳树问题&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    ：斯坦纳树问题是组合优化问题，与最小生成树相似，是最短网络的一种。最小生成树是在给定的点集和边中寻求最短网络使所有点连通。而最小斯坦纳树允许在给定点外增加额外的点，使生成的最短网络开销最小。&lt;/p&gt;
&lt;p&gt;给定边为非负权重$w_e$的无向图 $G=(V,E)$，和一个顶点序列（其中的顶点被称作终端）$T={t_1, t_2, t_3, \dots , t_k}$，它输出一个子图$S_i$，该子图覆盖${t_1, t_2, . . . , t_i}$。其目标是最小化总成本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/nodlink/index/1721808830076.png&#34;
	width=&#34;816&#34;
	height=&#34;559&#34;
	srcset=&#34;https://blog2.pillar.fun/p/nodlink/index/1721808830076_hu800f6fd5b598e4fc567f3d6431b92f6d_98650_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/nodlink/index/1721808830076_hu800f6fd5b598e4fc567f3d6431b92f6d_98650_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1721808830076&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;350px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这个贪心算法接受带权无向图G和终端流TS。&lt;/p&gt;
&lt;p&gt;首先初始化终端集T和已选边集S为空集。对于TS中的每个终端$t_i$，初始化PATH集为空，与T中所有终端求最短路径$P_j$加入PATH集。选择PATH中最短的P，将他的点加入到S，将当前终端$t_i$加入到已选终端集T中。最终结果即是S。&lt;/p&gt;
&lt;p&gt;这个算法的竞争比率为$O(\log{k})$，其中k是终端数，也就是说在线算法的最坏情况下的表现与最优离线算法的表现之比为$O(log{k})$&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;overview&lt;/h2&gt;
&lt;p&gt;NODLINK是一个在线APT攻击检测系统。它接受从安装在受监控主机上的代理收集的系统源事件的事件流。NODLINK的输出是包含关键攻击步骤的简洁警报来源图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/nodlink/index/1721811320961.png&#34;
	width=&#34;814&#34;
	height=&#34;729&#34;
	srcset=&#34;https://blog2.pillar.fun/p/nodlink/index/1721811320961_hu9eadc78977c9047842bfcaddd2c864f5_135962_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/nodlink/index/1721811320961_hu9eadc78977c9047842bfcaddd2c864f5_135962_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1721811320961&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;111&#34;
		data-flex-basis=&#34;267px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;算法每$\Delta$秒进行四个阶段，分别是&lt;/p&gt;
&lt;p&gt;（1） 内存缓存构建（第 6 行）：把窗口内的事件存在内存里以避免IO瓶颈，还保持和更新异常节点。&lt;/p&gt;
&lt;p&gt;（2） 终端识别（第 7 行）：通过分析本地特征（例如，命令行参数、进程名称、访问的文件等），NODLINK 可以对每个进程分配一个异常得分，用于衡量它是否存在异常行为。【这tm不还是规则】&lt;/p&gt;
&lt;p&gt;（3） Hopset 构建（第 8 行）：hopset可以理解为溯源子图的构建，方便溯源用的。把攻击相关的过程视为终端【这你咋知道哪些攻击相关的？】，将有向图转换为无向图，每条边的权重为相等的非负权重，w = 1，以简化化解。【说是最novel的点】除了标准STP（算法1，第6行说是对于在线任务还是太耗时了），还设计了一个面向重要性的贪婪算法，将时间复杂度从$O(n^2)$变成了$O(n)$。&lt;/p&gt;
&lt;p&gt;（4） 综合检测（第 9 行）：将Hopset 与之前构建的缓存数据相结合，分析子图的异常得分。&lt;/p&gt;
&lt;h2 id=&#34;设计细节&#34;&gt;设计细节&lt;/h2&gt;
&lt;h3 id=&#34;内存缓存&#34;&gt;内存缓存&lt;/h3&gt;
&lt;p&gt;内存中的缓存会缓冲以下子图：（1）具有较高的异常得分的子图（2）正在积极演变的子图。内存缓存会在每个长度为 ∆ 的时间窗口内更新当前时间窗口的最小 Steiner 树（STP）解决方案。&lt;/p&gt;
&lt;p&gt;缓存以&amp;lt;srcid, dstid, attr&amp;gt;的形式保存，srcid 和 dstid 分别是边的源 ID 和目标 ID，attr 是边的属性，包括操作类型和时间戳。边的类型及其可用的操作类型如表I所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/nodlink/index/1721815091775.png&#34;
	width=&#34;882&#34;
	height=&#34;255&#34;
	srcset=&#34;https://blog2.pillar.fun/p/nodlink/index/1721815091775_hu29cbf80a9ef187e7529c8d933f8c1ba5_57306_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/nodlink/index/1721815091775_hu29cbf80a9ef187e7529c8d933f8c1ba5_57306_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1721815091775&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;345&#34;
		data-flex-basis=&#34;830px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;定义一个hopset的能量为$E=\epsilon^{age}*has(h)$，其中$\epsilon$是衰减因子，age 是自上次更新到跳跃集以来经过的时间窗口。如果给定的hopset在上一个时间窗口内已更新（例如，新事件已添加到图形中），则 age = 0。否则，NODLINK 每过一次窗口，年龄就会增加 1。缓存满的时候能量最低的hopset被淘汰，放Neo4j数据库里&lt;/p&gt;
&lt;p&gt;从磁盘检索节点：为了检测长期攻击，NODLINK 设计了图数据库的存储策略，并在遇到被驱逐的节点时从数据库中检索子图。当将跳集驱逐到磁盘时，NODLINK 存储所有节点和边的关系和属性，包括异常得分，并将其移除。NODLINK 为每个节点分配一个唯一的 uuid，使用 md5 值生成。对于进程，NODLINK 计算 pid + tid + 命令行的 md5 值。对于文件，NODLINK 计算 /full/path/filename 的 md5 值。对于 IP，NODLINK 计算 src ip:port:dst ip:port 的 md5 值。这样，NODLINK 就可以检索被驱逐节点和跳集的属性。由于每个节点都有一个唯一的 uuid，我们可以通过在缓存中查找来检查节点是否被逐出到磁盘。由于缓存中的节点由哈希表组织，因此查找操作需要 O（1） 时间。&lt;/p&gt;
&lt;h3 id=&#34;终端识别&#34;&gt;终端识别&lt;/h3&gt;
&lt;p&gt;NODLINK扫描内存缓存，并根据其节点级特征将可疑进程节点识别为终端。请注意，尽管 NODLINK 的输出仅包含异常进程，但它确实考虑了异常文件和 IP 地址。&lt;strong&gt;NODLINK将异常文件和IP地址合并到访问它们的进程中。&lt;/strong&gt;【一个节点有三个内容，包括进程，文件和ip】此设计决策背后的逻辑是，恶意文件和 IP 在被进程访问之前无法生效。因此，关注异常进程可以减少文件和 IP 上的重复警报，而不会损失节点级别的准确性。NODLINK分析了三种类型的节点级特征：启动进程的命令行（命令行）、进程访问的文件（files）和进程（network）访问的IP地址。终端识别包括两个步骤：首先，它&lt;strong&gt;基于****节点级特征将过程节点嵌入到数值向量&lt;/strong&gt;中。其次，它使用&lt;strong&gt;机器学习模型来检测异常&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;嵌入&#34;&gt;嵌入&lt;/h4&gt;
&lt;p&gt;NODLINK首先将过程的节点级特征投影到数值向量上。在高层次上，进程的嵌入是三个节点级特征的嵌入向量的加权和。NODLINK选择使用自然语言处理（NLP）技术嵌入节点级特征，以处理节点级特征中看不见的模式。嵌入过程有两个步骤。NODLINK 首先分别嵌入命令行、文件名和 IP 地址。然后，它将它们的嵌入组合为进程的最终嵌入。&lt;/p&gt;
&lt;p&gt;NODLINK将非字母数字符号转换为空格，以将节点级特征转换为句子。转换后，NODLINK使用FastText将句子转换为数值向量。&lt;/p&gt;
&lt;p&gt;嵌入节点级特征的主要挑战是它们可能包含不属于自然语言的字符串。例如，“/var/spool/8b7dc29d0e”包含哈希字符串“8b7dc29d0e”。现有的基于 NLP 的文档嵌入技术无法处理这些特殊标记。因此，NODLINK通过删除NLP技术Nostril删除无效含义的标记来删除非自然语言。&lt;/p&gt;
&lt;p&gt;嵌入的第二步是将三个节点级特征的数值向量相加，如下式&lt;/p&gt;
&lt;p&gt;$$
V_{p} = w_{c} * V_{c} + \sum{w_{fi} * V_{fi}} + \sum{w_{ni} * V_{ni}}
$$&lt;/p&gt;
&lt;p&gt;其中$V_{c}$, $V_{fi}$, $V_{ni}$是命令行、文件和网络连接的嵌入，$w_{c}$, $w_{fi}$,  $w_{ni}$ 是权重，正式来说，文件的权重 $w_{fi}$ 定义为 $w = \log(\frac{P}{P_{fi}})$，其中 $P$ 是所有进程的数量，而 $P_{f}$ 是操作文件 $fi$ 的进程数量。我们采用类似的方法来计算 IP 地址的权重 $w_{ni}$。这个设计背后的逻辑是针对不同进程共享的文件和 IP 地址进行处理。例如，所有进程都会加载$ \textit{libc}$ 文件，因此$ \textit{libc}$ 对于建模进程的本地特征并没有太大用处。为了提高进程建模的准确性，我们设计权重来降低像$ \textit{libc}$ 文件这样的文件或 IP 地址的影响。最后，命令行的权重 $w_c$ 是所有文件和 IP 的权重的平均值，以确保 $w_c$ 在文件和 IP 地址的数量级上具有相同的规模。&lt;/p&gt;
&lt;p&gt;这个嵌入还是挺轻量的，因为NODLINK的设计目标就是速度。&lt;/p&gt;
&lt;p&gt;为了检测终端，NODLINK利用VAE模型来计算来源图中每个进程节点的异常分数，然后将异常分数较高的节点识别为终端。VAE模型在其他任务中被广泛用作轻量级异常检测模型。具体就是将进程的嵌入给VAE重构，由于VAE在学习过程中学习数据的潜在分布，如果是输入数据属于训练数据的分布，VAE应该能够很好地重构它。因此重构前后的嵌入相差越大就越可能是异常节点。&lt;/p&gt;
&lt;p&gt;然而重建误差可能会为不稳定的进程产生误报，比如Web 浏览器倾向于访问随机 IP 地址。直接使用重建错误会不断将 Web 浏览器等进程识别为终端，从而导致误报。为了解决这个问题，我们引入了稳定性评分SV来平衡不稳定过程的重建误差。我们将SV定义为历史数据中与p同名的过程的嵌入向量的簇数。假设我们有一个名为“web_browser”的进程，在历史数据中我们找到了100个名为“web_browser”的进程实例。我们对这100个进程的嵌入向量进行聚类分析，结果发现这些嵌入向量分成了10个簇。这表明名为“web_browser”的进程在历史数据中表现出10种不同的行为模式。于是，稳定性评分SV就等于10。&lt;/p&gt;
&lt;p&gt;$$
AS(p) = \log(\frac{RE(p)}{SV(p)})
$$&lt;/p&gt;
&lt;p&gt;RE是重构误差，SV是稳定性评分。这样，当一个进程的行为较为不稳定（SV较大）时，即使重构误差较大，异常评分也不会太高，从而减少了误报。&lt;/p&gt;
&lt;p&gt;如果进程节点的 AS 高于历史数据中 AS 的第 90 个百分位数，则 NODLINK 会将其标记为异常。&lt;/p&gt;
&lt;p&gt;离线模型训练：虽然NODLINK是一个在线检测框架，但它需要训练FastText模型、VAE模型、SV模型和阈值，才能从离线的历史数据中引发异常。NODLINK 在历史数据中的命令行、文件路径和 IP 地址上训练 FastText 模型。然后，NODLINK使用经过训练的嵌入向量来进一步训练VAE模型。NODLINK通过定期对历史数据运行DBSCAN算法[58]来离线计算SV。它首先将嵌入向量的过程分类为不同的组。然后，将进程名称及其集群数量存储在内存哈希表中，用于在线异常分数计算。&lt;/p&gt;
&lt;h4 id=&#34;hopset构建&#34;&gt;hopset构建&lt;/h4&gt;
&lt;p&gt;在终端识别步骤中检测到终端之后，NODLINK在当前时间窗口中运行Hopset构建来解决STP（最短路径树）问题。在某个时间窗口中的hopset是每个终端的邻域上下文，其中包括有界邻居节点和到这些节点的路径。NODLINK利用了一种称为重要性评分引导搜索（ISG）算法的贪心算法，基于本地信息（如AS和节点度）来构建hopset。Hopset构建输出一个具有低复杂度和有界竞争比的近似STP解。&lt;/p&gt;
&lt;p&gt;优化主要在于算法 1 中，使用 Dijkstra 算法找到第 6 行的最短路径，时间复杂度为$O(n^2)$，我们设计的基于重要性评分的搜索的复杂度为$O(n)$&lt;/p&gt;
&lt;p&gt;具体构建过程如下：假设终端集中有n个节点，Hopset构建首先启动n个搜索过程，每个搜索过程对应一个终端。搜索过程是通过一个重要性评分IV来进行贪心搜索的。每个贪心搜索过程生成一个hopset，并且节点数量是有界的。如果已经发现了θ个节点，NODLINK将停止探索新节点。这种提前停止的动机来自于攻击聚合假设。该假设指出在来源图中，攻击行为在拓扑上是接近的，因为攻击者需要通过一系列立足点逐步创建攻击行为的执行链。&lt;/p&gt;
&lt;p&gt;我们的工具在贪婪搜索过程中合并重叠的hopset，以高效地连接终端，同时限制节点探索，而不是使用经典贪婪算法中复杂度较高的最短路径算法。这种近似解决方案通过识别拓扑上相近的与攻击相关的异常，减少了异常检测中的假阳性。跳集构建为每个跳集分配一个HAS（hopset anomaly score）以便未来调查，重点关注基于异常分数(高)和与终端的距离(近)的重要节点。这使得远离的节点优先级降低，从而排除假阳性，并利用APT攻击的攻击多态性，后者指出攻击相关的事件级异常在起源图中是拓扑上相近的，因为攻击者通过少数的立足点（如后门或反向Shell）进行攻击活动。&lt;/p&gt;
&lt;p&gt;Hopset Construction的关键组成部分是重要性分数的设计，首先是异常分数 AS。第二个是与终端中最近节点的距离【直觉是利用 APT 攻击的攻击聚合性】。&lt;/p&gt;
&lt;p&gt;$$
IV(n)={\alpha}^i({\beta}*AS(n)+{\gamma}*FANOUT(n))
$$&lt;/p&gt;
&lt;p&gt;${\alpha}^i$反映了节点$n$与终端之间的距离。当$n$远离终端时，$IV$会降低。具体而言，$0&amp;lt;{\alpha}&amp;lt;1$是一个距离衰减因子，$i$是$n$与其最近的终端之间的跳数。$AS(n)$是该节点的异常分数。我们还为节点$n$引入了一个补充因子$FANOUT(n)$，其定义为$FANOUT(n)=out_degree(n)/(in_degree(n) + 1)$。引入这个术语是因为我们观察到某些过程倾向于生成大量不传播数据和控制流依赖的“叶子”节点。这些“叶子”节点对于\ac{apt}攻击检测和调查并不重要。因此，我们使用$FANOUT$术语来降低它们的优先级。我们将$FANOUT$和$AS$结合，使用两个权重$\beta$和$\gamma$。需要注意的是，$AS$是主要因素，而$FANOUT$是补充因素。因此，需要满足$\beta &amp;raquo; \gamma$。&lt;/p&gt;
&lt;p&gt;hopset的异常分数HAS是hopset中所有节点的异常分数之和。&lt;/p&gt;
&lt;h4 id=&#34;全面检测&#34;&gt;全面检测&lt;/h4&gt;
&lt;p&gt;首先使用在当前时间窗口内构造的跳跃集更新内存中的缓存跳跃集。NODLINK将当前时间窗口的跳跃集与缓存中的跳跃集（如果它们具有相同的节点）合并。但是，如果我们直接合并跳跃集，在最坏的情况下，一个长时间运行的进程被识别为终端，并在每个时间窗口内更新θ个不同的邻居，就有可能导致依赖性爆炸。为了防止这种情况的发生，我们限制了 θ 节点内每个终端的跳集，并在合并时将 IV 值较低的节点替换为 IV 值较高的节点。&lt;/p&gt;
&lt;p&gt;合并跳集之后将HAS也随之更新，通过HAS将每个时间窗口的STP解和全局解结合起来。NODLINK利用Grubbs检验来检测HAS异常高的跳跃集。Grubbs 检验检测一组样本的最大值是否为异常值。我们之所以选择 Grubbs 检验，是因为它是非参数化的，并且对污染的训练数据集具有鲁棒性。NODLINK 在内存缓存上运行 Grubbs 测试多轮，直到没有标记异常值。最后，将检测到的异常值标识为攻击活动，并发出警报。&lt;/p&gt;
&lt;p&gt;【Grubbs检验是一种用于检测数据集中异常值（即极端值或离群值）的统计方法。它基于假设检验的原理，通常用于正态分布的数据。】&lt;/p&gt;
&lt;h4 id=&#34;理论分析&#34;&gt;理论分析&lt;/h4&gt;
&lt;p&gt;终端识别过程中，NODLINK需要通过复杂度为O（E）的边缘来探索节点级的特征&lt;/p&gt;
&lt;p&gt;在 Hopset 构造中，NODLINK 通过贪婪探索 θ 节点为每个终端构造 hopset。因此，检测的复杂度为 O（θN），其中 N 是来源图中的节点数。&lt;/p&gt;
&lt;p&gt;竞争比率：在第 V-C 节中，我们将算法 1 第 6 行的最短路径探索替换为 importancescore 引导的搜索。这种替换可能会导致检测中出现更大的 Steiner 树，从而影响简洁性。因此，在本节中，我们分析了新的竞争比率，即我们的算法生成的斯坦纳树的大小与理论上最优解之间的比率。回想一下，标准在线 STP 优化算法的竞争比率为 O（log（k）），其中 k 是终端数。因此，我们只需要将我们的方法与标准的在线STP优化算法进行比较。&lt;/p&gt;
&lt;p&gt;总而言之，由于 θ 是一个常数，因此 NODLINK 可以得到 2θ log（k） = O（log（k）） 有界竞争比率。&lt;/p&gt;
&lt;h2 id=&#34;我的评价&#34;&gt;我的评价&lt;/h2&gt;
&lt;p&gt;论文写的稀烂，看得我头皮发麻。&lt;/p&gt;
&lt;p&gt;我觉得这篇文章只把进程作为节点，把和他相关的东西作为节点属性，比如启动这个进程的cmd，这个进程访问的文件、ip等，这些在其他常见的方法里都被当做实体，当做节点。&lt;/p&gt;
&lt;p&gt;这样做图的规模就会变小很多，对于节点属性的处理也是fasttext转成向量， 然后拿VAE去生成节点异常值，都是现成的东西。&lt;/p&gt;
&lt;p&gt;感觉他就是一个优化STP的工作，而寻找联通量大的节点也是出于一种恶意节点会创建很多子进程的直觉吧。&lt;/p&gt;
&lt;p&gt;但是讲道理他这也就是把一个子图变成节点了，把别人的子图粒度的检测在他这说成节点粒度的检测，感觉ndss不愧是曾经B里的A，现在纯纯是A里的B，这也太工程了。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>FLASH</title>
        <link>https://blog2.pillar.fun/p/flash/</link>
        <pubDate>Thu, 18 Apr 2024 10:51:57 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/flash/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/flash/index/1716374966746.png" alt="Featured image of post FLASH" /&gt;&lt;h2 id=&#34;导语&#34;&gt;导语&lt;/h2&gt;
&lt;p&gt;IEEE S&amp;amp;P 2024, doi is &lt;a class=&#34;link&#34; href=&#34;https://doi.ieeecomputersociety.org/10.1109/SP54263.2024.00139&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    &lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;motivation&lt;/h2&gt;
&lt;p&gt;简单来说还是对效率问题提出了一个解，针对大的溯源图，开发了一个嵌入回收数据库来&lt;strong&gt;存储&lt;/strong&gt;在训练阶段生成的节点嵌入，检测的时候读嵌入数据库即可，不用重新算嵌入&lt;/p&gt;
&lt;p&gt;Q:为什么训练阶段的节点嵌入在检测时能用到，覆盖率怎么搞？&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;现有的溯源图IDS的存在局限&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常会忽略有价值的语义数据，例如来源图中的进程名称、命令行参数、文件路径和 IP 地址。【Q：但是不是说这些内容容易造成混淆吗？把这些通过word2vec编码成向量不是还是容易造成混淆吗？】（A：意思是不带这些可以提升泛化性，对于没见过的攻击比较管用，但是可能因为学的不够多而有误报，我觉得还是泛化性更重要一点。差评)&lt;/li&gt;
&lt;li&gt;一些IDS忽略了系统事件的时间顺序和因果顺序的重要性。&lt;/li&gt;
&lt;li&gt;可扩展性问题，尤其是在处理大型来源图时。&lt;/li&gt;
&lt;li&gt;粗粒度检测：许多 IDS 识别恶意子图而不是单个恶意节点，这使得警报验证和攻击重建对安全分析师来说既耗时又容易出错。&lt;/li&gt;
&lt;li&gt;GNN技术不可扩展，而且速度非常慢&lt;/li&gt;
&lt;li&gt;现有方法都是学习良性行为&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/flash/FLASH/1713703958031.png&#34;
	width=&#34;894&#34;
	height=&#34;496&#34;
	srcset=&#34;https://blog2.pillar.fun/p/flash/FLASH/1713703958031_hu309cf4843cd94553ffc9c25bbf74bd10_70966_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/flash/FLASH/1713703958031_hu309cf4843cd94553ffc9c25bbf74bd10_70966_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1713703958031&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因此，这篇文章&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;采用基于Word2Vec的嵌入技术，将来源图中存在的各种节点属性（如进程名称、命令行参数、文件路径和IP地址）编码为语义丰富的特征向量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;修改了 Word2Vec 技术以获得时间敏感的嵌入，解决了忽略事件之间时间顺序的问题。(咋修改的？)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;通过仅选择与威胁识别相关的边来提高图表示学习中图遍历的效率，设计了一个GNN嵌入数据库，该数据库的灵感来自以前应用于&lt;strong&gt;语言模型的嵌入回收技术（2022)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;报告危险节点（node)而非危险子图（graph)，能够对adversarial mimicry attacks on provenance-based IDSes【Q：这是啥？】（A：http://dx.doi.org/10.1145/586110.586145)鲁棒。根据检测生成的演变图（AEG)来帮助溯源。说是检测危险边（edge)的消耗太大，node是最好的trade off&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇文章首次（2024年以来)在Darpa OpTC数据集上进行了评估，该数据集是DARPA迄今为止发布的最大的系统日志数据集。这些数据集涵盖了广泛的攻击场景和系统行为。说是比SOTA快三倍&lt;/p&gt;
&lt;p&gt;作者的小结：&lt;/p&gt;
&lt;p&gt;• 我们提出了一种基于来源的IDS，即FLASH，它利用来源图中的上下文和结构信息来增强其检测能力。&lt;/p&gt;
&lt;p&gt;• 我们引入了一个两步过程，分别使用 Word2Vec 和 GNN 生成语义和上下文嵌入。在此过程之后，通过轻量级分类器模型进行实时异常检测，确保系统的可扩展性和效率。&lt;/p&gt;
&lt;p&gt;• 我们提供两种方案——选择性图遍历和嵌入回收数据库——使图表示学习在 IDS 设置中变得实用。&lt;/p&gt;
&lt;p&gt;• 我们在真实世界的数据集上对我们的技术进行全面评估。结果突出了FLASH在识别恶意活动方面的有效性，其对对抗性模拟攻击的弹性，以及加速警报验证过程的能力&lt;/p&gt;
&lt;h2 id=&#34;motivation-1&#34;&gt;Motivation&lt;/h2&gt;
&lt;h3 id=&#34;optc的攻击场景&#34;&gt;OpTC的攻击场景&lt;/h3&gt;
&lt;p&gt;攻击者会向目标受害者发送网络钓鱼电子邮件。这些电子邮件包含恶意的 PowerShell Empire 暂存器。打开电子邮件附件后，攻击者将获得对受害者系统的访问权限。然后，攻击代理与命令和控制 （C&amp;amp;C） 服务器建立连接，并秘密地在系统中停留数天。代理的目标是检查系统配置并搜索敏感数据。为了保持隐身性，攻击者执行最少的系统活动并模仿良性系统实体的行为。代理找到所需文件后，会从命令服务器下载有效负载，并将数据泄露到服务器。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/flash/index/1716372896940.png&#34;
	width=&#34;2746&#34;
	height=&#34;728&#34;
	srcset=&#34;https://blog2.pillar.fun/p/flash/index/1716372896940_hu317dbaddf180456aa7acfb703b20839d_342945_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/flash/index/1716372896940_hu317dbaddf180456aa7acfb703b20839d_342945_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716372896940&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;377&#34;
		data-flex-basis=&#34;905px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在数据收集过程中，这种特殊的攻击是作为 DAPRA 红队演习的一部分执行的。红队在系统上安装了 C&amp;amp;C 代理，该代理使用find.exe搜索关键文件并收集系统信息。然后，攻击代理通过Chrome.exe从 news.com:8080 下载了一个名为fileTransfer1000.exe的程序。该程序压缩文档目录中的文件并将它们泄露到 news.com:9999。这是数据泄露攻击的典型示例，攻击者旨在从目标系统中窃取敏感信息，同时通过模拟良性系统进程来保持未被发现。&lt;/p&gt;
&lt;h3 id=&#34;现有方法缺陷&#34;&gt;现有方法缺陷&lt;/h3&gt;
&lt;p&gt;主要通过学习良性方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/flash/index/1716373026697.png&#34;
	width=&#34;1340&#34;
	height=&#34;914&#34;
	srcset=&#34;https://blog2.pillar.fun/p/flash/index/1716373026697_hu62b6fee0b0c876dbff500c3c93f92fbd_147645_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/flash/index/1716373026697_hu62b6fee0b0c876dbff500c3c93f92fbd_147645_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716373026697&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;146&#34;
		data-flex-basis=&#34;351px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Semantic Encoding（语义编码)：在节点编码过程中是否考虑了语义信息如进程名称、命令行或文件路径。&lt;/li&gt;
&lt;li&gt;Temporal Encoding（时间编码)：是否考虑系统事件的时间顺序&lt;/li&gt;
&lt;li&gt;Scalability（可拓展性)：GNN的高计算需求会阻碍系统的可扩展性，是否通过方法降了GNN计算开销&lt;/li&gt;
&lt;li&gt;Detection Granularity（检测粒度)：是否有粗细粒度的区分（检测出异常子图、检测出异常节点/边)&lt;/li&gt;
&lt;li&gt;Contextual Alerts（上下文警报)：按照他原文还是子图粒度和节点/边粒度的问题，他意思是能检测出节点/边的就能更好重建攻击（那为啥要分两条？多少沾点)&lt;/li&gt;
&lt;li&gt;Robustness Against Mimicry Attacks（对模仿攻击的鲁棒性)：对抗溯源图检测的模仿攻击指操纵分布图编码，修改攻击图中的节点邻域以模仿良性起源图中的节点邻域。归根接地还是要节点粒度/边粒度的检测（嗯是让你凑了三条啊？)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;flash-design&#34;&gt;FLASH design&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/flash/index/1716374966746.png&#34;
	width=&#34;1684&#34;
	height=&#34;1474&#34;
	srcset=&#34;https://blog2.pillar.fun/p/flash/index/1716374966746_huf8d1b81f197ed97f0307a2b1035badd9_647016_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/flash/index/1716374966746_huf8d1b81f197ed97f0307a2b1035badd9_647016_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716374966746&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;114&#34;
		data-flex-basis=&#34;274px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;五个关键模块组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;来源图构造函数
流式读，节点分为进程节点和对象节点，对象节点包括文件、网络流、模块和其他系统对象，节点包含属性，例如进程名称、命令行、文件路径、IP 地址、端口号和模块路径。边带有指定事件类型（系统调用）的标签，标签代表连接节点与事件时间戳之间的因果关系&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于 Word2Vec 的语义编码器
系统日志包含与各种系统实体相关的丰富属性。独热、词袋转换的向量过于稀疏，采用Word2Vec模型，将属性转换到密集的向量空间。考虑节点的以下属性：进程节点的进程名称和命令行参数、文件节点的文件路径、套接字节点的网络 IP 地址和端口以及模块节点的模块名称。通过结合语义属性和节点与其 1-hop 邻居之间的因果事件类型（系统调用）来为每个节点形成摘要句子。系统事件按时间戳排序，以保持时间顺序。每个句子都通过在良性系统日志上训练的 Word2Vec 模型编码为固定长度的向量。【就是说将节点的属性和对应1hop邻居的系统事件关系形成一个长句子在word2vec到固定长度】
标准的Word2Vec 模型不保留句子中单词的顺序，我们设计了一种为每个单词嵌入分配单独的权重的时间编码方案，为每个单词嵌入分配单独的权重。这些权重累积在一个句子上，产生一个富含时间信息的嵌入。我们通过根据时间戳按时间顺序排列系统事件来启动这种方法，从而促进将时间顺序整合到我们的句子中。我们将从Transformers[65]借来的概念位置编码合并到输入嵌入中，以传达有关序列中每个标记位置的信息。Word2Vec 缺乏内置的顺序概念，因此位置编码允许模型根据其序列位置来区分标记。
通过将Word2Vec嵌入与位置编码相结合，我们的模型不仅可以捕获单词之间的语义关联，还可以捕获摘要句子中单词的顺序。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于 GNN 的上下文编码器
通过GNN需要通过对节点周围k-hop邻域结构进行编码可以有效识别溯源图中的隐蔽攻击节点，但开销巨大。基于上一部分的word2vec的嵌入的图表示学习则加雪上加霜。
我们的GNN模型学习GraphSage的图遍历算法。我们设计了一系列图遍历原则。这些原则指导GraphSage在应用GNN之前有选择地聚合来自特定边缘的信息。我们用以下遍历原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;唯一边采样：我们仅对两个节点之间相同类型的单个边进行采样，确保在遍历过程中只包含一次此类边。【不还是图缩减吗……说的好像很屌的样子】&lt;/li&gt;
&lt;li&gt;低优先级事件排除：排除事件优先级的边和取证无关的系统事件的边。此类事件可能包括由进程临时创建且在系统执行期间从未与其他进程交互的文件的删除事件，以及表示为进程节点的自循环的退出事件。以前的工作也采用了类似的方法来减少系统日志中的噪声。&lt;/li&gt;
&lt;li&gt;特定于执行的信息过滤：仅包含一次具有相同执行特定信息的节点和边。溯源图中的许多相邻节点可能仅因特定于执行的属性而有所不同，但在其他方面是相同的。比如具有相同4元组但开始和结束时间不同的网络流&lt;/li&gt;
&lt;li&gt;用户特定属性处理：将仅在用户特定属性上不同的节点或边视为相同，例如，如果两个模块具有相同的模块路径，但不同的用户ID不同，则它们可能会有所不同。对于此类节点，我们忽略特定于用户的属性，仅选择其中一个。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们采用半监督节点分类方法来训练我们的新GNN模型。我们的模型使用节点的输入特征和图形结构来对其类型进行分类。GNN模型使用标记数据进行训练，学习识别良性节点的类型【你他妈不还是学的良性吗？你在这说什么p话呢？】。使用加权交叉熵来解决数据不平衡的问题，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;嵌入数据库
我们的系统利用训练好的图神经网络（GNN）模型为存在于我们良性数据集中的所有节点生成结构化嵌入。为了在实时威胁检测期间高效检索和存储这些GNN输出向量，我们设计了一个专门的键值存储结构。&lt;strong&gt;键&lt;/strong&gt;被设计为&lt;strong&gt;持久节点标识符&lt;/strong&gt;（PNI），它与节点属性相关联，这些属性在不同的系统执行过程中保持不变。这些属性包括进程名称、文件路径、模块路径和网络流IP地址。相应的&lt;strong&gt;值&lt;/strong&gt;则包含了由GNN导出的嵌入，以及与该特定节点相关联的一组邻居节点。
我们利用属性抽象技术来&lt;strong&gt;删除特定于用户和执行的信息&lt;/strong&gt;。这确保了存储的嵌入是可通用的。具体来说，有以下几种抽象模式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户抽象模式：针对进程和文件节点类型实现，该模式从进程名称和文件路径中省略&lt;strong&gt;用户ID&lt;/strong&gt;，实现高度泛化。例如，文件路径 /Users/john/.bashrc 被抽象为 /Users/*/.bashrc。&lt;/li&gt;
&lt;li&gt;网络连接抽象架构：应用于套接字节点类型，此架构消除了&lt;strong&gt;开始和结束时间&lt;/strong&gt;，从而增强了不同系统执行的通用性&lt;/li&gt;
&lt;li&gt;模块路径抽象架构：模块节点具有路径和基址属性。当基址更改时，路径在不同的执行中保持不变。此架构仅保留模块路径，确保模块节点的一致和可泛化表示。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过对具有稳定邻域结构的节点进行预计算和存储GNN嵌入，可以优化实时异常检测和减少计算开销。其中，邻域集起着关键作用。它有助于确定实时分析期间节点的邻域结构是否与良性阶段观察到的邻域结构相匹配。如果匹配就直接拿数据库里的嵌入，不需要再通过邻域关系进行图表示计算。如果不匹配则默认为实时生成的 Word2Vec 特征进行异常检测。【Q：对吗？是特征？不是应该是GNN算出来的节点嵌入吗？】我们使用 &lt;strong&gt;Jaccard 索引&lt;/strong&gt;来比较节点的存储邻域和当前邻域。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;异常检测器
我们选择了 XGBoost 作为我们的异常检测任务的分类器。XGBoost 最小化正则化目标函数 $J = L(y， f (x))+Ω(f)$，使用梯度提升以迭代方式向集合添加新树。每个新树都旨在最小化损失函数相对于当前集成预测的梯度。XGBoost 模型使用每个节点的串联 Word2Vec 编码向量和 GNN 嵌入向量。它从预训练的键值存储中检索 GNN 嵌入，实时生成 Word2Vec 特征，执行推理，并保存输出以供下一管道阶段使用。这种强大的流程巩固了我们IDS的性能和可扩展性。
FLASH通过比较预测节点类型和实际节点类型来检测异常节点。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;还有个攻击演化图的构造，用于更直观的溯源&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;从FLASH生成的大型来源图和警报中构建紧凑的攻击演化图（AEG）。中心概念是在来源图中相互链接因果相关的警报，从而构建一系列简洁的 AEG。这些 AEG 仅封装警报节点及其因果链接，从而提供警报节点交互的简化和清晰视图。这种减少大大简化了原始图形，使分析师更容易快速有效地掌握攻击进展。
&lt;img src=&#34;https://blog2.pillar.fun/p/flash/index/1716380160194.png&#34;
	width=&#34;890&#34;
	height=&#34;792&#34;
	srcset=&#34;https://blog2.pillar.fun/p/flash/index/1716380160194_hu937ddd0d45b066d7f519c0c9c14817c4_114837_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/flash/index/1716380160194_hu937ddd0d45b066d7f519c0c9c14817c4_114837_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716380160194&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;269px&#34;
	
&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;懒得写了&lt;/p&gt;
&lt;h2 id=&#34;主要贡献&#34;&gt;主要贡献&lt;/h2&gt;
&lt;p&gt;嵌入回收数据库存储在训练阶段生成的节点嵌入&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过图神经网络（GNN）在数据溯源图上利用图表示学习&lt;/li&gt;
&lt;li&gt;使用基于Word2Vec的语义编码器来捕获基本的语义属性和时间顺序&lt;/li&gt;
&lt;li&gt;采用了基于GNN的上下文编码器，可以有效地将局部和全局图结构编码为富有表现力的节点嵌入。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;补充知识&#34;&gt;补充知识&lt;/h2&gt;
&lt;h3 id=&#34;加权交叉熵&#34;&gt;加权交叉熵&lt;/h3&gt;
&lt;p&gt;加权交叉熵（Weighted Cross-Entropy）是一种改进的交叉熵损失函数，它通过为不同类别的样本分配不同的权重来调整损失的计算。在标准的交叉熵损失函数中，所有类别的样本对损失的贡献是相同的。然而，在实际应用中，数据集往往存在类别不平衡的问题，即某些类别的样本数量远多于其他类别。这种不平衡会导致模型在训练时偏向于预测样本数量多的类别，从而忽视了样本数量少的类别。&lt;/p&gt;
&lt;p&gt;加权交叉熵通过引入权重系数来解决这个问题。对于样本数量较少的类别，可以为其分配一个较大的权重，而对于样本数量较多的类别，则分配一个较小的权重。这样，在计算损失时，样本数量少的类别对损失的贡献会更大，从而促使模型更加关注这些类别的样本，提高模型对这些类别的预测能力。&lt;/p&gt;
&lt;h3 id=&#34;jaccard-索引&#34;&gt;Jaccard 索引&lt;/h3&gt;
&lt;p&gt;Jaccard 索引，也称为 Jaccard 相似系数或 Jaccard 系数，是一种衡量两个集合相似度的统计量。它定义为两个集合的交集大小与它们的并集大小的比值。Jaccard 索引的取值范围在 0 到 1 之间，其中 1 表示两个集合完全相同，0 表示两个集合没有共同元素。&lt;/p&gt;
&lt;p&gt;Jaccard 索引的数学表达式如下：&lt;/p&gt;
&lt;p&gt;J(A, B) = |A ∩ B| / |A ∪ B|&lt;/p&gt;
&lt;h3 id=&#34;xgboost&#34;&gt;XGBoost&lt;/h3&gt;
&lt;p&gt;XGBoost（eXtreme Gradient Boosting）是一种基于决策树的集成机器学习算法，它以梯度提升（Gradient Boosting）框架为基础，通过优化目标函数和高效地处理大规模数据集而广受欢迎&lt;/p&gt;
&lt;p&gt;XGBoost 在各种机器学习任务中都有广泛的应用，如分类、回归、排序等。由于其优异的性能和灵活性，XGBoost 成为了数据科学家和机器学习工程师的常用工具之一。&lt;/p&gt;
&lt;p&gt;XGBoost有以下优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;XGBoost 在传统的梯度提升框架基础上，引入了一个正则化项，用于控制模型的复杂度，从而减少过拟合的风险。这使得 XGBoost 在训练过程中能够生成更加稳健和泛化能力更强的模型。&lt;/li&gt;
&lt;li&gt;XGBoost 采用了一些优化技巧，如近似算法、特征分位点、缓存优化等，以提高算法的计算效率。这使得 XGBoost 能够处理大规模数据集，并在较短的时间内完成训练。&lt;/li&gt;
&lt;li&gt;XGBoost 提供了丰富的参数设置，用户可以根据具体问题的需求调整模型的性能。这些参数包括树的深度、学习率、子采样比例等，通过合理地设置这些参数，可以进一步提高模型的性能。&lt;/li&gt;
&lt;li&gt;XGBoost 可以处理多种类型的数据，包括数值型、类别型和缺失值等。这使得 XGBoost 在实际应用中具有很高的灵活性&lt;/li&gt;
&lt;li&gt;XGBoost 支持在多线程环境下进行并行计算，以及在分布式系统中进行计算，这使得 XGBoost 能够处理更大规模的数据集，并在更短的时间内完成训练。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;我的评价&#34;&gt;我的评价&lt;/h2&gt;
&lt;p&gt;感觉没啥特别的，就一个嵌入数据库比较有意思，抽空把嵌入回收那篇的原文读了吧。&lt;/p&gt;
&lt;p&gt;学到了用加权交叉熵来解决数据不平衡，但是我觉得KAIROS的欠采样和过采样更好一点。&lt;/p&gt;
&lt;p&gt;剩下的图缩减的优化算法其实感觉也都大差不差，都是尽可能缩就完了……&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MAGIC</title>
        <link>https://blog2.pillar.fun/p/magic/</link>
        <pubDate>Sat, 02 Mar 2024 20:19:11 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/magic/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/magic/MAGIC/1709702251187.png" alt="Featured image of post MAGIC" /&gt;&lt;p&gt;文章可以在&lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.48550/arXiv.2310.09831&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    获取&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;APT攻击越来越成为普遍的威胁，然而，以前的工作表现出几个缺点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需要包含攻击的数据和APT的先验知识&lt;/li&gt;
&lt;li&gt;无法提取隐藏在来源图中的丰富上下文信息&lt;/li&gt;
&lt;li&gt;由于其令人望而却步的计算开销和内存消耗而变得不切实际。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文介绍一种自监督APT检测方法MAGIC，能够在不同级别的监督下进行多粒度检测。利用掩码图表示学习对良性系统实体和行为进行建模，对来源图进行高效的深度特征提取和结构抽象。通过异常值检测方法检测异常系统行为，MAGIC 能够执行系统实体级和批处理日志级 APT 检测。&lt;/p&gt;
&lt;p&gt;MAGIC是专门为处理概念漂移而设计的，具有模型适配机制，并成功应用于通用条件和检测场景。我们在三个广泛使用的数据集上评估了MAGIC，包括真实世界和模拟攻击。评估结果表明，MAGIC在所有场景中都取得了令人鼓舞的检测结果，并且在性能开销方面比最先进的APT检测方法具有巨大的优势。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;高级持续性威胁（APTs）是由熟练的攻击者进行的蓄意和复杂的网络攻击，对企业和机构都构成巨大威胁。大多数 APT 都涉及零日漏洞，并且由于其隐蔽性和多变性而特别难以检测。&lt;/p&gt;
&lt;p&gt;最近关于APT检测的工作利用数据来源进行APT检测。数据来源将审计日志转换为来源图，从审计日志中提取丰富的上下文信息，为细粒度的因果关系分析和 APT 检测提供完美的平台。&lt;/p&gt;
&lt;p&gt;早期工作基于典型或特定的 APT 模式&lt;strong&gt;构建规则&lt;/strong&gt;，并将审计日志与这些规则进行匹配，以检测潜在的 APT。&lt;/p&gt;
&lt;p&gt;最近的一些工作采用&lt;strong&gt;统计异常检测&lt;/strong&gt;方法来检测APT，这些APT侧重于不同的来源图元素，例如系统实体、交互和社区。&lt;/p&gt;
&lt;p&gt;最近的研究基于&lt;strong&gt;深度学习的方法&lt;/strong&gt;。他们利用各种深度学习 （DL） 技术对 APT 模式或系统行为进行建模，并以分类或异常检测方式执行 APT 检测。&lt;/p&gt;
&lt;p&gt;虽然这些现有方法已经证明了它们能够以合理的准确性检测 APT，但它们遇到了以下挑战的各种组合：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;监督方法存在缺乏数据 （LOD） 问题，因为它们需要有关 APT 的先验知识（即攻击模式或包含攻击的日志）。此外，当面对他们没有接受过处理培训的新型 APT 时，这些方法特别容易受到攻击。&lt;/li&gt;
&lt;li&gt;基于统计的方法只需要良性数据即可发挥作用，但无法提取审计日志中埋藏的复杂良性活动的深层语义和相关性，导致&lt;strong&gt;误报率高。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;基于深度学习的方法，特别是基于序列和基于图的方法，以&lt;strong&gt;沉重的计算开销&lt;/strong&gt;为代价，取得了可观的有效性，使其在实际检测场景中不切实际&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在本文中，我们通过引入MAGIC来解决上述三个问题，MAGIC是一种新颖的自监督APT检测方法，它利用掩码图表示学习和简单的异常值检测方法从海量审计日志中识别关键攻击系统实体。MAGIC首先通过简单而通用的步骤从审计日志中构建出处图。然后，MAGIC使用图表示模块，该模块通过以自我监督的方式合并图特征和结构信息来获得嵌入。该模型建立在图形掩蔽自编码器[19]之上，在掩蔽特征重建和基于样本的结构重建的共同监督下。采用无监督异常值检测方法对计算出的嵌入进行分析，并得到最终的检测结果。&lt;/p&gt;
&lt;p&gt;MAGIC首先通过简单而通用的步骤从审计日志中构建出处图。然后，MAGIC使用图表示模块，该模块通过以自我监督的方式合并图特征和结构信息来获得嵌入。该模型建立在图形掩蔽自编码器之上，在掩蔽特征重建和基于样本的结构重建的共同监督下。采用无监督异常值检测方法对计算出的嵌入进行分析，并得到最终的检测结果。&lt;/p&gt;
&lt;p&gt;MAGIC 旨在灵活且可扩展。根据应用程序背景，MAGIC 能够执行多粒度检测，即检测批处理日志中是否存在 APT 或定位实体级对手。虽然 MAGIC 旨在执行 APT 检测而不会受到攻击包含数据，但它非常适合半监督和完全监督的情况。此外，MAGIC还包含一个可选的模型适配机制，为其用户提供反馈渠道。这样的反馈对于MAGIC进一步提高性能、对抗概念漂移和减少误报非常重要。&lt;/p&gt;
&lt;p&gt;我们在三个不同的 APT 攻击数据集上评估了MAGIC的性能和开销：DARPA Transparent Computing E3 数据集、StreamSpot 数据集和 Unicorn Wget 数据集。DARPA 数据集包含真实世界的攻击，而 StreamSpot 和 Unicorn Wget 数据集则在受控环境中完全模拟。评估结果表明，MAGIC 能够以 97.26% 的准确率和 99.91% 的召回率执行实体级 APT 检测，并且开销最小，对内存的要求更低，并且比最先进的方法快得多&lt;/p&gt;
&lt;p&gt;contribution总结&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了MAGIC，这是一种基于掩码图表示学习和异常值检测方法的通用APT检测方法，能够对海量审计日志进行多粒度检测。&lt;/li&gt;
&lt;li&gt;通过扩展的图形掩码自动编码器将计算开销降至最低，从而确保 MAGIC 的实用性，即使在狭小的条件下，也能在可接受的时间内完成训练和检测。&lt;/li&gt;
&lt;li&gt;通过各种努力确保MAGIC的普遍性。我们利用掩码图表示学习和异常值检测方法，使 MAGIC 能够在不同的监管级别、不同的检测粒度和来自不同来源的审计日志下进行精确检测。&lt;/li&gt;
&lt;li&gt;在三个广泛使用的数据集上评估了 MAGIC，涉及真实世界和模拟的 APT 攻击。评估结果表明，MAGIC检测的APTs具有良好的结果和最小的计算开销。&lt;/li&gt;
&lt;li&gt;提供 MAGIC 的开源实现，以造福社区未来的研究，并鼓励进一步改进我们的方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;攻击场景&#34;&gt;攻击场景&lt;/h3&gt;
&lt;p&gt;在这里，我们提供了我们在整篇文章中使用的 APT 场景的详细说明。带有 Drakon Dropper 的 Pine 后门是来自 DARPA Engagement 3 Trace 数据集的 APT 攻击 [20]。在攻击过程中，攻击者构建恶意可执行文件 （/tmp/tcexec） 并通过网络钓鱼电子邮件将其发送到目标主机。然后，用户会无意识地下载并打开电子邮件。电子邮件中包含的可执行文件旨在执行用于内部侦测的端口扫描，并在目标主机和攻击者之间建立静默连接。&lt;/p&gt;
&lt;p&gt;图 1 显示了我们的动机示例的出处图。图中的节点表示系统实体，箭头表示实体之间的定向交互。显示的图是通过删除大多数与攻击无关的实体和交互，从完整的来源图中抽象出来的子图。不同的节点形状对应不同类型的实体。被条纹覆盖的实体被视为恶意实体。&lt;/p&gt;
&lt;p&gt;![1709698742850](MA G IC1709698742850.png)&lt;/p&gt;
&lt;h3 id=&#34;prior-research-and-their-limitations&#34;&gt;Prior Research and their Limitations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;监督方法&lt;/strong&gt;：对于早期作品，需要&lt;strong&gt;构建特殊的启发式规则&lt;/strong&gt;来涵盖所有攻击模式。许多基于深度学习的APT检测方法基于良性和攻击性数据构建来源图，并以分类方式检测APT。这些监督方法可以在学习的攻击模式上获得近乎完美的检测结果，但在&lt;strong&gt;面临概念漂移或看不见的攻击模式时尤其容易受到攻击&lt;/strong&gt;。此外，对于基于规则的方法，启发式&lt;strong&gt;规则的构建和维护可能非常昂贵和耗时&lt;/strong&gt;。对于基于深度学习的方法，包含攻击的数据的稀缺性阻碍了这些监督方法的实际部署。针对上述问题，MAGIC 采用完全自监督的异常检测方式，在有效处理不可见攻击模式的同时，允许不存在包含攻击的数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于统计的方法&lt;/strong&gt;：最新的基于统计学的方法通过识别系统实体、交互和社区的稀有性或异常分数来检测APT信号。然而，系统实&lt;strong&gt;体的稀有性并不一定表明它们的异常&lt;/strong&gt;，通过因果分析或标签传播获得的异常评分是来源图上的浅层特征提取。为了说明这一点，在我们的攻击示例中，进程 tcexec 对不同的 IP 地址执行多个端口扫描操作（参见图 1），这可以被视为正常的系统行为。但是，考虑到源自外部网络的进程 tcexec 也会读取敏感的系统信息 （uname） 并与公共 IP 地址 （162.66.239.75） 建立连接，我们可以很容易地将 tcexec 识别为恶意实体。由于无法提取系统实体之间的深层语义和相关性，通常会导致基于统计的方法检测性能低下和误报率高。然而，MAGIC采用图表示模块对来源图进行深度图特征提取，从而产生高质量的嵌入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于 DL 的方法&lt;/strong&gt;：最近，基于DL的APT检测方法，无论是有监督还是无监督，都产生了非常有希望的检测结果。然而，在现实中，中型企业每天会产生数百GB的审计日志。因此，基于深度学习的方法，特别是基于序列的和基于图形的方法，由于其&lt;strong&gt;计算开销&lt;/strong&gt;而不可行。例如，ATLAS平均需要 1 小时才能在 676MB 的审计日志上进行训练，而 ShadeWatcher在具有 GPU 的 DARPA E3 Trace 数据集上训练平均需要 1 天。此外，一些基于图自编码器的方法在来源图规模扩大时会遇到爆炸性内存开销问题。MAGIC 通过&lt;strong&gt;引入图形掩码自动编码器避免了计算要求高&lt;/strong&gt;，并在短短几分钟内完成了对 DARPA E3 Trace 数据集的训练。第 6.4 节中详细介绍了 MAGIC 的性能开销。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;端到端方法&lt;/strong&gt;：除了上面讨论的三个主要局限性之外，还值得一提的是，最新的APT检测方法是端到端检测器，并且专注于一个特定的检测任务。例如，ATLAS专注于端到端的攻击重建，而 Unicorn则从流日志中生成系统级警报。相反，MAGIC的方法是通用的，可以在各种检测场景下执行多粒度APT检测，也可以应用于从不同来源收集的审计日志。&lt;strong&gt;（什么叫通用的？预训练精调？还是知识说能检测多场景多力度就算通用了？）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;threat-model-and-definitions&#34;&gt;Threat Model and Definitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;威胁模型&lt;/strong&gt;：我们假设攻击者来自系统外部，并以系统内的有价值信息为目标。攻击者可能会执行复杂的步骤来实现其目标，但在日志中留下可追踪的证据。系统硬件、操作系统和系统审计软件的组合是我们值得信赖的计算基础。在我们的威胁模型中不考虑毒物攻击和逃避攻击。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;出处图&lt;/strong&gt;：来源图是从原始审计日志中提取的有向循环图。构建来源图是数据来源的常见做法，因为它连接系统实体并呈现它们之间的交互关系。来源图包含代表不同系统实体（例如，进程、文件和套接字）的节点，以及代表系统实体之间交互（例如，执行和连接）的边缘，并标有它们的类型。例如，/tmp/tcexec 是一个 FileObject 系统实体，而 /tmp/tcexec 和 tcexec 之间的边缘是 FileObject 面向 Process 的执行操作（参见图 1）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多粒度检测&lt;/strong&gt;：MAGIC 能够执行&lt;strong&gt;两个粒度&lt;/strong&gt;的 APT 检测：批处理日志级别和系统实体级别。MAGIC的多粒度检测能力催生了两阶段检测方法：首先对流式日志进行批量日志级检测，然后对正批次进行系统实体级检测，以识别详细的检测结果。将这种方法应用于实际环境将有效减少工作量、资源消耗和误报，同时产生详细的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批处理日志级别检测。在这种粒度的 APT 检测下，主要任务是给定来自一致来源的批量审核日志，如果在一批日志中检测到潜在的 APT，MAGIC 会发出警报。与Unicorn类似，MAGIC无法在这种检测粒度下准确定位恶意系统实体和交互。&lt;/li&gt;
&lt;li&gt;系统实体级检测。在这种粒度的APT检测下，检测任务是给定来自一致来源的审计日志，MAGIC能够在这些审计日志中准确定位恶意系统实体。在APT期间识别关键系统实体对于后续任务（如攻击调查和攻击故事恢复）至关重要，因为它提供了可解释的检测结果，并减少了对领域专家的需求以及冗余的手动工作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MAGIC是一种新型的自监督APT检测方法，它利用掩蔽图表示学习和异常值检测方法，能够对海量审计日志进行高效的多粒度检测。MAGIC的流水线由三个主要组件组成：（1）来源图构建，（2）图表示模块和（3）检测模块。它还提供了可选的 （4） 模型适配机制。在训练过程中，MAGIC 用 （1） 转换训练数据，用 （2） 学习图嵌入，用 （3） 记住良性行为。在推理过程中，MAGIC 使用 （1） 转换目标数据，使用训练的 （2） 获得图形嵌入，并通过 （3） 检测异常值。图 2 概述了 MAGIC 架构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709702251187.png&#34;
	width=&#34;1736&#34;
	height=&#34;787&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709702251187_hu550e8822d6ecb8abbb01c6bde22a32f3_245553_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709702251187_hu550e8822d6ecb8abbb01c6bde22a32f3_245553_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709702251187&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;529px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;系统审计软件收集的流式审计日志通常以批量方式存储。在来源图构建 （1） 期间，MAGIC 将这些日志转换为静态来源图。系统实体及其之间的交互被提取并分别转换为节点和边。使用几种降低复杂性的技术来删除冗余信息。 	然后，将构建的出处图通过图表示模块（2）馈送，以获得输出嵌入（即对象的综合向量表示）。图表示模块基于图屏蔽自动编码器构建，并集成了基于样本的结构重构，将节点和边属性嵌入、传播和聚合到输出嵌入中，这些嵌入包含节点嵌入和系统状态嵌入。图形表示模块仅使用良性审核日志进行训练，以对良性系统行为进行建模。在对可能包含攻击的审计日志执行 APT 检测时，MAGIC 利用基于输出嵌入的异常值检测方法来检测系统行为中的异常值 （3）。根据任务的粒度，使用不同的嵌入来完成 APT 检测。在批处理日志级任务中，反映整个系统一般行为的系统状态嵌入是检测目标。此类嵌入中的异常值意味着其相应的系统状态是看不见的，并且可能是恶意的，这会显示该批次中的 APT 信号。在系统实体级任务中，检测目标是那些节点嵌入，它们表示系统实体的行为。节点嵌入中的异常值表示可疑的系统实体，并以更精细的粒度检测 APT 威胁。&lt;/p&gt;
&lt;p&gt;在实际检测设置中，MAGIC 有两个预先设计的应用程序。对于系统审计软件收集的每批日志，可以直接利用MAGIC的实体级检测来准确识别批次中的恶意实体，也可以按照第2.3节的规定进行两阶段检测。在这种情况下，MAGIC 首先扫描批次并查看批次中是否存在恶意信号（批处理日志级别检测）。如果警报为阳性，则 MAGIC 将执行实体级检测，以更精细的粒度识别恶意系统实体。与实体级检测相比，批量日志级别检测的计算要求要低得多。因此，这样的两阶段例程可以帮助MAGIC的用户节省计算资源，避免误报，同时不影响MAGIC的检测细度。但是，如果用户喜欢对所有系统实体进行细粒度检测，则前一个例程仍然是一个可访问的选项。&lt;/p&gt;
&lt;p&gt;为了应对概念漂移和看不见的攻击（unseen attack），采用了可选的模型适配机制为其用户提供反馈渠道（4）。由安全分析师检查和确认的检测结果将反馈给 MAGIC，帮助其以半监督的方式适应良性系统行为变化。在这种情况下，MAGIC获得了更有希望的检测结果，这将在第6.3节中讨论。此外，MAGIC 可以很容易地应用于现实世界的在线 APT 检测，这要归功于它能够适应概念漂移和最小的计算开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1用的啥？2用的基于图屏蔽自动编码器，3用的啥？KNN? &lt;del&gt;（4）是干啥的？&lt;/del&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;（4）是靠人工标注进行监督学习的。&lt;/p&gt;
&lt;h3 id=&#34;unseen-attack&#34;&gt;unseen attack&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Unseen attack&amp;rdquo;（不可见攻击）是一种高级持续性威胁（Advanced Persistent Threat，APT）攻击中的一种策略。这种攻击手段的目标是使攻击者的活动对受害者尽可能地不可见，让受害者很难察觉到自己受到了攻击。&lt;/p&gt;
&lt;p&gt;这种类型的攻击通常采取了多种隐蔽的方法，旨在规避传统安全监控和检测工具的检测。一些常见的不可见攻击技术包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;无文件攻击（Fileless Attacks）&lt;/strong&gt;：这种攻击方式不会在受害者系统上留下可被传统防病毒软件等检测到的文件。攻击者通过利用系统内置的工具或脚本语言，例如 PowerShell 或 WMI (Windows Management Instrumentation)，在内存中执行恶意代码。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐蔽通信&lt;/strong&gt;：攻击者会使用加密或隐藏的通信渠道，以避免被网络监控和入侵检测系统检测到。这可能包括使用加密协议、隐蔽通信端口或者隐藏在合法网络流量中的恶意数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;低频攻击&lt;/strong&gt;：攻击者会在较长时间间隔内执行活动，以减少被检测到的风险。这种攻击方式通常不会引起系统管理员的注意，因为攻击活动没有频繁发生。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续访问&lt;/strong&gt;：攻击者在成功进入受害者网络后，会尽可能长时间地保持对系统的访问，以获取更多的信息和权限。他们可能会隐藏在系统的深层次，并悄悄地窃取数据或监视受害者的活动。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态改变攻击模式&lt;/strong&gt;：攻击者会不断地改变他们的攻击方式和工具，以规避传统的安全防御措施。这种变化性可以使传统的签名检测和规则检测方法失效。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;design-details&#34;&gt;Design Details&lt;/h2&gt;
&lt;h3 id=&#34;provenance-graph-construction&#34;&gt;Provenance Graph Construction&lt;/h3&gt;
&lt;p&gt;MAGIC 首先从原始审计日志中构建出处图，然后再执行图表示和 APT 检测。我们遵循三个步骤来构建一致且优化的来源图，为图表示做好准备。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;日志解析&lt;/strong&gt;：第一步是简单地解析每个日志条目，提取系统实体以及它们之间的系统交互。然后，可以构建一个原型出处图，以系统实体为节点，以交互为边。现在，我们提取有关节点和边的分类信息。对于提供实体和交互标签的简单日志格式，我们直接使用这些标签。对于提供这些实体和交互的复杂属性的某种格式，我们应用多标签哈希（例如，xxhash）将属性转换为&lt;strong&gt;标签&lt;/strong&gt;。在这个阶段，来源图是有向多图。我们设计了一个示例来演示如何处理图 3 中日志解析后的原始来源图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;初始嵌入&lt;/strong&gt;：在这个阶段，我们将节点和边标签转换为维度 d 的固定大小的特征向量（即初始嵌入），其中 d 是图表示模块的隐藏维度。我们应用了查找嵌入，在节点/边标签和 d 维特征向量之间建立了一对一的映射。如图 3（I 和 II）所示，进程 a 和 b 共享相同的标签，因此它们映射到相同的特征向量，而 a 和 c 嵌入到不同的特征向量中，因为它们具有不同的标签。我们注意到，唯一节点/边缘标签的可能数量由数据源（即审计日志格式）决定。因此，查找嵌入在转导设置下工作，不需要学习看不见的标签的嵌入。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709705666067.png&#34;
	width=&#34;811&#34;
	height=&#34;975&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709705666067_hucce759ef0e4eecef2a6b3c179664aa0b_123849_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709705666067_hucce759ef0e4eecef2a6b3c179664aa0b_123849_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709705666067&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;199px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;降噪&lt;/strong&gt;：我们的图表示模块的预期输入出处图将是简单图。因此，我们需要在节点对之间组合多个边。如果一对节点之间存在同一标签的多个边（也共享相同的初始嵌入），我们将删除多余的边，以便只保留其中一条。然后，我们将剩余的边合并为一条最终边。我们注意到，在一对节点之间，可能会保留几个不同标签的边缘。组合后，通过对剩余边的初始嵌入进行平均来获得所得唯一边的初始嵌入。为了说明这一点，我们在图3（II和III）中展示了我们的降噪如何结合多边，以及它如何影响边的初始嵌入。首先，对于每个标签，将 a 和 c 之间的 3 次读取和 2 次写入交互合并为一个。然后我们将它们组合在一起，形成一个边缘 eac，其初始嵌入等于其余边缘的&lt;strong&gt;平均&lt;/strong&gt;初始嵌入（e′ 2 和 e′ 5）。我们在附录E中提供了我们的降噪步骤与以前工作的比较&lt;/p&gt;
&lt;p&gt;简单来说就是先对边和节点进行多标签哈希，目的是将多个类别标签映射到设计的那几个标签上（如果是有标签的日志格式则不需要这一步），这样每个边都有标签，标签就是边的含义（read,write等)。然后将节点和标签转为d维的初始嵌入，再将节点对之间嵌入值相同的边合并成一个边。&lt;/p&gt;
&lt;h3 id=&#34;graph-representation-module&#34;&gt;Graph Representation Module&lt;/h3&gt;
&lt;p&gt;MAGIC 采用图表示模块从特色出处图（featured provenance graphs）中获取高质量的嵌入。如图 4 所示，图表示模块由三个阶段组成：用于部分隐藏节点特征（即初始嵌入）以进行重建的掩码过程，通过传播和聚合图特征生成节点和系统状态输出嵌入的图编码器，图解码器通过掩码特征重建和基于样本的结构为图表示模块的训练提供监督信号重建。编码器和解码器形成图形掩码自动编码器，在生成快速且节省资源的嵌入方面表现出色。&lt;/p&gt;
&lt;h4 id=&#34;feature-masking&#34;&gt;Feature Masking&lt;/h4&gt;
&lt;p&gt;在训练我们的图表示模块之前，我们对节点执行掩码，以便在重建这些节点时可以训练图掩码自动编码器。屏蔽节点是随机选择的，覆盖所有节点的一定比例。此类屏蔽节点的初始嵌入将替换为特殊的屏蔽令牌 xmask，以涵盖有关这些节点的任何原始信息。但是，边缘不会被屏蔽，因为这些边缘提供了有关系统实体之间关系的宝贵信息。总之，给定节点初始嵌入 $x_n$，我们按如下方式屏蔽节点：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/1709708984909.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1709708984909&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其中 $\tilde{N}$ 是随机选择的掩码节点，$emb_n$ 是节点 n 的嵌入，准备训练图表示模块。此掩蔽过程仅在训练期间发生。在检测过程中，我们不会屏蔽任何节点。&lt;/p&gt;
&lt;h4 id=&#34;graph-encoder&#34;&gt;Graph Encoder&lt;/h4&gt;
&lt;p&gt;从图构建步骤中获得的初始嵌入仅考虑原始特征。然而，原始特征还远远不足以对系统实体的详细行为进行建模。实体的上下文信息，如其邻域、多跳关系以及与其他系统实体的交互模式，对于获得高质量的实体嵌入起着重要作用。在这里，我们采用并扩展了&lt;strong&gt;图形屏蔽自编码器&lt;/strong&gt;，以自监督的方式生成输出嵌入。图形屏蔽自动编码器由编码器和解码器组成。编码器通过传播和聚合图特征来生成输出嵌入，解码器重建图特征以提供用于训练的监督信号。这种编码器-解码器架构在生成的嵌入中保留了上下文和语义信息，同时通过掩蔽学习显着降低了其计算开销。&lt;/p&gt;
&lt;p&gt;我们的图表示模块的编码器包含多个堆叠层的图注意力网络（GAT）。GAT层的功能是根据节点本身及其相邻节点的特征（初始嵌入）生成输出节点嵌入。与普通的GNN不同，GAT引入了一种注意力机制来衡量这些邻居的重要性。【GAT加了attention真的还好算吗？训练代价应该也不小吧】&lt;/p&gt;
&lt;p&gt;为了详细解释，GAT 的一层将前几层生成的节点嵌入作为输入，并将嵌入从源节点传播到目标节点，并沿交互传播到消息中。该消息包含有关源节点以及源节点和目标节点之间交互的信息：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710234712.png&#34;
	width=&#34;523&#34;
	height=&#34;97&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710234712_hua36d1db2226ef0c7dab718083c445773_7961_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709710234712_hua36d1db2226ef0c7dab718083c445773_7961_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709710234712&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;539&#34;
		data-flex-basis=&#34;1294px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;并采用注意力机制来计算消息源与其目的地之间的注意力系数：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710298170.png&#34;
	width=&#34;819&#34;
	height=&#34;113&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710298170_hu85b48369b6085df9fdcff38080ffe969_17564_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709710298170_hu85b48369b6085df9fdcff38080ffe969_17564_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709710298170&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;724&#34;
		data-flex-basis=&#34;1739px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然后，对于目标节点，GAT 聚合来自传入边缘的消息，以通过计算所有传入消息的加权和来更新其节点嵌入。权重正是注意力系数：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710344097.png&#34;
	width=&#34;763&#34;
	height=&#34;185&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710344097_hu5c46dd1cd3605c2d547afa478ca5264b_16511_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709710344097_hu5c46dd1cd3605c2d547afa478ca5264b_16511_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709710344097&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;412&#34;
		data-flex-basis=&#34;989px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中 $h^l_n$ 是 GAT 第 l 层节点 n 的隐藏嵌入，$h^{l-1}_n$ 是 l − 1 层的隐藏嵌入，$\mathcal{N}&lt;em&gt;n$ 是 n 的单跳邻域。第一个 GAT 层的输入是初始节点嵌入。Embe 是初始边缘嵌入，在整个图形表示模块中保持不变。$W_as$，$W_am$，$W&lt;/em&gt;{self}$ ，$W{msg}$ 是可训练的参数。更新的节点嵌入构成了节点单跳交互行为的一般抽象。&lt;/p&gt;
&lt;p&gt;将此类 GAT 的多层堆叠以获得最终的节点嵌入 h，该节点嵌入由原始节点嵌入和所有 GAT 层的输出连接起来：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710765514.png&#34;
	width=&#34;391&#34;
	height=&#34;81&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710765514_hud313a304217bb9525bb8ee55275c4f0c_4387_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709710765514_hud313a304217bb9525bb8ee55275c4f0c_4387_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709710765514&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;482&#34;
		data-flex-basis=&#34;1158px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中 ·||·表示串联操作。GAT堆叠的层数越多，相邻范围就越宽，节点的多跳交互模式能够表示的就越远。因此，图编码器有效地结合了节点初始特征和多跳交互行为，将系统实体行为抽象到节点嵌入中。图编码器还对所有节点嵌入应用平均池化，以生成图本身的全面嵌入，它概括了系统的整体状态：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710804533.png&#34;
	width=&#34;295&#34;
	height=&#34;117&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709710804533_hu18cf87b6386ab6bddc11df307a6c214f_4926_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709710804533_hu18cf87b6386ab6bddc11df307a6c214f_4926_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709710804533&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;605px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;图编码器生成的节点嵌入和系统状态嵌入被视为图表示模块的输出，用于不同场景下的后续任务。&lt;/p&gt;
&lt;h4 id=&#34;graph-decoder&#34;&gt;Graph Decoder&lt;/h4&gt;
&lt;p&gt;图形编码器不提供支持模型训练的监督信号。在典型的图自编码器中，使用图解码器对节点嵌入进行解码，并通过特征重构和结构重构来监督模型训练。然而，图形屏蔽自编码器放弃了结构重建，以减少计算开销。我们的图解码器是两者的混合体，它集成了掩码特征重建和基于样本的结构重建，以构建优化图表示模块的目标函数。&lt;/p&gt;
&lt;p&gt;给定从图编码器获得的节点嵌入 $h_n$，解码器首先重新屏蔽这些屏蔽节点，并将它们转换为屏蔽特征重建的输入：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711180082.png&#34;
	width=&#34;433&#34;
	height=&#34;113&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711180082_hu051c36bc1b5666d451eb10a0eb894d14_9252_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709711180082_hu051c36bc1b5666d451eb10a0eb894d14_9252_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709711180082&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;383&#34;
		data-flex-basis=&#34;919px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;随后，解码器使用上述类似的GAT层来重建掩码节点的初始嵌入，从而可以计算特征重建损失：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711206698.png&#34;
	width=&#34;513&#34;
	height=&#34;187&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711206698_hu77ff8b359efadd2edc7202c3b77a4dc5_14330_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709711206698_hu77ff8b359efadd2edc7202c3b77a4dc5_14330_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709711206698&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;274&#34;
		data-flex-basis=&#34;658px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;【所以这个掩码节点是拿个seed全局保留的马？】&lt;/p&gt;
&lt;p&gt;其中 $L_{fr} $是通过计算掩码节点的初始嵌入和重建嵌入之间的缩放余弦损失获得的掩蔽特征重建损失。这种损失在简单样本和困难样本之间急剧增加，从而有效地加快了学习速度。这种缩放的程度由超参数γ控制。&lt;/p&gt;
&lt;p&gt;与此同时，基于样本的结构重建旨在重建图结构（即预测节点之间的边）。与重建整个邻接矩阵不同，后者具有O(N2)的复杂度，基于样本的结构重建在节点对上应用对比采样，并预测这些节点对之间的边概率。仅非掩蔽节点参与结构重建。正样本是由所有非掩蔽节点之间的所有现有边构建的，负样本则是在那些没有现有边的节点对中进行采样构建的。&lt;/p&gt;
&lt;p&gt;使用简单的两层 MLP 重建节点对样本之间的边，为每个样本生成一个概率。在这些样本上，重建损失的形式为简单的二元交叉熵损失：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711513256.png&#34;
	width=&#34;819&#34;
	height=&#34;185&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711513256_hu32346daf17371ca74b8acb0e42c31b4c_17333_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709711513256_hu32346daf17371ca74b8acb0e42c31b4c_17333_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709711513256&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;442&#34;
		data-flex-basis=&#34;1062px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中 （n， n−） 和 （n， n+） 分别是负样本和正样本，ˆ N =N− e N 是一组非掩码节点。基于样本的结构重建仅对输出嵌入进行监督。我们没有使用点积，而是使用 MLP 来计算边缘概率，因为交互实体的行为不一定相似。此外，我们并没有强迫模型学习预测边缘概率。这种结构重构的功能是最大化抽象节点嵌入中包含的行为信息，以便一个简单的MLP足以将这些信息合并并解释为边缘概率。&lt;/p&gt;
&lt;p&gt;最终的目标函数 L = $L_fr$ + $L_sr$ 结合了 $L_fr$ 和 $L_sr$，并为图表示模块提供监督信号，使其能够以自监督的方式学习参数。&lt;/p&gt;
&lt;h3 id=&#34;detection-module&#34;&gt;Detection Module&lt;/h3&gt;
&lt;p&gt;基于图表示模块生成的输出嵌入，利用异常值检测方法以无监督方式进行APT检测。如前几节所述，此类嵌入以不同的粒度总结了系统行为。我们的检测模型的目标是识别恶意系统实体或状态，前提是仅对良性系统行为有先验的了解。如果通过图表示学习生成的嵌入在图中具有相似的交互行为，则它们的相应实体往往会形成聚类。因此，系统状态嵌入中的异常值表示不常见和可疑的系统行为。基于这样的洞见，我们开发了一种特殊的异常值检测方法来进行APT检测。&lt;/p&gt;
&lt;p&gt;在训练过程中，首先从训练来源图中抽象出良性输出嵌入。在这个阶段，检测模块所做的只是记住这些嵌入，并将它们组织在一个K-D树中[33]。经过训练后，检测模块通过三个步骤揭示异常值：k-最近邻搜索、相似度计算和过滤。给定目标嵌入，检测模块首先通过 K-D 树搜索获得其 k 最近邻。这样的搜索过程只需要 log（N） 时间，其中 N 是记忆训练嵌入的总数。然后，应用相似性准则来评估目标嵌入与其邻居的接近程度并计算异常分数。如果其异常分数高于超参数 θ，则目标嵌入被视为异常值，其相应的系统实体或系统状态为恶意。检测模块的示例工作流程形式化如下，使用欧几里得距离作为相似性准则：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711997347.png&#34;
	width=&#34;399&#34;
	height=&#34;339&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709711997347_hu17c2e8d081b68e3a8299bded3e1504fa_23202_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709711997347_hu17c2e8d081b68e3a8299bded3e1504fa_23202_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709711997347&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;282px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中 $\overline{dist}$ 是训练嵌入与其 k 最近邻之间的平均距离。在执行批处理日志级别检测时，检测模块会记住反映系统状态的良性系统状态嵌入，并检测新到达的来源图的系统状态嵌入是否为异常值。在执行系统实体级检测时，检测模块会记住指示系统实体行为的良性节点嵌入，并给定一个新到达的来源图，它会检测所有系统实体嵌入中的异常值。&lt;/p&gt;
&lt;h3 id=&#34;model-adaption&#34;&gt;Model Adaption&lt;/h3&gt;
&lt;p&gt;为了使APT检测器在实际检测场景中有效运行，必须考虑概念漂移。当面对良性但以前未见过的系统行为时，MAGIC 会产生误报检测结果，这可能会误导后续应用程序（例如攻击调查和故事恢复）。最近的工作通过忘记过时的数据[10]或通过模型适应机制[18]将他们的模型拟合到良性系统变化来解决这个问题。MAGIC还集成了模型适应机制，以对抗概念漂移，并从安全分析师识别的误报中学习。与其他仅使用误报来重新训练模型的作品略有不同，MAGIC可以使用所有反馈进行重新训练。如前几节所述，MAGIC 中的图形表示模块以自监督的方式将系统实体编码为嵌入，而无需知道其标签。任何看不见的数据，包括那些true negative，都是图表示模块的宝贵训练数据，以增强其对看不见的系统行为的表示能力。&lt;/p&gt;
&lt;p&gt;检测模块只能通过良性反馈进行重新训练，以跟上系统行为的变化。而且随着它记住越来越多的良性反馈，它的检测效率会降低。为了解决这个问题，我们还在检测模块上实现了折扣机制。当记忆嵌入的数量超过一定数量时，随着新到达的嵌入被学习，最早的嵌入被简单地删除。我们提供模型适配机制作为概念漂移和看不见的系统行为的可选解决方案。建议通过将确认的假阳性样本提供给 MAGIC 的模型适应机制来使 MAGIC 适应系统变化。&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;我们在 Python 3.8 中使用了大约 3,500 行代码实现了 MAGIC。我们开发了几个日志解析器来应对不同格式的审计日志，包括 StreamSpot [34]、Camflow [35] 和 CDM [36]。来源图是使用图处理库 Networkx [37] 构建的，并以 JSON 格式存储。图形表示模块是通过 PyTorch [38] 和 DGL [39] 实现的。该检测模块是用Scikit-learn[40]开发的。对于MAGIC的超参数，特征重建损失中的比例因子γ设置为3，相邻变量k设置为10，学习率为0.001，权重衰减因子等于5×10−4。我们在实验中使用了 3 层图形编码器和 0.5 的掩码率。在批处理日志级别检测和实体级检测两种检测方案中，输出嵌入维度 d 是不同的。我们在批处理日志级别检测中使用 d 等于 256，在实体级检测中使用 和 an 等于 64 以减少资源消耗。检测阈值 θ 是通过对每个数据集分别进行的简单线性搜索来选择的。超参数可能有其他选择。我们将在稍后的评估部分演示这些超参数对 MAGIC 的影响。在我们的超参数分析中，d 是从 {16， 32， 64， 128， 256} 中选出的，l 是从 {1， 2， 3， 4} 中选出的，r 是从 {0.3， 0.5， 0.7} 中选出的。对于阈值 θ，在批处理日志级别检测中选择介于 1 和 10 之间。有关实体级检测，请参阅附录 D。&lt;/p&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;我们使用来自各种系统审计软件的 131GB 审计日志来评估 MAGIC 的有效性和效率。我们首先描述了我们的实验设置（第 6.1 节），然后详细说明了 MAGIC 在不同场景中的有效性（第 6.2 节），进行误报分析并评估模型适应机制的有用性（第 6.3 节），并分析了 MAGIC 的运行时性能开销（第 6.4 节）。MAGIC的不同组件和超参数的影响在第6.5节中进行了分析。此外，附录 C 中还对我们的动机示例进行了详细的案例研究，以说明 MAGIC 的管道如何用于 APT 检测。这些实验在相同的设备设置下进行。&lt;/p&gt;
&lt;h3 id=&#34;experimental-settings&#34;&gt;Experimental Settings&lt;/h3&gt;
&lt;p&gt;我们评估了MAGIC在三个公共数据集上的有效性：StreamSpot数据集[21]，Unicorn Wget数据集[22]和DARPA Engagement 3数据集[20]。这些数据集在数量、来源和粒度方面各不相同。我们相信，通过在这些数据集上测试MAGIC的性能，我们能够将MAGIC与尽可能多的最先进的APT检测方法进行比较，并探索MAGIC的普遍性和适用性。我们对这三个数据集的详细说明如下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;StreamSpot 数据集&lt;/strong&gt;：StreamSpot数据集（见表1）是由StreamSpot[34]使用审计系统SystemTap [41]收集并公开的模拟数据集。StreamSpot 数据集包含 600 批次审计日志，用于监控 6 个独特场景下的系统调用。其中五个方案是模拟的良性用户行为，而攻击方案模拟的是偷渡式下载攻击。该数据集被认为是一个相对较小的数据集，由于没有提供日志条目和系统实体的标签，因此我们对 StreamSpot 数据集执行批量日志级别检测，类似于以前的工作 [10， 15， 17]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unicorn Wget 数据集&lt;/strong&gt;：Unicorn Wget数据集（见表1）包含Unicorn[10]设计的模拟攻击。具体来说，它包含 Camflow [35] 收集的 150 批日志，其中 125 批是良性的，其中 25 批包含供应链攻击。这些攻击被归类为隐形攻击，经过精心设计，其行为类似于良性系统工作流程，预计很难识别。这个数据集被认为是我们实验数据集中最难的，因为它的体积大，日志格式复杂，而且这些攻击的隐蔽性。与最先进的方法相同，我们在此数据集上执行批处理日志级别检测。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709712995732.png&#34;
	width=&#34;817&#34;
	height=&#34;325&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709712995732_hu038a9df923508568e84903c6bd1623d5_49229_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709712995732_hu038a9df923508568e84903c6bd1623d5_49229_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709712995732&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;251&#34;
		data-flex-basis=&#34;603px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DARPA E3 数据集&lt;/strong&gt;：DARPA Engagement 3 数据集（见表 2）作为 DARPA 透明计算计划的一部分，在对抗性参与期间在企业网络中收集。利用不同漏洞的 APT 攻击 [20] 由红队进行，以泄露敏感信息。蓝队试图通过审核网络主机并对其执行因果关系分析来识别这些攻击。Trace、THEIA 和 CADETS 子数据集包含在我们的评估中。这三个子数据集总共包含 51.69GB 的审计记录，包含多达 6,539,677 个系统实体和 68,127,444 次交互。因此，我们评估了MAGIC的系统实体级检测能力，并解决了这些数据集的开销问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713015449.png&#34;
	width=&#34;831&#34;
	height=&#34;329&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713015449_hud9843b27b4024551280b826d1d324b03_52173_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713015449_hud9843b27b4024551280b826d1d324b03_52173_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713015449&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;606px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对于不同的数据集，我们采用不同的数据集拆分来评估模型，并且我们仅使用良性样本进行训练。对于 StreamSpot 数据集，我们从 500 个良性日志中随机选择 400 个批次进行训练，其余批次进行测试，从而形成一个平衡的测试集。对于 Unicorn Wget 数据集，选择了 100 批良性日志进行训练，其余用于测试。对于 DARPA E3 数据集，我们使用与 ThreaTrace [17] 相同的真值标签，并根据其出现的顺序拆分日志条目。最早的 80% 日志条目用于训练，其余条目保留用于测试。在评估过程中，MAGIC在100个全局随机种子下的平均性能被报告为最终结果，因此实验结果可能包含系统实体/日志批次的分数。&lt;/p&gt;
&lt;h3 id=&#34;effectiveness&#34;&gt;Effectiveness&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713083774.png&#34;
	width=&#34;839&#34;
	height=&#34;521&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713083774_hucc8f575a68082b3edb5e18e57ba59e25_61653_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713083774_hucc8f575a68082b3edb5e18e57ba59e25_61653_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713083774&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;161&#34;
		data-flex-basis=&#34;386px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713103510.png&#34;
	width=&#34;1731&#34;
	height=&#34;383&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713103510_hu0c8ae3e5da56cd76803a95e065c0c69a_103069_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713103510_hu0c8ae3e5da56cd76803a95e065c0c69a_103069_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713103510&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;451&#34;
		data-flex-basis=&#34;1084px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713117689.png&#34;
	width=&#34;837&#34;
	height=&#34;743&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713117689_hu4387d49ca41b8e66fefd80563e191f71_90569_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713117689_hu4387d49ca41b8e66fefd80563e191f71_90569_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713117689&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;270px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713129950.png&#34;
	width=&#34;837&#34;
	height=&#34;727&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713129950_hu3fa0a3dbdbed859ed7a4be337bec3ab2_156190_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713129950_hu3fa0a3dbdbed859ed7a4be337bec3ab2_156190_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713129950&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;115&#34;
		data-flex-basis=&#34;276px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713153070.png&#34;
	width=&#34;885&#34;
	height=&#34;801&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713153070_hud6612d36fb5c0e22a75c5a5a157aca07_106887_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713153070_hud6612d36fb5c0e22a75c5a5a157aca07_106887_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713153070&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;110&#34;
		data-flex-basis=&#34;265px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713173936.png&#34;
	width=&#34;857&#34;
	height=&#34;509&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713173936_hu8fc3646338be836665a4c5a8db534fea_62888_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713173936_hu8fc3646338be836665a4c5a8db534fea_62888_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713173936&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713196429.png&#34;
	width=&#34;1561&#34;
	height=&#34;587&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713196429_hu0fad53adbd08d285dc62c0f6b0e42f51_93870_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713196429_hu0fad53adbd08d285dc62c0f6b0e42f51_93870_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713196429&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;265&#34;
		data-flex-basis=&#34;638px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713209284.png&#34;
	width=&#34;867&#34;
	height=&#34;525&#34;
	srcset=&#34;https://blog2.pillar.fun/p/magic/MAGIC/1709713209284_hud365458e9fec964efcc85d1f686cb918_54083_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/magic/MAGIC/1709713209284_hud365458e9fec964efcc85d1f686cb918_54083_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709713209284&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;396px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;conclusion&lt;/h2&gt;
&lt;p&gt;我们推出了 MAGIC，这是一种普遍适用的 APT 检测方法，它以最高的效率运行，开销很小。MAGIC 利用掩码图表示学习从原始审计日志中对良性系统行为进行建模，并通过异常值检测方法执行多粒度 APT 检测。在各种检测场景下对三个广泛使用的数据集进行评估表明，MAGIC以低误报率和最小的计算开销取得了良好的检测结果。&lt;/p&gt;
&lt;h2 id=&#34;知识补充&#34;&gt;知识补充&lt;/h2&gt;
&lt;h3 id=&#34;多标签哈希&#34;&gt;多标签哈希&lt;/h3&gt;
&lt;p&gt;多标签哈希（Multi-Label Hashing）是一种用于解决多标签分类问题的技术。在多标签分类问题中，每个样本可以属于一个或多个类别，而不是单一的类别。&lt;/p&gt;
&lt;p&gt;哈希是一种将数据映射到固定长度的二进制编码的方法。多标签哈希技术将这种哈希方法应用于多标签分类问题，将样本的多个标签映射为固定长度的二进制编码，从而方便快速的类别检索和处理。&lt;/p&gt;
&lt;p&gt;以下是多标签哈希的一些关键思想和方法：&lt;/p&gt;
&lt;h4 id=&#34;1-单一哈希方法&#34;&gt;1. 单一哈希方法&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bit-Vector Hashing&lt;/strong&gt;：最简单的多标签哈希方法之一是将每个标签映射为二进制编码的位向量。例如，如果有5个类别，则可以用5位二进制编码来表示每个标签，如 &lt;code&gt;[1, 0, 1, 1, 0]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-多哈希方法&#34;&gt;2. 多哈希方法&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Binary Relevance&lt;/strong&gt;：这种方法将每个标签独立地进行哈希处理。对于每个标签，都使用一个单独的哈希函数，将其映射为固定长度的二进制编码。这种方法简单直观，但可能导致标签之间的相关性被忽略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple Hashing&lt;/strong&gt;：这种方法使用多个哈希函数，将每个标签映射为多个不同的二进制编码。这可以捕捉到标签之间的一些相关性，提高多标签哈希的性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-哈希函数的选择&#34;&gt;3. 哈希函数的选择&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;局部敏感哈希（Locality Sensitive Hashing，LSH）&lt;/strong&gt;：LSH 是一种常用的哈希方法，它能够使相似的样本在哈希空间中映射为相邻的编码。这有助于快速的近似最近邻（ANN）搜索，对于多标签分类中的相似性检索很有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Learning Hashing&lt;/strong&gt;：使用深度学习模型学习哈希函数也是一种常见的方法。例如，可以使用卷积神经网络（CNN）或循环神经网络（RNN）来学习将标签映射为二进制编码的函数。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-检索和评估&#34;&gt;4. 检索和评估&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;哈希编码检索&lt;/strong&gt;：一旦对样本进行了哈希处理，可以使用快速的哈希编码检索技术来查找与查询标签最相似的样本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：对于多标签哈希，常用的评估指标包括 Hamming Loss、Hamming Distance、Precision、Recall、F1 Score 等，用于衡量模型的分类准确性和性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多标签哈希方法可以提高对于多标签数据的处理效率和准确性，尤其在大规模的多标签数据集中具有很好的应用前景。&lt;/p&gt;
&lt;h3 id=&#34;图形掩蔽自编码器&#34;&gt;图形掩蔽自编码器&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Graph Masked Autoencoders&amp;rdquo; 是一种用于图形数据的自编码器模型，旨在学习图形数据的低维度表示。&lt;/p&gt;
&lt;p&gt;这个模型的主要思想是结合了自编码器（Autoencoder）的概念和图形数据的结构。自编码器是一种无监督学习算法，用于学习数据的紧凑表示，并在重建时最大程度地保留原始数据的信息。&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Graph Masked Autoencoders&amp;rdquo; 在这个基础上加入了一种“掩蔽”机制，用于处理图形数据。这个“掩蔽”机制的目的是在训练过程中限制模型只能看到部分图形数据，从而强制模型学习到更加泛化的图形特征表示。&lt;/p&gt;
&lt;p&gt;具体来说，训练过程包含以下步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;掩蔽图形数据（Masking Graph Data）&lt;/strong&gt;：模型会随机地将一些节点或边从输入图中“掩蔽”（即隐藏），使得模型在训练时只能看到部分图形数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;编码器（Encoder）&lt;/strong&gt;：掩蔽后的图形数据通过编码器部分进行编码，将其映射到一个低维度的特征表示空间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解码器（Decoder）&lt;/strong&gt;：然后，模型尝试从这个低维度表示中重构原始的图形数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这个过程，模型被迫学习到不同节点和边之间的潜在关系，以及如何在只看到部分数据时对图形数据进行有效的编码和解码。&lt;/p&gt;
&lt;p&gt;这种方法的优势在于它能够提高模型对图形数据的泛化能力。因为模型只能看到部分数据，它不会过度依赖于特定的节点或边，从而可以更好地处理未见过的图形数据。&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Graph Masked Autoencoders&amp;rdquo; 的应用包括图形节点分类、图形重构、图形生成等任务。它是图神经网络（Graph Neural Networks）领域中的一种重要技术，用于学习和处理复杂的图形结构数据。&lt;/p&gt;
&lt;h3 id=&#34;概念漂移&#34;&gt;概念漂移&lt;/h3&gt;
&lt;p&gt;概念漂移是指机器学习模型在应用于新数据时性能下降的现象。这种现象通常发生在训练模型的数据分布与新数据的分布不同时。具体来说，概念漂移可能会导致模型在新数据上的预测准确性降低。&lt;/p&gt;
&lt;p&gt;概念漂移可以分为几种不同类型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;特征漂移（Feature Drift）&lt;/strong&gt;：特征漂移是指输入特征的分布在训练数据和测试数据中不同的情况。例如，训练数据中的特征范围可能与测试数据中的范围不同，这会导致模型无法准确地泛化到新数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标签漂移（Label Drift）&lt;/strong&gt;：标签漂移是指目标变量的分布在训练数据和测试数据中不同的情况。换句话说，模型在训练数据中学到的标签分布可能与实际数据中的标签分布不同，从而影响模型的性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;概念漂移（Concept Drift）&lt;/strong&gt;：概念漂移是指预测变量和目标变量之间的关系在时间或数据分布上发生变化。这种情况下，模型在训练时学到的规律可能在应用到新数据时不再适用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;概念漂移是机器学习应用中一个重要的挑战，因为它可能导致模型的预测性能下降，需要采取一些方法来处理。一些应对概念漂移的方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;在线学习（Online Learning）&lt;/strong&gt;：使用新数据不断更新模型，使其能够适应新的数据分布。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;监督漂移检测（Supervised Drift Detection）&lt;/strong&gt;：监测模型在新数据上的表现，当性能下降时触发模型的重新训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;集成学习（Ensemble Learning）&lt;/strong&gt;：结合多个模型的预测结果，以减少概念漂移对整体性能的影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域适应（Domain Adaptation）&lt;/strong&gt;：通过调整模型或数据来使其适应新的数据分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;处理概念漂移是一个活跃的研究领域，因为许多现实世界的应用场景中数据分布经常发生变化。&lt;/p&gt;
&lt;h3 id=&#34;k-d-tree&#34;&gt;K-D tree&lt;/h3&gt;
&lt;p&gt;K-D 树（K-Dimensional Tree，K-Dimensional Binary Tree）是一种用于高效处理k维空间的数据结构，用于解决近似最近邻搜索（Approximate Nearest Neighbor Search）等问题。它是一种二叉树结构，用于对 k 维数据进行分割和组织，以便快速地搜索最近的邻居。&lt;/p&gt;
&lt;h1 id=&#34;我的想法&#34;&gt;我的想法&lt;/h1&gt;
&lt;p&gt;和之前看的ATLAS比较起来，通过mask和减小模型复杂度来实现降低开销，并且没有造成结果的损失。好像就没啥更吊的地方了吧？&lt;/p&gt;
&lt;p&gt;加了个监督学习的步骤&lt;/p&gt;
&lt;p&gt;嵌入阶段用了mask和图&lt;strong&gt;注意力&lt;/strong&gt;网络的技术手段，能获得质量更高，多跳敏感的嵌入。&lt;/p&gt;
&lt;p&gt;通过检测时候通过KD树找到K个临近的embedding，然后通过相似性计算， 设立一个阈值来判断是否具有恶意。再用decoder来还原出embedding所对应的边或者节点&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ATLAS</title>
        <link>https://blog2.pillar.fun/p/atlas/</link>
        <pubDate>Sat, 02 Mar 2024 20:16:17 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/atlas/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/atlas/ATLAS/1709635161753.png" alt="Featured image of post ATLAS" /&gt;&lt;p&gt;文章可以在&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/usenixsecurity21/presentation/alsaheel&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    获取&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;introduction&lt;/h2&gt;
&lt;p&gt;APT攻击涉及长期的多个攻击步骤，其调查需要分析大量日志以确定其攻击步骤。因此提出ATLAS从现成的审计日志构建端到端攻击故事的框架。&lt;/p&gt;
&lt;p&gt;ATLAS基于的观察：&lt;strong&gt;无论利用的漏洞和执行的有效载荷如何，不同的攻击可能具有相似的抽象攻击策略&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;ATLAS利用因果关系分析、自然语言处理和机器学习技术的新颖组合来构建&lt;strong&gt;基于序列&lt;/strong&gt;的模型，该模型从&lt;strong&gt;因果图&lt;/strong&gt;中建立攻击和非攻击行为的关键模式。&lt;/p&gt;
&lt;p&gt;取证分析从多个主机、应用程序和网络接口收集各种审计日志。海量日志通常被离线分析或实时监控，以调试系统故障并识别复杂的威胁和漏洞。&lt;/p&gt;
&lt;p&gt;现有方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从审计日志中构建因果依赖关系图，并使用查询系统来定位关键攻击阶段（例如，受损进程或恶意负载）。&lt;/li&gt;
&lt;li&gt;扩展机器学习（ML）技术，从日志中提取特征/序列，以自动检测入侵和故障。&lt;/li&gt;
&lt;li&gt;构建了通过事件关联发现不同日志事件之间关联的技术&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些方法在很大程度上无法精确定位关键攻击步骤，从而有效地突出端到端攻击故事。因此我们希望从审计日志中识别关键实体（节点），帮助网络分析师构建 APT 攻击的关键步骤。&lt;/p&gt;
&lt;p&gt;ATLAS将自然语言处理 （NLP） 和深度学习技术集成到数据来源分析中，以识别攻击和非攻击序列。分为三个阶段&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;处理系统日志并构建自己的优化因果依赖图&lt;/li&gt;
&lt;li&gt;通过NLP技术从因果图中构建语义增强序列（时间戳事件）&lt;/li&gt;
&lt;li&gt;学习表示攻击语义的基于序列的模型，这有助于在推理时恢复描述攻击故事的关键攻击实体。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ATLAS不会带来额外开销，不同的审计日志可以很容易地集成到 ATLAS 日志解析器中用来构建因果图并获得精确的序列和模型&lt;/p&gt;
&lt;p&gt;我们的方法基于：&lt;strong&gt;因果依赖关系图中不同攻击的关键步骤可能具有相似的模式&lt;/strong&gt;。这些模式可以通过NLP技术（即词形还原和词嵌入）转换为序列，将攻击和非攻击实体之间各种变化形式的关系组合在一起。它为模型提供了具有不同因果关系的更深层次的&lt;strong&gt;记忆&lt;/strong&gt;。，进而提高了序列模型从未知审计日志中识别攻击步骤的准确性。&lt;/p&gt;
&lt;p&gt;但是这样的方法面对以下三个挑战，相应的，ATLAS采取手段来应对：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;因果图通常庞大而复杂，这使得序列构建变得困难&amp;ndash;&amp;gt;采用定制化的图优化算法来降低图的复杂度&lt;/li&gt;
&lt;li&gt;它需要一种方法来精确构建序列，以有效地模拟合法和可疑的活动&amp;ndash;&amp;gt;提出一种从事件中提取攻击模式序列的新技术&lt;/li&gt;
&lt;li&gt;需要一种自动化方法来识别给定的攻击症状中的攻击事件&amp;ndash;&amp;gt;通过攻击症状进行攻击调查，以恢复攻击事件，帮助全面构建攻击故事。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，ATLAS做了&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;引入了 ATLAS，这是一个用于攻击故事恢复的框架，它利用自然语言处理和基于序列的模型学习技术来帮助网络分析师从审计日志中恢复攻击步骤.&lt;/li&gt;
&lt;li&gt;提出了一种新的序列表示，通过词形还原和词嵌入来抽象攻击和非攻击语义模式。这些序列使 ATLAS 能够构建一个有效的基于序列的模型，以识别构成攻击故事的攻击事件&lt;/li&gt;
&lt;li&gt;我们在受控环境中通过其真实世界报告开发的 10 种现实 APT 攻击中验证了 ATLAS。结果表明，ATLAS能够高精度、最小开销地识别攻击故事的关键攻击条目。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;motivation-and-definitions&#34;&gt;Motivation and Definitions&lt;/h2&gt;
&lt;p&gt;整篇论文中设定了一种攻击场景：攻击者通过电子邮件向企业中的目标用户发送恶意Microsoft Word文件（contract.doc）。用户被欺骗使用 Firefox 从 Gmail 下载和打开 Word 文件。该文档包含一段恶意代码，该代码利用易受攻击的 Microsoft Word （winword.exe） 并发出 HTTPS 请求以下载恶意 Microsoft HTA 脚本 （template.hta）。此脚本执行恶意 Visual Basic 脚本 （maintenance.vbs），其中包含安装后门以泄露敏感文件的 PowerShell 命令。最后，攻击者横向移动到其他主机。&lt;/p&gt;
&lt;p&gt;调查这个场景通常从从审核日志中收集有关攻击的数据开始，例如系统事件、DNS 查询和浏览器事件。攻击调查工具通常以因果图（或来源图）的形式表示审核日志，该图用作取证工具，使安全调查人员能够执行根本原因分析，并更好地了解攻击的性质。大多数先前的研究将因果图中的攻击故事恢复为子图，其中该图中的节点和边与攻击症状具有因果关系。图 1 （a） 显示了由这些工具生成的示例攻击场景的因果关系图。红色虚线箭头表示从中启动攻击调查的警报事件（α，可疑网络连接），红色虚线矩形区域表示已恢复的攻击子图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1709629615797.png&#34;
	width=&#34;1739&#34;
	height=&#34;810&#34;
	srcset=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1709629615797_hu7294ec030933aa612c8e36d38b404b7e_254270_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/atlas/ATLAS/1709629615797_hu7294ec030933aa612c8e36d38b404b7e_254270_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709629615797&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;515px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;但是即使应用了不同的图优化技术，这样的图仍然非常大，并且在实践中难以解释。这些工作很大程度上依赖于启发式或硬编码&lt;strong&gt;规则&lt;/strong&gt;，这些&lt;strong&gt;规则&lt;/strong&gt;的开发和维护非常耗时。&lt;strong&gt;领域知识专家&lt;/strong&gt;需要不断更新这些规则，以涵盖新开发的攻击。而ATLAS只需要更多的攻击训练数据来学习新的攻击模式。&lt;/p&gt;
&lt;p&gt;其他人提出了&lt;strong&gt;基于异常的方法&lt;/strong&gt;，该方法可以学习用户行为，并将任何偏离该行为的行为识别为异常。虽然基于异常的方法可以识别未知攻击，但随着用户行为随时间的变化，它们可能会出现许多&lt;strong&gt;误报&lt;/strong&gt;。为了解决这个问题，ATLAS旨在学习攻击模式和用户行为，以确定两者之间的异同。&lt;/p&gt;
&lt;p&gt;与ATLAS类似，基于学习的方法使用ML算法从日志中对攻击事件进行建模。虽然这些方法可以有效地减少日志条目的数量，但仍需要&lt;strong&gt;大量的手动工作&lt;/strong&gt;才能找到攻击事件的高级视图。为了解决这个问题，ATLAS调查旨在识别攻击&lt;strong&gt;关键实体&lt;/strong&gt;（节点），使其能够自动识别相关攻击事件的子集。&lt;/p&gt;
&lt;p&gt;APT攻击可以概括为从审计日志中获取的攻击阶段的&lt;strong&gt;时间序列，&lt;/strong&gt; 例如图1（b）中所示的步骤1-14，类似于自然语言中描述的攻击步骤。这些攻击步骤通常适合在特定上下文中作为表示攻击语义的唯一序列，这可以与审核日志中的正常活动区分开来。&lt;/p&gt;
&lt;p&gt;ATLAS 在推理时给定攻击症状节点（警报事件α包含的恶意 IP 地址），提取一组与症状节点关联的候选序列，并使用基于序列的模型来识别序列中的哪些节点参与了攻击。此后，它使用已识别的攻击节点来构建攻击故事，其中包括已识别的攻击节点的事件，从而使攻击调查更加简洁，更容易被调查人员解读。&lt;/p&gt;
&lt;p&gt;图 1 （c） 说明了 ATLAS 为激励示例恢复的攻击故事，其中包括示例攻击的完整关键攻击步骤。此过程大大减少了从大型因果图中进行攻击调查的手动工作，该图排除了对攻击没有影响的事件，并减少了调查大型因果图所需的时间。&lt;/p&gt;
&lt;h3 id=&#34;definition&#34;&gt;definition&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1709635161753.png&#34;
	width=&#34;847&#34;
	height=&#34;474&#34;
	srcset=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1709635161753_hua3897747c65368fc3b388dfc773651dc_99107_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/atlas/ATLAS/1709635161753_hua3897747c65368fc3b388dfc773651dc_99107_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1709635161753&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;428px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因果图G：因果图是从审计日志中提取的数据结构，通常用于来源跟踪，指示主题（例如，流程）和对象（例如，文件或连接）之间的因果关系。因果图由节点组成，节点代表主体和客体，边缘连接，边缘代表主体和客体之间的动作（例如，读取或连接）。我们在这里考虑一个有向循环因果图，它的边缘从主体指向对象。&lt;/p&gt;
&lt;p&gt;实体e：实体是从因果图中提取的唯一系统主体或对象，在其中它表示为节点。我们考虑的实体包括进程、文件和网络连接（即IP地址和域名）&lt;/p&gt;
&lt;p&gt;邻域图。给定因果图，如果两个节点u和v通过一条边连接，则称它们为邻居。节点 n 的邻域是由节点 n 和连接相邻节点与节点 n 的边组成的 G 子图。类似地，给定一组节点 {n1,n2,&amp;hellip;,nn}，我们提取一个统一的邻域图，其中包括将它们连接到相邻节点的所有节点和边。&lt;/p&gt;
&lt;p&gt;事件：事件ε是一个四元组（src、action、dest、t），源 （src） 和目标 （dest） 是与动作相关的两个实体。t 是显示事件发生时间的事件时间戳。给定一个实体 e，可以从 e 邻域图中提取其事件，其中包括与 e 的邻居相关的所有操作。例如，给定一个实体Firefox.exe和一个邻域图，其中包含从节点 Firefox.exe 到节点 Word.doc 的操作 open 和时间戳 t，那么 （Firefox.exe， open， Word.doc， t） 是 Firefox 进程在时间 t 打开 Word 文件的事件&lt;/p&gt;
&lt;p&gt;序列：给定一个实体 e，可以从因果图中提取序列 S。序列 S 按时间顺序包括实体 e 的邻域图的所有事件，使得 S{e} ：= {ε1， ε2， . . . ， εn}。同样，如果给定一组实体，我们可以从它们的统一邻域图中提取一个包含所有事件的序列。&lt;/p&gt;
&lt;p&gt;图 2 （a） 说明了具有六个实体 {eA， eB， . . . ， eF} 的因果图。图 2 （b） 显示了 eB 的邻域图，其中包括节点 B、相邻节点 {A， C} 及其连接边 {EAB， EBC}。类似地，实体集 {eB， eC} 的邻域图包括节点 {A， B， C， D， E} 和边 {EAB， EBC， ECD， ECE}，如图 2 （b） 所示。实体 eB 的事件为 εAB =&amp;lt; eA， a1， eB， t1 &amp;gt; 和 εBC =&amp;lt; eB， a2， eC， t2 &amp;gt;如图 2 （c） 所示。图 2 （d） 显示了实体集 {eB， eC} 的事件序列。&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;序列生成&#34;&gt;序列生成&lt;/h3&gt;
&lt;p&gt;首先从日志中提取出平台无关的因果图（图缩减）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;消除攻击节点无法访问到的所有节点和边缘【这缩的有点狠啊】&lt;/li&gt;
&lt;li&gt;从审计日志中构建具有非重复边的因果图，构建的因果图中不包含重复的边（边属性相同且首尾节点相同）&lt;/li&gt;
&lt;li&gt;如果某些节点和边引用相同类型的事件，则 ATLAS 会合并它们，合成的边的时间戳按原始边中最早的。
&lt;img src=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1716023699194.png&#34;
	width=&#34;798&#34;
	height=&#34;322&#34;
	srcset=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1716023699194_hud84dbde1e7c0ad0ed8876d9717f7ce09_69072_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/atlas/ATLAS/1716023699194_hud84dbde1e7c0ad0ed8876d9717f7ce09_69072_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716023699194&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;247&#34;
		data-flex-basis=&#34;594px&#34;
	
&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;构造序列：&lt;/p&gt;
&lt;p&gt;将因果图转换为标记为“攻击”或“非攻击”的序列，并将词形还原和选择性采样扩展到序列构造中，以有效地抽象攻击和非攻击模式，最后，使用词嵌入将序列转换为实数向量，并通过 LSTM 学习基于序列的模型。&lt;/p&gt;
&lt;p&gt;对于攻击节点：&lt;/p&gt;
&lt;p&gt;对于一个图中的攻击节点，构建攻击节点子集，子集节点数$\geq$2，因此对于n个攻击节点的集合，有$\sum_{i=2}^k C_n^i$个子集，这个方法在面对攻击节点数量很多的时候复杂度会爆炸，但是攻击者一般会为了隐藏而最小化攻击节点数量【这样每个攻击子图都得比较小才合适，怪不得图缩减缩得这么狠】。&lt;/p&gt;
&lt;p&gt;1、对于攻击实体中的每个实体，ATLAS提取其在攻击子图中的邻域图，即捕捉与攻击节点节点有因果关系的实体，2、从邻域图中获取按照时间戳排序的攻击事件。如果源节点或目标节点代表攻击实体，则事件被标记为攻击。3、最后，ATLAS 将提取的时间戳排序的攻击事件转换为序列，并在以下情况下将其标记为攻击：（a） 它仅包含攻击事件， （b）它包括实体子集的所有攻击事件。【还是在缩减，这样攻击序列理论上是最小的】&lt;/p&gt;
&lt;p&gt;对于非攻击节点：&lt;/p&gt;
&lt;p&gt;意义不是很大，主要学习攻击序列。同时，对于一个有k个攻击节点和k&amp;rsquo;个非攻击节点的因果图，会有$\sum^{k}_{i=1} C^i_k.k&amp;rsquo;$(攻击节点的子集（子集节点数不必$\geq$ 2)+非攻击节点的全集)。提取非攻击序列的方法和提取攻击序列的方法的1、2步骤一样，3、如果序列与任何提取的攻击序列不匹配，则 ATLAS 会将其标记为非攻击，否则，处理后的序列将被丢弃。&lt;/p&gt;
&lt;p&gt;下面这张图是例子&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1716026032629.png&#34;
	width=&#34;1518&#34;
	height=&#34;402&#34;
	srcset=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1716026032629_hu7fd7b8e71a58997854afb105ce0a1630_114728_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/atlas/ATLAS/1716026032629_hu7fd7b8e71a58997854afb105ce0a1630_114728_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716026032629&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;377&#34;
		data-flex-basis=&#34;906px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;序列还原&#34;&gt;序列还原&lt;/h3&gt;
&lt;p&gt;ATLAS使用&lt;strong&gt;词形还原&lt;/strong&gt;化将序列转换为表示序列模式的广义文本，以便进行语义解释。&lt;/p&gt;
&lt;p&gt;表1显示了 ATLAS 用于抽象序列中的实体和操作的四种不同的词汇类型以及每种类型中的词汇。词汇表总共包括 30 个单词，能够将单词的&lt;strong&gt;屈折形式&lt;/strong&gt;和&lt;strong&gt;派生形式&lt;/strong&gt;简化为共同的基本形式。词汇根据单词的细粒度语义分为四种类型：进程、文件、网络和动作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1716026181025.png&#34;
	width=&#34;772&#34;
	height=&#34;242&#34;
	srcset=&#34;https://blog2.pillar.fun/p/atlas/ATLAS/1716026181025_hu59fb008fe5d3dbeea8565b3f1ddc5a1c_33371_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/atlas/ATLAS/1716026181025_hu59fb008fe5d3dbeea8565b3f1ddc5a1c_33371_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1716026181025&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;319&#34;
		data-flex-basis=&#34;765px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;ATLAS解析每个序列，查找实体并将每个实体映射到相应的词汇表。词形还原过程后的序列被转换为“类似句子”的中间表示，其中包含广义序列模式的完整语义。&lt;/p&gt;
&lt;p&gt;我们注意到，在对序列进行词形还原后，可能会发生攻击和非攻击序列的意外重复。为了使用非重复序列训练模型，我们丢弃了所有与攻击序列重叠的非攻击序列，然后再将其传递到&lt;strong&gt;选择性序列采样&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;选择性序列采样&#34;&gt;选择性序列采样&lt;/h3&gt;
&lt;p&gt;构建的攻击和非攻击序列的数量可能不平衡。原因是日志条目中的攻击实体通常少于非攻击实体。例如，我们在通过分析审计日志进行评估时发现，攻击实体的平均数量为 61，而非攻击实体的平均数量约为 21K。为了平衡训练数据集，ATLAS首先对具有一定相似性阈值的非攻击序列进行&lt;strong&gt;欠采样&lt;/strong&gt;。然后，使用过采样机制随机&lt;strong&gt;变异&lt;/strong&gt;攻击序列，直到它们的总数达到相同数量。不能直接采取复制的过采样因为会造成过拟合。&lt;/p&gt;
&lt;h4 id=&#34;欠采样&#34;&gt;欠采样&lt;/h4&gt;
&lt;p&gt;通过Levenshtein Distance计算相似性，用80%作为阈值。复杂度为$O(n^2)$&lt;/p&gt;
&lt;h4 id=&#34;过采样&#34;&gt;过采样&lt;/h4&gt;
&lt;p&gt;对于攻击序列，ATLAS会随机将一种词汇类型变异为另一种相同类型的词汇。&lt;/p&gt;
&lt;h3 id=&#34;序列学习&#34;&gt;序列学习&lt;/h3&gt;
&lt;p&gt;通过&lt;strong&gt;词表示嵌入&lt;/strong&gt;将词形还原序列转换为表示序列模式的广义文本（30个词-&amp;gt;4type)【对……对吗？】，然后用了多种方法生成嵌入向量并在后文进行了比较。&lt;/p&gt;
&lt;p&gt;通过LSTM和CNN来进行序列学习&lt;/p&gt;
&lt;h3 id=&#34;攻击还原&#34;&gt;攻击还原&lt;/h3&gt;
&lt;p&gt;遍历所有节点，来判断是否是攻击节点，O(n)复杂度。&lt;/p&gt;
&lt;p&gt;为了了识别未知的攻击实体，ATLAS首先从因果图中获取所有未知实体，构建只包含一个未知实体的子集。然后，ATLAS 将攻击实体附加到每个子集;因此，每个子集包含所有已知的攻击症状实体和一个未知实体。ATLAS 使用这些子集从因果图中提取序列，LSTM 模型用于通过预测分数预测每个序列是攻击还是非攻击。此过程通过检查这两个实体的时间顺序事件是否形成模型先前学习的攻击模式来识别未知实体是否与攻击症状实体密切相关。&lt;/p&gt;
&lt;p&gt;ATLAS 攻击故事恢复的目标是从攻击调查阶段识别与已识别攻击实体关联的攻击事件。ATLAS提取已识别攻击实体的邻域图，并将所有包含的事件作为攻击事件获取。这些事件按其时间戳进一步排序，作为恢复的攻击故事。ATLAS 的有效性不受跨多个主机执行的攻击的影响，它只需要对来自单个主机的审计日志执行分析即可发现所有攻击实体&lt;/p&gt;
&lt;h2 id=&#34;工作亮点&#34;&gt;工作亮点&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;新的图缩减方法来构建因果图&lt;/li&gt;
&lt;li&gt;通过因果图生成序列用于学习（词型还原+词表示嵌入+过采样&amp;amp;欠采样)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;我的想法&#34;&gt;我的想法&lt;/h2&gt;
&lt;p&gt;图缩减方法是没见过的，按照文章的结果，这种图缩减方法也很值得参考，一顿猛缩减可以导致每个小子图的学习代价比较小（或者说因为序列长度有限制吧？)。把图学习转成序列识别的方法很有意思，这样直接就不用使用图学习了。但是感觉他这个识别是否攻击节点计算量也不小，O(n)是n*100s感觉也没啥用啊？抽时间复现下看看。&lt;/p&gt;
&lt;h2 id=&#34;补充知识&#34;&gt;补充知识&lt;/h2&gt;
&lt;h3 id=&#34;屈折形式inflectional-forms和派生形式derivational-forms&#34;&gt;屈折形式（Inflectional Forms）和派生形式（Derivational Forms）&lt;/h3&gt;
&lt;p&gt;屈折形式是指单词在不同语法环境下的形式变化，这种变化不改变单词的基本词义或词类。屈折形式主要用于表示语法信息，如时态、数、格、性、比较级等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;walk -&amp;gt; walked -&amp;gt; walking&lt;/li&gt;
&lt;li&gt;book -&amp;gt; books&lt;/li&gt;
&lt;li&gt;big -&amp;gt; bigger -&amp;gt; biggest&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;派生形式是通过添加前缀或后缀来改变单词的词性或基本词义，生成新的单词的过程。派生通常会改变单词的基本意义和/或词类。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;clear (清楚的) -&amp;gt; unclear (不清楚的)&lt;/li&gt;
&lt;li&gt;happy (形容词) -&amp;gt; happiness (名词)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;levenshtein-distance&#34;&gt;Levenshtein Distance&lt;/h3&gt;
&lt;p&gt;Levenshtein Distance，也称为编辑距离（Edit Distance），是一种衡量两个字符串之间差异的度量方法。它计算将一个字符串转换为另一个字符串所需的最少单字符编辑操作次数，这些操作包括插入（insertion）、删除（deletion）和替换（substitution）。Levenshtein Distance广泛应用于文本处理、拼写检查、语音识别、DNA序列比对等领域。它可以帮助确定两个字符串的相似度，或者在搜索时提供近似匹配。&lt;/p&gt;
&lt;p&gt;{{ &lt;code&gt;&amp;lt;heatmap&amp;gt;&lt;/code&gt; }}&lt;/p&gt;
</description>
        </item>
        <item>
        <title>KAIROS</title>
        <link>https://blog2.pillar.fun/p/kairos/</link>
        <pubDate>Sat, 02 Mar 2024 20:15:55 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/kairos/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/kairos/KAIROS/1715250405508.png" alt="Featured image of post KAIROS" /&gt;&lt;h2 id=&#34;导语&#34;&gt;导语&lt;/h2&gt;
&lt;p&gt;SP24&lt;/p&gt;
&lt;p&gt;doi is &lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.48550/arXiv.2308.05034&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;

&lt;span style=&#34;white-space: nowrap;&#34;&gt;&lt;svg width=&#34;.7em&#34;
    height=&#34;.7em&#34; viewBox=&#34;0 0 21 21&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
    &lt;path d=&#34;m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z&#34; fill=&#34;currentColor&#34; /&gt;
    &lt;path d=&#34;M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z&#34;
        fill=&#34;currentColor&#34;&gt;
&lt;/svg&gt;&lt;/span&gt;

    &lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;溯源图是描述系统执行历史的结构化审计日志。最近的研究探索了多种技术来分析自动主机入侵检测的溯源图，特别关注高级持续威胁（APT）。通过筛选他们的设计文档，我们确定了推动基于溯源图的入侵检测系统 (PIDS) 开发的四个常见维度：范围（能否检测到渗透到应用程序边界的现代攻击？）、攻击不可知性（能否检测到新颖的攻击而无需攻击特征的先验知识？）、及时性（能否在主机系统运行时有效地监控主机系统？）以及攻击重建能力（能否从大型溯源图中提取攻击活动，以便系统管理员能够轻松理解并快速响应系统入侵？） 。我们提出了 KAIROS，这是第一个同时满足所有四个维度的需求的 PIDS，而现有方法至少牺牲了一个维度，并且难以实现可比的检测性能。 KAIROS 利用基于新型图神经网络的编码器-解码器架构，该架构可以学习溯源图结构变化的时间演化，以量化每个系统事件的异常程度。然后，基于这些细粒度信息，KAIROS 重建攻击足迹，生成紧凑的摘要图，准确描述系统审核日志流上的恶意活动。使用最先进的基准数据集，我们证明 KAIROS 优于以前的方法。&lt;/p&gt;
&lt;h2 id=&#34;senario&#34;&gt;senario&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/kairos/KAIROS/1715250405508.png&#34;
	width=&#34;1756&#34;
	height=&#34;426&#34;
	srcset=&#34;https://blog2.pillar.fun/p/kairos/KAIROS/1715250405508_hu2f94ae9770155cf79c4d05b797271e0d_220753_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/kairos/KAIROS/1715250405508_hu2f94ae9770155cf79c4d05b797271e0d_220753_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1715250405508&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;412&#34;
		data-flex-basis=&#34;989px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;来自DARPA E3-THEIA的溯源图的摘要，描述了一种攻击活动，由KAIROS自动生成。矩形、椭圆形和菱形分别表示进程、文件和套接字。R=读取，W=写入，O=打开，S=发送，Rc=接收，C=克隆，E=执行。为了清晰起见，我们添加了颜色和虚线元素，以突出显示 KAIROS 生成的输出。KAIROS从原始来源图中提取实体节点和边，以重建攻击。根据攻击地面真相，虚线粉红色节点和边缘是 KAIROS 错过的与攻击相关的活动。蓝色节点和边是基本事实中未明确提及但包含在 KAIROS 中的活动。&lt;/p&gt;
&lt;h2 id=&#34;challenge&#34;&gt;challenge&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;不可知性：有没见过的攻击，需要模型具有泛化性&lt;/li&gt;
&lt;li&gt;攻击重建：很难通过溯源图得到并重现完整的攻击流程&lt;/li&gt;
&lt;li&gt;及时性：作为入侵检测系统（IDS)需要及时性，这就要求模型性能比较高&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;KAIROS是一个基于异常的入侵检测和攻击调查系统。它通过来源图中的因果依赖关系，利用最先进的&lt;strong&gt;深度图学习&lt;/strong&gt;和&lt;strong&gt;社区发现&lt;/strong&gt;， 可以做到&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在事先不了解任何特定攻击特征的情况下检测异常系统行为。&lt;/li&gt;
&lt;li&gt;根据信息流关联检测到的异常内核对象之间。 KAIROS 提供简洁且有意义的摘要图表，用于节省人力的人机交互取证分析。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下图描述了KAIROS的架构，由四个主要组件组成：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/kairos/KAIROS/1715251388567.png&#34;
	width=&#34;1732&#34;
	height=&#34;786&#34;
	srcset=&#34;https://blog2.pillar.fun/p/kairos/KAIROS/1715251388567_hua06a32d18c9f2c33ed34385a24464937_264162_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/kairos/KAIROS/1715251388567_hua06a32d18c9f2c33ed34385a24464937_264162_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1715251388567&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;528px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;图的构建和表示。
KAIROS以流式传输方式分析图表，按时间顺序摄取图表中出现的边。KAIROS 考虑&lt;strong&gt;三种类型的内核对象和九种类型的交互&lt;/strong&gt;（即系统事件）。 KAIROS 将每个事件转换为有向、带时间戳的边，其中源节点代表事件的主体，目标节点代表所作用的对象。使用&lt;strong&gt;基于节点属性的分层特征哈希技术&lt;/strong&gt;对节点的特征进行编码（但是感觉是先层次花，再用FeatureHasher，感觉也没有多次hash，而且他的形式化表达里映射正负的$\mathcal{H}$也没看到啊。)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/kairos/KAIROS/1715253831664.png&#34;
	width=&#34;930&#34;
	height=&#34;186&#34;
	srcset=&#34;https://blog2.pillar.fun/p/kairos/KAIROS/1715253831664_hu38a6058028f39050fdcd8295286cf12f_31842_480x0_resize_box_3.png 480w, https://blog2.pillar.fun/p/kairos/KAIROS/1715253831664_hu38a6058028f39050fdcd8295286cf12f_31842_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1715253831664&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;500&#34;
		data-flex-basis=&#34;1200px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;图学习。
当图中出现新的边缘，KAIROS 使用编码器-解码器架构来重建边缘。编码器将边缘周围的邻域结构和邻域中节点的状态作为输入。节点的状态是与每个节点关联的特征向量，描述节点邻域的变化历史。然后，解码器根据编码器输出的边缘嵌入重建边缘。原始边缘和重建边缘之间的差异称为重建误差。在训练阶段，KAIROS同时训练编码器和解码器，以最大限度地减少良性边缘的重建误差。在部署过程中，各个边缘的重建误差被用作异常检测和调查的基础。此外，KAIROS更新新边的源节点和目标节点的状态。在encoder-decoder结构中，使用基于节点属性的分层特征哈希技术对节点的特征进行编码，利用时间图网络（TGN）对边进行嵌入，通过新的边（事件）动态更新节点特征，可以有效地将时序特征保存在节点特征中，同时边的嵌入也基于邻域内节点的特征向量。这样可以有效地保存事件的时序信息（感觉这个想法还挺常见的，就是要获取时间相关的特征)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;异常检测。
KAIROS 构建时间窗口队列来检测部署期间的异常情况。为此，KAIROS根据边的重建误差在每个时间窗口中识别一组可疑节点。具有重叠可疑节点的两个时间窗口被排在一起。当新的时间窗口添加到队列时，KAIROS 也会根据重建错误更新队列的异常分数。如果分数超过阈值，KAIROS会认为队列异常并触发警报。因此，KAIROS以时间窗口的间隔定期执行异常检测。在图中，KAIROS 检测到由时间窗口 1、2 和 4 组成的异常队列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;异常调查。
为了帮助系统管理员推理警报，KAIROS 自动从异常时间窗口队列生成紧凑的攻击摘要图。这涉及识别具有高重建误差的边缘社区以提高易读性。图形简化是必要的，因为与图像和文本不同，图形即使是人类专家也很难可视化和解释。在图中，系统管理员只需要了解 KAIROS 中的一个小的汇总图，而不是跟踪触发警报的异常时间窗口队列中的一个大得多的图。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;补充知识&#34;&gt;补充知识&lt;/h2&gt;
&lt;h3 id=&#34;分层特征哈希&#34;&gt;分层特征哈希&lt;/h3&gt;
&lt;p&gt;在传统的哈希技术中，常见的方法是将特征空间的维度映射到固定大小的哈希表中。然而，当特征空间非常大时，这种方法可能会导致哈希冲突，进而影响模型的性能。&lt;/p&gt;
&lt;p&gt;分层特征哈希通过将特征空间分解为多个层级来解决这个问题。每个层级都具有不同的哈希函数，用于将特征映射到不同的桶中。通常，初始层级的哈希函数将原始特征映射到较小的中间空间，然后通过逐渐应用更多的哈希函数，将特征映射到最终的哈希桶中。&lt;/p&gt;
&lt;p&gt;分层特征哈希将高维输入向量投影到低维特征空间，同时保留原始输入之间的分层相似性。分层特征哈希在处理大规模高维数据时具有很好的效果，例如在文本分类、推荐系统和图像检索等任务中经常被使用。&lt;/p&gt;
&lt;h3 id=&#34;tgn&#34;&gt;TGN&lt;/h3&gt;
&lt;p&gt;TGN，全称Temporal Graph Networks，是一种针对时间图的网络嵌入方法。在许多实际应用中，包括社交网络、交通网络等，图的拓扑结构和节点的交互是随着时间发展而变化的，这种图被称为时间图。TGN的目标是为时间图中的节点和边生成嵌入向量，以便在这些向量上进行各种预测任务，如链接预测、节点分类等。&lt;/p&gt;
&lt;p&gt;TGN对边的嵌入主要涉及以下步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;节点嵌入更新&lt;/strong&gt;：当新的边事件（如用户间的交互）发生时，TGN会根据新的边事件信息更新对应节点的嵌入。具体来说，TGN采用了记忆化的节点嵌入更新机制，即根据节点的历史嵌入和新的边事件信息共同决定节点的新的嵌入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;边嵌入生成&lt;/strong&gt;：在节点嵌入更新后，TGN会生成新的边的嵌入。具体来说，边的嵌入是由连接该边的两个节点的嵌入和边事件的时间信息共同决定的。例如，一种简单的方法是将两个节点的嵌入和边事件的时间信息拼接起来，然后通过一个全连接网络生成边的嵌入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时间编码器&lt;/strong&gt;：为了捕获边事件的时间信息，TGN引入了一个时间编码器，它可以将边事件的时间戳编码为一个连续的向量。在生成边的嵌入时，会将这个时间向量和节点的嵌入一起考虑。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过以上步骤，TGN能够处理时间图中的边事件，并为每个边事件生成一个嵌入向量，这个嵌入向量同时考虑了边事件的拓扑结构信息和时间信息。这使得TGN能够适应图的动态变化，并进行各种预测任务。&lt;/p&gt;
&lt;h2 id=&#34;我的评价&#34;&gt;我的评价&lt;/h2&gt;
&lt;p&gt;贡献主要有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了PIDS的四个重要指标&lt;/li&gt;
&lt;li&gt;提出了新的三对象九交互的建图模式（但是没怎么证明有效性)&lt;/li&gt;
&lt;li&gt;提出了时间窗口的社区发现的攻击重建方法，比较紧凑，会更方便溯源人员重建攻击。&lt;/li&gt;
&lt;li&gt;实验做的很丰富，包括对不同数据集的分析，以及在相应的数据集上和其他的现有方法的对比做的很好。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我的评价是，这篇文章最厉害的是实验部分，他把DARPA的数据集都做了检测，其他的感觉中规中矩也就，但是他能发SP你有意见吗？&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
