<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>课业 on π1l4r_のblog</title>
        <link>https://blog2.pillar.fun/categories/%E8%AF%BE%E4%B8%9A/</link>
        <description>Recent content in 课业 on π1l4r_のblog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>pill4r</copyright>
        <lastBuildDate>Wed, 27 Dec 2023 19:07:10 +0800</lastBuildDate><atom:link href="https://blog2.pillar.fun/categories/%E8%AF%BE%E4%B8%9A/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>网络认证技术笔记</title>
        <link>https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 27 Dec 2023 19:07:10 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/img/placeholder.jpeg" alt="Featured image of post 网络认证技术笔记" /&gt;&lt;h1 id=&#34;导语&#34;&gt;导语
&lt;/h1&gt;&lt;p&gt;说是开课以来从未有过挂科选手，但是想得不错的分数还是要努努力，进自己脑子的知识才是最好的知识&lt;/p&gt;
&lt;h1 id=&#34;笔记&#34;&gt;笔记
&lt;/h1&gt;&lt;p&gt;网络认证技术 ≈ 密码学+计算机网络&lt;/p&gt;
&lt;p&gt;网络认证：在信息系统/网络环境中，实现身份的确认。目标：在不可信的网络环境中确认主体是谁，有什么属性、权限、能力&lt;/p&gt;
&lt;p&gt;身份确认的主体：人、设备、软件服务……&lt;/p&gt;
&lt;p&gt;PKI：公钥基础设施&lt;/p&gt;
&lt;p&gt;CA：认证中心，生成数字证书&lt;/p&gt;
&lt;p&gt;CA是PKI的核心组成成分，但是在很多地方把CA和PKI混用了。&lt;/p&gt;
&lt;h1 id=&#34;考试用&#34;&gt;考试用
&lt;/h1&gt;&lt;p&gt;两个半小时&lt;/p&gt;
&lt;p&gt;题型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 2分&lt;/li&gt;
&lt;li&gt;简答 5-8分
&lt;ol&gt;
&lt;li&gt;建议不要空这&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;关键是原理之类的要记住的&lt;/p&gt;
&lt;h2 id=&#34;01-导言意义不大&#34;&gt;01 导言（意义不大）
&lt;/h2&gt;&lt;p&gt;相关概念&lt;/p&gt;
&lt;h2 id=&#34;02-密码学基础会涉及题目需要复习复习&#34;&gt;02 密码学基础（会涉及题目，需要复习复习）
&lt;/h2&gt;&lt;p&gt;对称：加解密&lt;/p&gt;
&lt;p&gt;非对称：签名&lt;/p&gt;
&lt;p&gt;光看密钥长度不能知道强度，RSA1024bits=ECC160bits。短密钥可以达到高强度&lt;/p&gt;
&lt;p&gt;哈希：验证&lt;/p&gt;
&lt;p&gt;消息鉴别码：MAC=C(K,M)，K为密钥M为消息，把密钥跟着一块哈希了&lt;/p&gt;
&lt;p&gt;可鉴别加密CCM、GCM、AEAD（简单了解）&lt;/p&gt;
&lt;p&gt;国外的密码基本原理不细说了&lt;/p&gt;
&lt;p&gt;国产的了解一下&lt;/p&gt;
&lt;p&gt;SM2 非对称 ECC &amp;mdash; 椭圆曲线 知道基于椭圆曲线域上的离散对数困难问题。 替换RSA&lt;/p&gt;
&lt;p&gt;SM3 哈希 256bit和sha256差不多 分组长度512bit，摘要值长度256bit&lt;/p&gt;
&lt;p&gt;SM4 分组工作模式&lt;/p&gt;
&lt;p&gt;ECB：对每个块独立加密：明文同样的块会加密成同样的密文&lt;/p&gt;
&lt;p&gt;CBC：明文先与上一个密文异或在加密，需要初始化向量&lt;/p&gt;
&lt;p&gt;OFB：将块密码转为流密码，生成密钥流的块&lt;/p&gt;
&lt;p&gt;CTR（ICM、SIC）：将块密码变为流密码，通过递增加密计数器产生密钥流&lt;/p&gt;
&lt;p&gt;ZUC 流密码&lt;/p&gt;
&lt;p&gt;128位的初始密钥key和128位的初始向量iv来作为输入。每个时钟周期能生成32bit&lt;/p&gt;
&lt;p&gt;共享密钥问题-&amp;gt;为什么要有非对称的原因-&amp;gt;数字签名&lt;/p&gt;
&lt;h2 id=&#34;03-口令鉴别&#34;&gt;03 口令鉴别
&lt;/h2&gt;&lt;p&gt;client 用复杂口令，不要告诉别人，次数限制&lt;/p&gt;
&lt;p&gt;传输 使用已被验证的安全信道&lt;/p&gt;
&lt;p&gt;server 存储，验证&lt;/p&gt;
&lt;h2 id=&#34;04-基于密码技术的鉴别&#34;&gt;04 基于密码技术的鉴别
&lt;/h2&gt;&lt;p&gt;两大类：&lt;/p&gt;
&lt;p&gt;对称： 有没有密钥&lt;/p&gt;
&lt;p&gt;提一个协议框架，让你看有没有什么错，一些参数有什么用【用什么方式可以抵抗什么攻击】&lt;/p&gt;
&lt;p&gt;replay attack（重放攻击）：通过加一个nonce抵抗&lt;/p&gt;
&lt;p&gt;oracle session attack（就是攻击者使另一方帮自己来计算）：让u和v不同。比如u为加密，v为解密，被挑战方只能加密，就不能被当成解密服务器了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705143946346.png&#34;
	width=&#34;498&#34;
	height=&#34;316&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705143946346_hu2095797235635835411.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705143946346_hu12065670770239322718.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705143946346&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Parallel Session attack：p(), q()与方向有关。从而攻击者不能利用服务器的计算。比如发起者会加一个xor，被挑战者会加一个左移&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144131279.png&#34;
	width=&#34;466&#34;
	height=&#34;301&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144131279_hu7997728563994467757.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144131279_hu3773060652756052723.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144131279&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;371px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;offset attack：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144243068.png&#34;
	width=&#34;1240&#34;
	height=&#34;995&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144243068_hu5939699765668807220.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144243068_hu17260106512263764369.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144243068&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;299px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;把返回的东西改为E(f()#E(g))&lt;/p&gt;
&lt;p&gt;可信第三方，kobras&lt;/p&gt;
&lt;p&gt;非对称：数字签名和验证&lt;/p&gt;
&lt;p&gt;单向（带一个时间戳之类的约定好的东西）、双向（A发给B后B还要发给A）&lt;/p&gt;
&lt;p&gt;PPT标红好好看看&lt;/p&gt;
&lt;h2 id=&#34;0506-pki技术&#34;&gt;05+06 PKI技术
&lt;/h2&gt;&lt;p&gt;CA：认证机构，权威第三方，公钥（证书）可信发布[根CA、子CA]&lt;/p&gt;
&lt;p&gt;RA：注册机构，审查信息，防止CA职能太多导致一个出问题导出都出问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144615935.png&#34;
	width=&#34;586&#34;
	height=&#34;391&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144615935_hu4210782284210374799.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705144615935_hu13358447639732878381.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144615935&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;repository（存数据的吧）&lt;/p&gt;
&lt;p&gt;CRL（Certificate Revocation List，证书撤销列表）&lt;/p&gt;
&lt;p&gt;Online Certificate Status Protocol（OCSP）一种通信协议，专门用于检查证书是否已经被撤销 相应的服务器称为OCSP Server-&amp;gt;（证书有三种状态）Good、Revoked、Unknown ：未撤销、已经撤销、未知&lt;/p&gt;
&lt;p&gt;ASN.1-基本数据类型-DER编码-sequence-implicit/explicit tag  稍微看一下&lt;/p&gt;
&lt;h2 id=&#34;07-证书拓展&#34;&gt;07 证书拓展
&lt;/h2&gt;&lt;p&gt;证书基本域&lt;/p&gt;
&lt;p&gt;证书扩展域 X.509版本3 18种，了解功能即可&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1704713547641.png&#34;
	width=&#34;1121&#34;
	height=&#34;619&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1704713547641_hu3332231256943693864.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1704713547641_hu11437562375678032862.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704713547641&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;434px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;拓展有关键和非关键，如果关键出了错（识别不出来），直接认定证书非法。非关键出错则忽略拓展&lt;/p&gt;
&lt;p&gt;Basic Constraints：区分是否是CA证书（能否签发其他证书）以及路径的深度（说明CA可以有多少层次的下级）&lt;/p&gt;
&lt;p&gt;Authority Key Identifier：证书链中可能有多个公钥，这个确定哪个是用来验证证书的颁发者（CA）的公钥&lt;/p&gt;
&lt;p&gt;Subject Key Identifier：证书链中可能有多个公钥，确定哪个是证书自己的公钥&lt;/p&gt;
&lt;p&gt;Key Usage：密钥的用途。7种+2种辅助用途&lt;/p&gt;
&lt;p&gt;Private Key Usage Period：给出证书有效的开始到结束的时间&lt;/p&gt;
&lt;p&gt;Issuer Alternative Name：放置签发者（CA）的消息（DN存放CA信息，子CN没法用DN，就用这来放）&lt;/p&gt;
&lt;p&gt;Subject Alternative Name：放置证书拥有者的消息&lt;/p&gt;
&lt;p&gt;Subject Directory Attributes：可加入任何与Subject有关的信息，例如，民族、生日等&lt;/p&gt;
&lt;p&gt;Name Constraints：限制下级CA所能够签发证书的订户的名字空间（只在下级CA中有用）&lt;/p&gt;
&lt;p&gt;Certificate Policies（CP）：区分不同证书的安全等级&lt;/p&gt;
&lt;p&gt;Inhibit Any-Policy：（CP的Any-Policy指对于该CA所签发的订户证书的CP没有限制），值是整数N，表示：在证书路径中，本证书之下的N个证书可带有Any-Policy的证书&lt;/p&gt;
&lt;p&gt;Policy Mappings：说明了不同CA域之间的CP等级的相互映射关系&lt;/p&gt;
&lt;p&gt;Policy Constraints：对于证书认证路径的策略映射过程中，有关CP的处理，进行限制。N：在N个证书后，不允许再进行策略映射；M：在M个证书后，就必须要有认识的、明确的CP&lt;/p&gt;
&lt;p&gt;Extended Key Usage：证书/密钥可用的用途（拓展）&lt;/p&gt;
&lt;p&gt;CRL Distribution Points：和应用系统约定在哪儿获取CRL&lt;/p&gt;
&lt;p&gt;Freshest CRL：增量CRL情况下，获取最新的增量CRL的地址&lt;/p&gt;
&lt;p&gt;Authority Information Access：如何在Internet上面，访问一些CA的信息（目前只有 1、上级CA的情况 2、OCSP服务器的情况两个信息）&lt;/p&gt;
&lt;p&gt;Subject Information Access： l如何在Internet上面，访问一些用户的信息 （目前只有 1、资料库的地址（针对CA）2、TSA服务地址（针对TSA服务器））&lt;/p&gt;
&lt;h2 id=&#34;08-pki信任体系&#34;&gt;08 PKI信任体系
&lt;/h2&gt;&lt;p&gt;信任模型&lt;/p&gt;
&lt;p&gt;单根CA&lt;/p&gt;
&lt;p&gt;多根CA 根之间要互相通信-&lt;strong&gt;CTL&lt;/strong&gt;（用户自主+权威发布）&lt;em&gt;&lt;strong&gt;沟通方式、原理、优缺点，应用&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CTL（信任锚）由权威机构统一地发布1个可信的信任锚列表（Certificate Trust List）包括多个根CA证书文件的HASH结果和受信任CA对其签名&lt;/p&gt;
&lt;p&gt;【信任锚里有根CA证书的hash、其他CA证书、CRL、信任策略和规则等。然后由一个我信任的CA对CTL签名，一般不用CTL里信任的CA来签名。】&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705156671337.png&#34;
	width=&#34;1011&#34;
	height=&#34;569&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705156671337_hu13648435200720500449.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705156671337_hu16012163551749894447.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705156671337&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;方式：1、不同PKI域用同一个CTL 2、加一个ACA的信任锚说明哪些根CA是可以信任的&lt;/p&gt;
&lt;p&gt;交叉认证-网状mesh-&amp;gt;桥CA&lt;/p&gt;
&lt;p&gt;相当于将对方CA认作是我的子CA。&lt;/p&gt;
&lt;p&gt;mesh-&amp;gt;信任链变成信任网&lt;/p&gt;
&lt;p&gt;桥CA-&amp;gt;不同域之间的证书传递&lt;/p&gt;
&lt;h2 id=&#34;09-证书撤销&#34;&gt;09 证书撤销
&lt;/h2&gt;&lt;p&gt;验证签名-验证有效期-验证撤销状态&lt;/p&gt;
&lt;p&gt;撤销状态 CRL、OCSP、CRT 原理&lt;/p&gt;
&lt;p&gt;CA/CRL Issuer定期地签发CRL CRL，certificate revocation list&lt;/p&gt;
&lt;p&gt;完全CRL－Complete CRL：所有CRL信息一次发布&lt;/p&gt;
&lt;p&gt;增量CRL－Delta CRL：发布新增的CRL信息发布&lt;/p&gt;
&lt;p&gt;直接CRL－Direct CRL：证书签发者签发CRL&lt;/p&gt;
&lt;p&gt;间接CRL－Indirect CRL：使用CRL issuer签发CRL&lt;/p&gt;
&lt;p&gt;OCSP在线证书状态协议 Online Certificate Status Protocol 在线服务器&lt;/p&gt;
&lt;p&gt;CRT：证书撤销树，对于各证书序列号进行一定的结构化，形成了HASH链&lt;/p&gt;
&lt;p&gt;使用了merkle hash tree【区块链信任算法】&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705164608101.png&#34;
	width=&#34;531&#34;
	height=&#34;419&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705164608101_hu7216011066195967886.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705164608101_hu11335282673389705324.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705164608101&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;304px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;拿加粗的子哈希算哈希，就可以推出根hash，验证起来需要更少的那啥&lt;/p&gt;
&lt;h2 id=&#34;10-tls&#34;&gt;10 TLS
&lt;/h2&gt;&lt;p&gt;handshake怎么shake的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705163142817.png&#34;
	width=&#34;1215&#34;
	height=&#34;589&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705163142817_hu9741049117357267131.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705163142817_hu5660297110154725848.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705163142817&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;495px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;增加一个server对client的鉴别&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208493971.png&#34;
	width=&#34;1195&#34;
	height=&#34;709&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208493971_hu6180500869490691705.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208493971_hu11560087961794679250.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705208493971&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果server证书只能签名不能加密，则要生成一个临时公钥，签名后发给client【ServerKeyExchange】&lt;/p&gt;
&lt;p&gt;两张图里的消息有什么含义 1.3和1.2的区别&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208812117.png&#34;
	width=&#34;975&#34;
	height=&#34;391&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208812117_hu7319128881007979932.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705208812117_hu17421658603890731294.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705208812117&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;598px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;直接在client hello中发了选择算法的key（用server 公钥加密）&lt;/p&gt;
&lt;h2 id=&#34;105wifi认证&#34;&gt;10.5wifi认证
&lt;/h2&gt;&lt;p&gt;WPA-PSK共享口令 （路由器上做）&lt;/p&gt;
&lt;p&gt;WPA-802.1X 基于账号的身份鉴别 （身份鉴别server）&lt;/p&gt;
&lt;p&gt;客户端或网页 （微信、短信）&lt;/p&gt;
&lt;h2 id=&#34;11-不考&#34;&gt;11 不考
&lt;/h2&gt;&lt;h2 id=&#34;12-pki安全增强&#34;&gt;12 PKI安全增强
&lt;/h2&gt;&lt;h3 id=&#34;入侵容忍-解决了什么问题怎么解决的-原理&#34;&gt;入侵容忍 解决了什么问题？怎么解决的 原理
&lt;/h3&gt;&lt;p&gt;解决了在入侵场景下的高可用。黑客侵入了其中一个PKI节点无法获利，同时PKI系统任然保持可用性&lt;/p&gt;
&lt;p&gt;【门限密码学：把密钥分成L份，当有其中f+1份时可以解密，否则解密不了】&lt;/p&gt;
&lt;p&gt;eg. Shamir&amp;rsquo;s Secret Sharing 基于拉格朗日插值法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065190379.png&#34;
	width=&#34;1071&#34;
	height=&#34;392&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065190379_hu7135214991858497998.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065190379_hu7155085174980983458.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065190379&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;eg2. ITTC 基于离散对数的子密钥分配&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065815902.png&#34;
	width=&#34;1315&#34;
	height=&#34;752&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065815902_hu11287122052346273300.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065815902_hu5777546212009236755.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065815902&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065831688.png&#34;
	width=&#34;701&#34;
	height=&#34;180&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065831688_hu15423727207887363475.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705065831688_hu16427163421748195862.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065831688&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;389&#34;
		data-flex-basis=&#34;934px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;server上用密码算，CA来整合&lt;/p&gt;
&lt;p&gt;也就是说黑客就算攻入了一个节点，他仍然无法获取PKI用来签名证书的私钥，同时其他节点还能继续工作。&lt;/p&gt;
&lt;h3 id=&#34;信任增强-解决方式原理&#34;&gt;信任增强 解决方式原理
&lt;/h3&gt;&lt;p&gt;信任机制基本假设：1、CA行为不会出错，证书中的信息不会出错【只有可能是错误操作导致的签发给错误的人】 2、无限制权利&lt;/p&gt;
&lt;p&gt;三个思路：&lt;/p&gt;
&lt;p&gt;1、 浏览器端实施检测：&lt;/p&gt;
&lt;p&gt;（1）浏览器维护证书信息&lt;/p&gt;
&lt;p&gt;（2）多个会话之间互相比较&lt;/p&gt;
&lt;p&gt;2、限制CA权利&lt;/p&gt;
&lt;p&gt;（1）假定server只会向同一个国家的CA申请证书&lt;/p&gt;
&lt;p&gt;（2）限定CA能签发的顶级域名范围&lt;/p&gt;
&lt;p&gt;（3）域名拥有者可以控制哪个CA给他签发证书&lt;/p&gt;
&lt;p&gt;（4）server再次确认机制：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705067041133.png&#34;
	width=&#34;780&#34;
	height=&#34;480&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705067041133_hu10210819328900081578.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705067041133_hu2460349736240520960.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705067041133&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;server在多一个sovereign Key的公私钥对挂在timeline上，浏览器看到timeline上有sovereign Key，会要求server再次拿sovereign Key私钥签名，黑客控制了CA，却无法获取server的sovereign Key私钥，因此仍然无法伪造身份&lt;/p&gt;
&lt;p&gt;3、证书透明化：&lt;/p&gt;
&lt;p&gt;假定CA也会出错，审计CA&lt;/p&gt;
&lt;h2 id=&#34;13-证书透明化&#34;&gt;13 证书透明化
&lt;/h2&gt;&lt;p&gt;虚假证书：证书可以被严格验证通过，但是证书对应的私钥并不被证书主体拥有，而是被其他人拥有（CA被人黑了，一顿乱发）&lt;/p&gt;
&lt;p&gt;透明化增加哪些步骤SCT相关特点弄清楚一点&lt;/p&gt;
&lt;p&gt;增加&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;公开日志服务器（Public Log Server）：保存和维护记录证书的公开日志（Public Log）&lt;/p&gt;
&lt;p&gt;收到证书并验证通过后，公开日志服务器会向提交者返回一个凭据（SCT）Signed Certificate Timestamp。（有可能多个公开日志服务器，就会返回多个SCT）用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705068113028.png&#34;
	width=&#34;1456&#34;
	height=&#34;630&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705068113028_hu9424455462047417422.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705068113028_hu5724579507490634671.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705068113028&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;231&#34;
		data-flex-basis=&#34;554px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）

怎么获得SCT呢？

1.从X.509证书扩展项获得SCT

2.从连接建立时TLS扩展项获得SCT -&amp;gt;TLS客户端要支持

3.从OCSP stapling的扩展项获得SCT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面不重要&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;监视员（Monitor）：周期性的访问公开日志服务器，寻找和发现可疑的证书&lt;/li&gt;
&lt;li&gt;审计员（Auditor）：审计公开日志的行为&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;14-隐式证书&#34;&gt;14 隐式证书
&lt;/h2&gt;&lt;p&gt;传统和隐式的结构和使用的区别&lt;/p&gt;
&lt;p&gt;在带宽、计算能力、存储资源有限制的环境下，隐式证书是传统X.509证书的一种高效替代&lt;/p&gt;
&lt;p&gt;X.509证书基本内容：订户身份信息+公钥数据+CA数字签名&lt;/p&gt;
&lt;p&gt;隐式证书基本内容：中间公钥数据$P_U$  + 订户身份标识。 最终公钥 P=$P_{CA}$+$P_U$以及身份信息也有关  $P_{CA}$：CA证书公钥&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705069930310.png&#34;
	width=&#34;1030&#34;
	height=&#34;827&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705069930310_hu10721782855505182717.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705069930310_hu11531986103520418197.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705069930310&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;298px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070190100.png&#34;
	width=&#34;1092&#34;
	height=&#34;391&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070190100_hu6511117380717266410.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070190100_hu11774285079405670562.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705070190100&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;279&#34;
		data-flex-basis=&#34;670px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070203597.png&#34;
	width=&#34;1049&#34;
	height=&#34;351&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070203597_hu2315117063556090242.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705070203597_hu468023201583066472.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705070203597&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;298&#34;
		data-flex-basis=&#34;717px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;X.509： 需要对订户证书进行CA签名的验证&lt;/p&gt;
&lt;p&gt;隐式证书：需要重构出订户公钥，在对消息的验签时同时完成对证书本身的验证&lt;/p&gt;
&lt;p&gt;隐式证书中，没有对CA数字签名的验证，取而代之的是，重构公钥的计算，后者的计算量较小。&lt;/p&gt;
&lt;p&gt;假名证书不考&lt;/p&gt;
&lt;h2 id=&#34;15-kerberos&#34;&gt;15 kerberos
&lt;/h2&gt;&lt;p&gt;可信第三方TTP，基于对称密码，也支持在某些过程使用非对称&lt;/p&gt;
&lt;p&gt;获得一个TGT，用TGT和要访问的目，请求问kerberos服务器，来获取访问目标的票据（不是TGT，TGT只是告诉kerberos我已经被验证过了）&lt;/p&gt;
&lt;p&gt;kerberos票据流程&lt;/p&gt;
&lt;p&gt;长期密钥（主密钥）Long-term Key/Master Key： 长期保持不变的密钥。被长期密钥（主密钥）加密的数据尽量不在网络上传输。（防止暴力破解、分析）&lt;/p&gt;
&lt;p&gt;短期密钥（会话密钥）Short-term Key/Session Key： 加密需要进行网络传输的数据。只在一段时间内有效，即使被加密的数据包被黑客截获并破解成功后，这个Key早就已经过期了。&lt;/p&gt;
&lt;p&gt;KDC（Key Distribution Center）：kerberos server作为可信第三方，维护所有帐户（client、server）的注册信息、用户名、口令、用户主密钥、服务器主密钥&lt;/p&gt;
&lt;p&gt;Server 与Client之间基于共享秘密短期密钥key实现身份鉴别&lt;/p&gt;
&lt;p&gt;KDC仅仅是允许进入应用系统，至于有什么权限、由应用系统自主决定&lt;/p&gt;
&lt;p&gt;获取TGT：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080938952.png&#34;
	width=&#34;1269&#34;
	height=&#34;425&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080938952_hu7572133327739404646.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080938952_hu13967206367631477746.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705080938952&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;298&#34;
		data-flex-basis=&#34;716px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;client发请求，KDC用client的master key加密一个会话密钥$S_{KDC-Client}$，用KDC的master key加密TGT，TGT里包含会话密钥和client信息（让client 鉴别KDC是KDC而非被伪造）&lt;/p&gt;
&lt;p&gt;获取ST：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705139628762.png&#34;
	width=&#34;790&#34;
	height=&#34;324&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705139628762_hu174052309057831376.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705139628762_hu3049550556592316702.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705139628762&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;585px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;这个图有问题，KDC还给client的不是用clinet的master key，而是用session key。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当client要访问server的时候，给KDC自己的TGT和要访问的server。&lt;/p&gt;
&lt;p&gt;KDC根据TGT来对client进行认证，生成$S_{Server-Client}$和ST(session ticket)&lt;/p&gt;
&lt;p&gt;$S_{Server-Client}$：用client的主密钥加密一个会话密钥，&lt;/p&gt;
&lt;p&gt;ST：用server的主密钥加密，ST包含会话密钥和client的信息。&lt;/p&gt;
&lt;p&gt;将这两个被加密的Copy一并发送给Client&lt;/p&gt;
&lt;p&gt;client得到会话密钥后，用session key解密，创建Authenticator（Client Info + Timestamp）并用会话密钥加密&lt;/p&gt;
&lt;p&gt;client将ST和Authenticator访问server，server用自己的主密钥解密ST得到会话密钥，在用会话密钥解密Authenticator，比较Authenticator里的client info和ST里的client info来确定client就是client&lt;/p&gt;
&lt;p&gt;那如果TGT没过期，session key过期了呢？可以用TGT再申请一个，因为TGT用KDC的master key加密，KDC可以得到旧的session key和client info，进而再发一个session key。由于session key是TGT的一部分，这其实也就相当于重新申请了TGT&lt;/p&gt;
&lt;p&gt;client鉴别server（双向鉴别）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080767000.png&#34;
	width=&#34;1165&#34;
	height=&#34;397&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080767000_hu1825950727476635132.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/1705080767000_hu7340608113318124904.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705080767000&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;293&#34;
		data-flex-basis=&#34;704px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在Authenticator里在加一个flag要求server自证。&lt;/p&gt;
&lt;p&gt;server看到后，用ST里得到的会话密钥解密Authenticator，把里面的timestamp用会话密钥加密发给client&lt;/p&gt;
&lt;h2 id=&#34;16-oauthoidc&#34;&gt;16 OAuth&amp;amp;OIDC
&lt;/h2&gt;&lt;p&gt;单点登录(Single Sign on)在某个地方认证了之后，在整个域里都不用再认证了。&lt;/p&gt;
&lt;p&gt;SSO 口令记录器-&amp;gt;保存在edge/chrome&lt;/p&gt;
&lt;p&gt;OAuth 协议流程图，理解认证的流程，有那几个角色，分别做了什么&lt;/p&gt;
&lt;p&gt;OIDC 协议流程图，理解认证的流程&lt;/p&gt;
&lt;h2 id=&#34;17-fido&#34;&gt;17 FIDO
&lt;/h2&gt;&lt;p&gt;在服务器端将用户与移动终端的可信环境进行身份绑定
将用户与服务器之间的直接鉴别转变为两段式鉴别
1 移动终端鉴别用户主要是靠生物特征
2 服务器端鉴别移动终端主要是靠数字签名&lt;/p&gt;
</description>
        </item>
        <item>
        <title>自然语言处理笔记</title>
        <link>https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 27 Dec 2023 19:07:00 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp.png" alt="Featured image of post 自然语言处理笔记" /&gt;&lt;h1 id=&#34;导语&#34;&gt;导语
&lt;/h1&gt;&lt;p&gt;期末全是开放问题，因此弄清楚各种模型的优劣非常有必要。&lt;/p&gt;
&lt;h1 id=&#34;笔记&#34;&gt;笔记
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp.png&#34;
	width=&#34;1780&#34;
	height=&#34;1273&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp_hu10569731052445655537.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp_hu17970110324252980944.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;nlp.png&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;335px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;model
&lt;/h2&gt;&lt;p&gt;常见的模型有DNN、CNN、RNN、GNN、LSTM、&lt;/p&gt;
&lt;h2 id=&#34;task&#34;&gt;task
&lt;/h2&gt;&lt;p&gt;NLP的经典问题有&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704274435391.png&#34;
	width=&#34;1080&#34;
	height=&#34;885&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704274435391_hu9554095579011663006.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704274435391_hu1260430606365475350.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704274435391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;122&#34;
		data-flex-basis=&#34;292px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在课程中，我们主要学习了&lt;/p&gt;
&lt;h4 id=&#34;属性抽取ae&#34;&gt;属性抽取（AE）
&lt;/h4&gt;&lt;p&gt;opinion target和aspect的区别：opinion target是被评价对象，aspect是对象的属性&lt;/p&gt;
&lt;p&gt;eg&amp;quot;这个手机的摄像头很出色，但电池寿命较短。&amp;ldquo;手机是opinion target，而摄像头和电池寿命是手机的两个aspect。&lt;/p&gt;
&lt;p&gt;目标：抽取对象。eg：华为技术遥遥领先！-&amp;gt; 抽取出“华为”&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/51189078&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Aspect Term Extraction 论文阅读（一） - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704948448294.png&#34;
	width=&#34;1843&#34;
	height=&#34;1053&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704948448294_hu7252898151843594783.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704948448294_hu12337758229638144476.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704948448294&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;THA是用了attention机制的LSTM，用于获得上文（单向）已经标注过的aspect的信息，来指导当前aspect标注。&lt;/p&gt;
&lt;p&gt;STN是LSTM，用于获得opinion的摘要信息。首先，STN单元获得基于给定aspect的opinion的表示，接下来利用attention机制来获得基于全局的opinion的表示。自此就可以获得基于当前aspect的opinion摘要。将aspect的表示和opinion摘要拼接作为特征，用于标注。&lt;/p&gt;
&lt;p&gt;（表示就是一个框里三个圆圆）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704954212132.png&#34;
	width=&#34;1811&#34;
	height=&#34;1203&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704954212132_hu9098182855452119018.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704954212132_hu5815002755997944694.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704954212132&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;将ATE形式化为一个seq2seq的学习任务。在这个任务中，源序列和目标序列分别由单词和标签组成。为了使Seq2Seq学习更适合ATE,作者设计了门控单元网络和位置感知注意力机制。门控单元网络用于将相应的单词表示融入解码器，而位置感知注意力机制则用于更多地关注目标词的相邻词。&lt;/p&gt;
&lt;p&gt;decoder包含一个门控单元，用于控制编码器和解码器产生的隐状态。当解码标签时，这个门控单元可以自动的整合来自编码器和解码器隐状态的信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704955929276.png&#34;
	width=&#34;1851&#34;
	height=&#34;1163&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704955929276_hu14855034582785969387.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704955929276_hu15496416382940042424.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704955929276&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;381px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;masked seq2seq。首先，对输入句子的连续几个词进行掩码处理。然后，encoder接收部分掩码的句子及其标签序列作为输入，decoder尝试根据编码上下文和标签信息重建句子原文。要求保持opinion target位置不变&lt;/p&gt;
&lt;h4 id=&#34;观点抽取oe&#34;&gt;观点抽取（OE）
&lt;/h4&gt;&lt;p&gt;一般都是先抽取aspect，在对aspect进行情感预测的流水线方式&lt;/p&gt;
&lt;p&gt;IMN使用非流水线方式。与传统的多任务学习方法依赖于学习不同任务的共同特征不同，IMN引入了一种消息传递体系结构，通过一组共享的潜在变量将信息迭代地传递给不同的任务&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704957048177.png&#34;
	width=&#34;1889&#34;
	height=&#34;961&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704957048177_hu2394841028843130231.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704957048177_hu15981515056896036166.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704957048177&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;196&#34;
		data-flex-basis=&#34;471px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;它接受一系列tokens{x1，…，xn}作为特征提取组件fθs的输入，该组件在所有任务之间共享。该组件由单词嵌入层和几个特征提取层（好多个CNN）组成。输出所有任务共享的潜在向量{hs1，hs2，…，hsn}的序列。该潜在向量序列会根据来自不同任务组件传播来的信息来更新。&lt;/p&gt;
&lt;p&gt;$hi^{s(T)}$ 表示为t轮消息传递后Xi对应的共享潜在向量的值。&lt;/p&gt;
&lt;p&gt;共享潜在向量序列用作不同任务特定组件的输入。每个特定于任务的组件都有自己的潜在变量和输出变量集。输出变量对应于序列标签任务中的标签序列；在AE中，我们为每个令牌分配一个标签，表明它是否属于任何aspect或opinion，而在AS中，我们为每个单词加上它的情感标签。在分类任务中，输出对应于输入实例的标签：情感分类任务(DS)的文档的情感，以及领域分类任务(DD)的文档域。在每次迭代中，适当的信息被传递回共享的潜在向量以进行组合；这可以是输出变量的值，也可以是潜在变量的值，具体取决于任务。 此外，我们还允许在每次迭代中在组件之间传递消息（opinion transmission）。&lt;/p&gt;
&lt;p&gt;感觉有点训练词向量的感觉，像是预处理一下得到向量序列来方便其他任务。&lt;/p&gt;
&lt;p&gt;【超，好像这些都不是考试重点】&lt;/p&gt;
&lt;h2 id=&#34;属性级情感分类&#34;&gt;属性级情感分类
&lt;/h2&gt;&lt;h1 id=&#34;for-exam&#34;&gt;For exam
&lt;/h1&gt;&lt;p&gt;试卷题型：简答题 40 分（5*8）好多个问号（内容为胡老师讲的基础部分）+ 综合题 60 分（内容为曹老师讲的核心应用部分）&lt;/p&gt;
&lt;p&gt;简答题重点章节：&lt;/p&gt;
&lt;p&gt;什么是语言模型、神经网络语言模型、几种、特点（优点）&lt;/p&gt;
&lt;p&gt;概念性的简答题， 不难+&lt;/p&gt;
&lt;p&gt;第4章 语言模型+词向量 （要求掌握：语言模型概念，神经网络语言模型 ）&lt;/p&gt;
&lt;p&gt;第 5章 NLP中的注意力机制 （全部要求掌握）概念、用处&lt;/p&gt;
&lt;p&gt;第 7 章 预训练语言模型（全部要求掌握）[主要掌握GPT，BERT 是 怎么训练的，与下游任务是如何对接的]prompt，inconcert learning，思维链【建模的几种范式】&lt;/p&gt;
&lt;p&gt;主观题重点章节： 设计东西&lt;/p&gt;
&lt;p&gt;第9章 情感分析（要求掌握：方面级情感分析基本方法原理）&lt;/p&gt;
&lt;p&gt;第10章 信息抽取 （要求掌握：实体和关系联合抽取基本方法原理）&lt;/p&gt;
&lt;p&gt;第 11章 问答系统（要求掌握：检索式问答系统基本方法原理）&lt;/p&gt;
&lt;h2 id=&#34;语言模型概念&#34;&gt;语言模型概念
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349075208.png&#34;
	width=&#34;1880&#34;
	height=&#34;1373&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349075208_hu7148588915045855034.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349075208_hu15066290272219750648.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349075208&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;328px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349189011.png&#34;
	width=&#34;1915&#34;
	height=&#34;1431&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349189011_hu25984211657887555.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349189011_hu9453327631917202114.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349189011&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;321px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349255995.png&#34;
	width=&#34;1917&#34;
	height=&#34;1436&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349255995_hu14041180707454219547.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704349255995_hu2862239320593814500.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349255995&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型
&lt;/h2&gt;&lt;p&gt;统计的方法使用最大似然估计，需要数据平滑否则会出现0概率问题。&lt;/p&gt;
&lt;p&gt;神经网络使用DNN和RNN&lt;/p&gt;
&lt;p&gt;利用RNN 语言模型可以解决以上概率语言模型问题，在神经网络一般用RNN语言模型&lt;/p&gt;
&lt;h3 id=&#34;一些我不会的背景知识&#34;&gt;一些（我不会的）背景知识
&lt;/h3&gt;&lt;h4 id=&#34;梯度下降算法&#34;&gt;梯度下降算法
&lt;/h4&gt;&lt;p&gt;梯度下降法是一种常用的优化算法，主要用于找到函数的局部最小值。它的基本思想是：在每一步迭代过程中，选择函数在当前点的负梯度（即函数在该点下降最快的方向）作为搜索方向，然后按照一定的步长向该方向更新当前点，不断迭代，直到满足停止准则。&lt;/p&gt;
&lt;p&gt;具体来说，假设我们要最小化一个可微函数$f(x)$，我们首先随机选择一个初始点$x_0$，然后按照以下规则更新$x$：&lt;/p&gt;
&lt;p&gt;$$
x_{n+1} = x_n - \alpha \nabla f(x_n)
$$&lt;/p&gt;
&lt;p&gt;其中，$\nabla f(x_n)$是函数$f$在点$x_n$处的梯度，$\alpha$是步长（也称为学习率），控制着每一步更新的幅度。&lt;/p&gt;
&lt;p&gt;梯度下降法只能保证找到局部最小值&lt;/p&gt;
&lt;h4 id=&#34;双曲正切函数&#34;&gt;双曲正切函数
&lt;/h4&gt;&lt;p&gt;它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在&lt;/p&gt;
&lt;p&gt;$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$&lt;/p&gt;
&lt;h4 id=&#34;bp算法&#34;&gt;BP算法
&lt;/h4&gt;&lt;p&gt;反向传播算法，当正向传播得到的结果和预期不符，则反向传播，修改权重&lt;/p&gt;
&lt;h4 id=&#34;bptt算法&#34;&gt;BPTT算法
&lt;/h4&gt;&lt;p&gt;是BP算法的拓展，可以处理具有时间序列结构的数据，用于训练RNN&lt;/p&gt;
&lt;p&gt;BPTT的工作原理如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;正向传播&lt;/strong&gt; ：在每个时间步，网络会读取输入并计算输出。这个过程会持续进行，直到处理完所有的输入序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt; ：一旦完成所有的正向传播步骤，网络就会计算最后一个时间步的误差（即网络的预测与实际值之间的差距），然后将这个误差反向传播到前一个时间步。这个过程会持续进行，直到误差被传播回第一个时间步。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;参数更新&lt;/strong&gt; ：在误差反向传播的过程中，网络会计算误差关于每个参数的梯度。然后，这些梯度会被用来更新网络的参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;one-hot编码&#34;&gt;one-hot编码
&lt;/h4&gt;&lt;p&gt;独热编码是一种将离散的分类标签转换为二进制向量的方法&lt;/p&gt;
&lt;p&gt;假设我们要做一个分类任务，总共有3个类别，分别是猫、狗、人。那这三个类别就是一种离散的分类：它们之间互相独立，不存在谁比谁大、谁比谁先、谁比谁后的关系。&lt;/p&gt;
&lt;p&gt;在神经网络中，需要一种数学的表示方法，来表示猫、狗、人的分类。最容易想到的，便是以 0 代表猫，以 1 代表狗，以 2 代表人这种简单粗暴的方式。但这样会导致分类标签之间出现了不对等的情况。（2比1大……）&lt;/p&gt;
&lt;p&gt;而进行如下的编码的话就可以解决这个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;猫：[1, 0, 0]&lt;/li&gt;
&lt;li&gt;狗：[0, 1, 0]&lt;/li&gt;
&lt;li&gt;人：[0, 0, 1]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这就是独热码&lt;/p&gt;
&lt;h4 id=&#34;pairwise&#34;&gt;pairwise
&lt;/h4&gt;&lt;p&gt;&amp;ldquo;Pairwise&amp;quot;是一种常用于排序和推荐系统的方法。它的主要思想是将排序问题转换为二元分类问题。每次取一对样本，预估这一对样本的先后顺序，不断重复预估一对对样本，从而得到某条查询下完整的排序。如果文档A的相关性高于文档B，则赋值+1，反之则赋值-1。这样，我们就得到了二元分类器训练所需的训练样本&lt;/p&gt;
&lt;p&gt;Pairwise方法也有其缺点。例如，它只考虑了两篇文档的相对顺序，没有考虑他们出现在搜索结果列表中的位置。&lt;/p&gt;
&lt;p&gt;除了Pairwise，还有其他的方法如Pointwise和Listwise。Pointwise方法每次仅仅考虑一个样本，预估的是每一条和查询的相关性，基于此进行排序。而Listwise方法则同时考虑多个样本，找到最优顺序。这些方法各有优缺点，选择哪种方法取决于具体的应用场景和需求。&lt;/p&gt;
&lt;h4 id=&#34;zero-shot&#34;&gt;zero-shot
&lt;/h4&gt;&lt;p&gt;&amp;ldquo;Zero-shot learning&amp;rdquo;（零样本学习）是一种机器学习范式，它允许模型在没有先前训练过相关数据集的情况下，对不包含在训练数据中的类别或任务进行准确的预测或推断。这种能力是由先进的深度学习模型和迁移学习方法得以实现的。&lt;/p&gt;
&lt;p&gt;举个例子，假设我们的模型已经能够识别马，老虎和熊猫了，现在需要该模型也识别斑马，那么我们需要告诉模型，怎样的对象才是斑马，但是并不能直接让模型看见斑马。所以模型需要知道的信息是马的样本、老虎的样本、熊猫的样本和样本的标签，以及关于前三种动物和斑马的描述。&lt;/p&gt;
&lt;p&gt;这种方法的优点是可以极大地节省标注量。不需要增加样本，只需要增加描述即可。&lt;/p&gt;
&lt;h4 id=&#34;ppo&#34;&gt;PPO
&lt;/h4&gt;&lt;p&gt;PPO（Proximal Policy Optimization，近端策略优化）是一种强化学习算法，由OpenAI在2017年提出。PPO算法的目标是解决深度强化学习中策略优化的问题。&lt;/p&gt;
&lt;p&gt;PPO算法的核心在于更新策略梯度，主流方法有两种，分别是KL散度做penalty，另一种是Clip剪裁，它们的主要作用都是限制策略梯度更新的幅度，从而推导出不同的神经网络参数更新方式。&lt;/p&gt;
&lt;p&gt;PPO算法具备Policy Gradient、TRPO的部分优点，采样数据和使用随机梯度上升方法优化代替目标函数之间交替进行，虽然标准的策略梯度方法对每个数据样本执行一次梯度更新，但PPO提出新目标函数，可以实现小批量更新。&lt;/p&gt;
&lt;h3 id=&#34;dnnnnlm&#34;&gt;DNN（NNLM）
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704359720351.png&#34;
	width=&#34;583&#34;
	height=&#34;489&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704359720351_hu13849669228920333610.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704359720351_hu6881378755015954780.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704359720351&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;286px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-grambigram&#34;&gt;2-gram（bigram）
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364780556.png&#34;
	width=&#34;1383&#34;
	height=&#34;965&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364780556_hu5206342092695132015.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364780556_hu697973188039246000.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704364780556&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;343px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364894689.png&#34;
	width=&#34;1747&#34;
	height=&#34;1247&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364894689_hu12650470872253249758.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704364894689_hu11019578300207287735.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704364894689&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;336px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中$\theta$就是训练过程中要学习的参数，有了这些参数就可以直接的到 $p(w_i|w_{i-1})$， 找到一组足够好的参数，就能让得到的$p(w_i|w_{i-1})$最接近训练语料库&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704365950174.png&#34;
	width=&#34;1037&#34;
	height=&#34;963&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704365950174_hu243089660874867028.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704365950174_hu6784773614582394759.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704365950174&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;107&#34;
		data-flex-basis=&#34;258px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这里的最大化就是损失函数最小（最接近0），因为P永远小于1，所以log永远是负数，他们加起来永远小于0，让log最大，也就是让log最接近0&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366274757.png&#34;
	width=&#34;1703&#34;
	height=&#34;1093&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366274757_hu7942169347453778617.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366274757_hu16929117068862146117.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366274757&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;373px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;n-gram&#34;&gt;n-gram
&lt;/h4&gt;&lt;p&gt;拓展一下罢了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366342703.png&#34;
	width=&#34;1463&#34;
	height=&#34;973&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366342703_hu155407216083637494.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366342703_hu2147065768408602024.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366342703&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366354123.png&#34;
	width=&#34;1753&#34;
	height=&#34;1131&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366354123_hu4173227589321463038.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366354123_hu2795487770049872044.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366354123&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;371px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366370789.png&#34;
	width=&#34;1699&#34;
	height=&#34;1149&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366370789_hu4608303676490383403.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704366370789_hu9801645650636107861.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366370789&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;354px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;rnnrnnlm&#34;&gt;RNN（RNNLM）
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367534089.png&#34;
	width=&#34;1053&#34;
	height=&#34;603&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367534089_hu1901926409324907308.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367534089_hu8303562354694809749.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367534089&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367561457.png&#34;
	width=&#34;1913&#34;
	height=&#34;1245&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367561457_hu3775081826526692143.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367561457_hu9928910760197121293.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367561457&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;153&#34;
		data-flex-basis=&#34;368px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367582922.png&#34;
	width=&#34;1853&#34;
	height=&#34;1329&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367582922_hu13774891187685011919.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704367582922_hu14354817128195904025.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367582922&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;334px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704368432414.png&#34;
	width=&#34;2015&#34;
	height=&#34;1253&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704368432414_hu3478665190384474791.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704368432414_hu14248960346229469281.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704368432414&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;385px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;词向量&#34;&gt;词向量
&lt;/h2&gt;&lt;p&gt;自然语言问题要用计算机处理时，第一步要找一种方法把这些符号数字化，成为计算机方便处理的形式化表示&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NNLM模型词向量&lt;/li&gt;
&lt;li&gt;RNNLM模型词向量&lt;/li&gt;
&lt;li&gt;C&amp;amp;W 模型词向量&lt;/li&gt;
&lt;li&gt;CBOW 模型词向量&lt;/li&gt;
&lt;li&gt;Skip-gram模型词向量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不同模型的词向量之间的主要区别在于它们捕获和编码词义和上下文信息的方式。以下是一些常见模型的词向量特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;神经网络语言模型（NNLM）&lt;/strong&gt; ：NNLM通过学习预测下一个词的任务来生成词向量。这种方法可以捕获词义和词之间的关系，但是它通常无法捕获长距离的依赖关系，因为它只考虑了固定大小的上下文。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;循环神经网络语言模型（RNNLM）&lt;/strong&gt; ：RNNLM使用循环神经网络结构，可以处理变长的输入序列，并能捕获长距离的依赖关系。因此，RNNLM生成的词向量可以包含更丰富的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt; ：Word2Vec是一种预训练词向量的方法，它包括两种模型：Skip-gram和CBOW。Skip-gram模型通过一个词预测其上下文，而CBOW模型则通过上下文预测一个词。Word2Vec生成的词向量可以捕获词义和词之间的各种关系，如同义词、反义词、类比关系等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GloVe&lt;/strong&gt; ：GloVe（Global Vectors for Word Representation）是另一种预训练词向量的方法，它通过对词-词共现矩阵进行分解来生成词向量。GloVe生成的词向量可以捕获词义和词之间的线性关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; ：BERT（Bidirectional Encoder Representations from Transformers）使用Transformer模型结构，并通过预训练任务（如Masked Language Model和Next Sentence Prediction）来生成词向量。BERT生成的词向量是上下文相关的，也就是说，同一个词在不同的上下文中可能有不同的词向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，不同模型的词向量之间的区别主要在于它们捕获和编码词义和上下文信息的方式。选择哪种词向量取决于具体的任务需求和计算资源。&lt;/p&gt;
&lt;h3 id=&#34;nnlm的词向量&#34;&gt;NNLM的词向量
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369375532.png&#34;
	width=&#34;1459&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369375532_hu11791388516145315893.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369375532_hu5347906828137267166.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369375532&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;326px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;解决办法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369411606.png&#34;
	width=&#34;1977&#34;
	height=&#34;1085&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369411606_hu18010801362951293503.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369411606_hu2962734281792836040.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369411606&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;437px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;通过一个|D| * |V|的矩阵，额可以将one-shot的编码转为D维的稠密的词向量，所以管他叫lookup表&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369517877.png&#34;
	width=&#34;1845&#34;
	height=&#34;1273&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369517877_hu6022174232349225325.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369517877_hu12170194622389324118.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369517877&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369568799.png&#34;
	width=&#34;1783&#34;
	height=&#34;1209&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369568799_hu2629325575528303610.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369568799_hu3081145315247324033.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369568799&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;NNLM 语言模型在训练语言模型同时也训练了词向量&lt;/p&gt;
&lt;h3 id=&#34;rnnlm的词向量&#34;&gt;RNNLM的词向量
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369951464.png&#34;
	width=&#34;1795&#34;
	height=&#34;1075&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369951464_hu5335953422025431181.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704369951464_hu6351636930104683506.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369951464&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704370827646.png&#34;
	width=&#34;1955&#34;
	height=&#34;1039&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704370827646_hu3837567339487575745.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704370827646_hu7794605885183383365.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704370827646&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;cw&#34;&gt;C&amp;amp;W
&lt;/h3&gt;&lt;p&gt;C&amp;amp;W模型是靠两边猜中间的一种模型，输入层是wi上下文的词向量&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704371807694.png&#34;
	width=&#34;1935&#34;
	height=&#34;1207&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704371807694_hu5924944851468101737.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704371807694_hu9652196218904914160.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704371807694&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;score是wi中间这个word在这个位置有多合理，越高越合理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372099728.png&#34;
	width=&#34;1669&#34;
	height=&#34;1173&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372099728_hu89602233514865288.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372099728_hu12309457551597153123.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372099728&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;正样本通常是指在实际语料库中出现过的词语及其上下文。负样本则是人为构造的，通常是将一个词与一个随机的上下文配对。&lt;/p&gt;
&lt;p&gt;Pairwise方法在训练C&amp;amp;W词向量时，主要是通过比较一对词的上下文来进行训练的。具体来说，对于每一对词（一个正样本和一个负样本），我们都会计算它们的词向量，并通过比较这两个词向量的相似度来更新我们的模型。&lt;/p&gt;
&lt;p&gt;在训练过程中，我们首先需要选择一个损失函数，这里是修改后的HingeLoss&lt;/p&gt;
&lt;p&gt;然后，我们会使用一种优化算法来最小化这个损失函数，这里是梯度下降，在每一次迭代中，我们都会根据当前的损失来更新我们的词向量。&lt;/p&gt;
&lt;p&gt;训练的目标是在正样本中的score高，负样本的score低，然后score差的越大效果越好&lt;/p&gt;
&lt;h3 id=&#34;cbow&#34;&gt;CBOW
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372201199.png&#34;
	width=&#34;1851&#34;
	height=&#34;1247&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372201199_hu8310480521774336892.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372201199_hu6275935430363075353.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372201199&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;356px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;CBOW也是靠两边猜中间，输入层是wi上下文词向量的平均值，目标是最小化（最收敛与0）上下文词的平均与目标词之间的距离。输出是&lt;/p&gt;
&lt;h3 id=&#34;skip-gram&#34;&gt;skip-gram
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372268757.png&#34;
	width=&#34;1903&#34;
	height=&#34;1265&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372268757_hu553392154223137432.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704372268757_hu13513263286140047249.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372268757&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;skip-gram是知道中间猜两边，训练最小化（最收敛于0）目标词与上下文词之间的距离。&lt;/p&gt;
&lt;h2 id=&#34;注意力机制&#34;&gt;注意力机制
&lt;/h2&gt;&lt;h3 id=&#34;概述&#34;&gt;概述
&lt;/h3&gt;&lt;p&gt;在注意力机制中，Q、K、V 分别代表查询（Query）、键（Key）和值（Value）。&lt;/p&gt;
&lt;p&gt;注意力机制的工作过程可以简单概括为：对于每一个查询，计算它与所有键的匹配程度（通常使用点积），然后对这些匹配程度进行归一化（通常使用 softmax 函数），得到每个键对应的&lt;strong&gt;权重&lt;/strong&gt;。最后，用这些权重对所有的值进行加权求和，得到最终的输出。&lt;/p&gt;
&lt;p&gt;这种机制允许模型在处理一个元素时，考虑到其他相关元素的信息，从而捕捉输入元素之间的依赖关系。在自然语言处理、计算机视觉等领域，注意力机制已经被广泛应用，并取得了显著的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978816700.png&#34;
	width=&#34;1335&#34;
	height=&#34;435&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978816700_hu4462468707929930117.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978816700_hu9955950540691445957.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978816700&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;306&#34;
		data-flex-basis=&#34;736px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978826803.png&#34;
	width=&#34;1409&#34;
	height=&#34;681&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978826803_hu12027745851328612262.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978826803_hu6442961506963118138.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978826803&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;496px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978837242.png&#34;
	width=&#34;1409&#34;
	height=&#34;689&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978837242_hu5701760137382784419.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704978837242_hu15067890877558608844.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978837242&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;204&#34;
		data-flex-basis=&#34;490px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;W是权重，都是学来的。&lt;/p&gt;
&lt;p&gt;参考&lt;/p&gt;
&lt;p&gt;KQV矩阵： &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;amp;vd_source=2fbfeabcc6cdd857dcd6247eb0154d83&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;vd_source=2fbfeabcc6cdd857dcd6247eb0154d83&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Attention机制： &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1YA411G7Ep&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1YA411G7Ep&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455161185.png&#34;
	width=&#34;1261&#34;
	height=&#34;844&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455161185_hu9550958242342426309.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455161185_hu13731709649762223091.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704455161185&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;358px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455261000.png&#34;
	width=&#34;1334&#34;
	height=&#34;886&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455261000_hu1795727090414588602.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704455261000_hu9475837615987744653.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704455261000&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;K、V都是经过线性变换的词向量集合（矩阵）&lt;/p&gt;
&lt;p&gt;Q是隐藏状态（隐藏向量）&lt;/p&gt;
&lt;p&gt;A是一个注意力值，就是我们设置的这个字的注意力值&lt;/p&gt;
&lt;p&gt;通过attention的学习，可以得到a1、a2……，这些就是K中各个向量对Q的权重&lt;/p&gt;
&lt;p&gt;步骤1：计算 f ( Q ,Ki )&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467445387.png&#34;
	width=&#34;1222&#34;
	height=&#34;329&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467445387_hu573042396487751300.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467445387_hu8475829467231160794.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467445387&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;371&#34;
		data-flex-basis=&#34;891px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;步骤2：计算对于Q 各个 Ki 的权重&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467647363.png&#34;
	width=&#34;1220&#34;
	height=&#34;839&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467647363_hu10294667419307046925.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467647363_hu13678268764052841366.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467647363&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;步骤3：计算输出 Att-V值（各 Ki 乘以自己的权重，然后求和 ）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467679320.png&#34;
	width=&#34;1175&#34;
	height=&#34;770&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467679320_hu18213072229280182051.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704467679320_hu15390336586699316045.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467679320&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;366px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;举例1-seq2seqrnn2rnn的机器翻译中&#34;&gt;举例1， seq2seq（RNN2RNN）的机器翻译中
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704543763032.png&#34;
	width=&#34;2310&#34;
	height=&#34;1297&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704543763032_hu2416228904652508962.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704543763032_hu11660643713283632477.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704543763032&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;427px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;seq2seq做机器翻译的过程，需要大量的两种语言的平行语料，就是意思相同的语言的一一对应的关系。&lt;/p&gt;
&lt;p&gt;其中x为词向量，A为权重矩阵，h为隐藏状态（隐藏向量）。&lt;/p&gt;
&lt;p&gt;RNN是用预训练的词向量，然后通过学习权重矩阵A来微调，得到隐藏状态，可以理解为隐藏状态是带了上下文的更加符合RNN的词的向量表示。&lt;/p&gt;
&lt;p&gt;每一个时间步中，A都被微调， 因此x1、x2、x3的A可能都是不一样的。在大量预料的训练下会获得表现比较好的A和A&#39;&lt;/p&gt;
&lt;p&gt;h可以表示为$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$其中$h_t$ 是当前时间步的隐藏状态，$h_{t-1}$ 是前一时间步的隐藏状态，$x_t$ 是当前时间步的输入，$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b_h$ 是偏置项，$f$ 是激活函数&lt;/p&gt;
&lt;p&gt;不加注意力机智的seq2seq模型，encoder是RNN，decoder也是RNN，在encoder接受了$x_1$到$x_m$的词向量序列后，得到最终的隐藏状态$h_m$， 也就是$s_0$，作为decoder的初始状态。&lt;/p&gt;
&lt;p&gt;如果不加注意力机制，decoder那边也就是靠隐藏状态、x和参数（A矩阵，偏置值b）来继续进行RNN的步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550430429.png&#34;
	width=&#34;1957&#34;
	height=&#34;1270&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550430429_hu11810975591048376855.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550430429_hu15423165203858316339.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704550430429&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;369px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;现在我们引入注意力机制，也就是图上的$c_0$，权重的计算按照上面所说的KQV计算方法，这里K是词向量集合x1,x2&amp;hellip; Q是隐藏状态。也就是对于每一个隐藏状态，都可以求一个关于词向量序列的权重值$\alpha$。&lt;/p&gt;
&lt;p&gt;通过求出这一系列的$\alpha$，就可以加权求出上下文矩阵$c$，c知道当前隐藏状态和词向量矩阵的全部关系。&lt;/p&gt;
&lt;p&gt;加了注意力机制之后，decoder的各个隐藏状态求解过程就会向之前提到的那样变得更复杂&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551152595.png&#34;
	width=&#34;810&#34;
	height=&#34;274&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551152595_hu6561141564076664470.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551152595_hu12522388303645044446.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704551152595&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;295&#34;
		data-flex-basis=&#34;709px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;而每一个步骤的c都不一样，比如&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550965949.png&#34;
	width=&#34;2303&#34;
	height=&#34;1289&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550965949_hu12090806647308353477.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704550965949_hu16511135493717711051.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704550965949&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;428px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;c0是s0对于x1,x2&amp;hellip;的att-V，也就是hm对于x1,x2&amp;hellip;的att-V；c1是隐藏状态s1对于x1,x2&amp;hellip;的att-V，c2是隐藏状态s2对于x1,x2&amp;hellip;的att-V这些c都需要花算力来算&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551282267.png&#34;
	width=&#34;686&#34;
	height=&#34;415&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551282267_hu2850356592142729500.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704551282267_hu14724564460805022119.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704551282267&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;396px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;注意力机制的问题是时间复杂度太大了。如果是简单的RNN2RNN，如果encoder词向量矩阵大小为m，decoder词向量矩阵大小为n，所需的时间复杂度为O(m+n)，而使用注意力机制之后就会变成O(mn)，还是打分函数比较简单的情况下。&lt;/p&gt;
&lt;h3 id=&#34;注意力编码机制&#34;&gt;注意力编码机制
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470189498.png&#34;
	width=&#34;1231&#34;
	height=&#34;851&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470189498_hu10679274088995285671.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470189498_hu4462398709098643524.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470189498&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470407421.png&#34;
	width=&#34;1216&#34;
	height=&#34;871&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470407421_hu9077372946137028041.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470407421_hu8526947382436987903.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470407421&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;335px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470394991.png&#34;
	width=&#34;1266&#34;
	height=&#34;806&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470394991_hu10177989624341252182.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704470394991_hu14852274426566810750.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470394991&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;376px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;attention机制还可以将不同序列融合编码（将多个序列经过某种处理或嵌入方式，转换为一个固定长度的向量或表示形式。）&lt;/p&gt;
&lt;p&gt;就是给每个词向量乘个权重加起来，被称作注意力池化（Attention Pooling）或加权求和（Weighted Sum）。这个操作的含义是将注意力权重分配给输入序列中的不同部分，从而形成一个汇聚了注意力的向量表示。&lt;/p&gt;
&lt;p&gt;这个操作的效果是聚焦于输入序列中具有更高注意力权重的部分，形成一个综合的表示，其中对于重要的部分有更大的贡献。这对于处理序列数据中的上下文信息，关注重要元素，以及实现对不同部分不同程度的关注都非常有用，特别是在自然语言处理中的任务中。&lt;/p&gt;
&lt;h2 id=&#34;预训练语言模型&#34;&gt;预训练语言模型
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705278999.png&#34;
	width=&#34;2297&#34;
	height=&#34;1405&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705278999_hu6148364222294170945.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705278999_hu13141951995967316973.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705278999&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;163&#34;
		data-flex-basis=&#34;392px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;迁移学习&#34;&gt;迁移学习
&lt;/h4&gt;&lt;p&gt;迁移学习（Transfer Learning）是一种机器学习方法，其核心思想是利用已有的知识来辅助学习新的知识。例如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#。&lt;/p&gt;
&lt;p&gt;迁移学习通常会关注有一个源域（源任务） $D_ {s}$ 和一个目标域（目标任务） $D_ {t}$ 的情况.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705772371.png&#34;
	width=&#34;2055&#34;
	height=&#34;1303&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705772371_hu14715286294481801945.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705772371_hu11622754388318737559.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705772371&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;迁移方式分为两种&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705819455.png&#34;
	width=&#34;1753&#34;
	height=&#34;1187&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705819455_hu13728448372062211467.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704705819455_hu17844380266122088749.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705819455&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;354px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;几个范式&#34;&gt;几个范式
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782526810.png&#34;
	width=&#34;991&#34;
	height=&#34;363&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782526810_hu11658811999004605953.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782526810_hu6122374904788392108.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704782526810&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;第三范式预训练-精调范式&#34;&gt;第三范式：预训练-精调范式
&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782261195.png&#34;
	width=&#34;1617&#34;
	height=&#34;1219&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782261195_hu15613038910977029143.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704782261195_hu15212538424616698871.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704782261195&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;318px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704785636077.png&#34;
	width=&#34;1575&#34;
	height=&#34;1011&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704785636077_hu5679361005501424414.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704785636077_hu8783815005090963338.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704785636077&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;373px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;自回归：预测序列的下一个或者上一个&lt;/p&gt;
&lt;p&gt;自编码：预测序列中的某一个或某几个&lt;/p&gt;
&lt;p&gt;广义自回归：和自回归主要区别在于他们处理输入数据的方式。自回归预训练语言模型在生成序列时，会一个接一个地生成新的词，每个新词都依赖于前面的词。如GPT，而广义自回归预训练语言模型则更为灵活，它们可以在生成序列时考虑更多的上下文信息，模型不仅可以查看前面的词，还可以查看后面的词或者整个序列。如XLNet&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704787153210.png&#34;
	width=&#34;1881&#34;
	height=&#34;1367&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704787153210_hu14883124871598954356.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704787153210_hu14859808629617235661.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704787153210&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;137&#34;
		data-flex-basis=&#34;330px&#34;
	
&gt;&lt;/p&gt;
&lt;h6 id=&#34;gpt训练和对接&#34;&gt;GPT训练和对接
&lt;/h6&gt;&lt;p&gt;GPT 采用了 Transformer 的 Decoder 部分，并且每个子层只有一个 Masked Multi Self-Attention（768 维向量和 12 个 Attention Head）和一个FeedForward （无普通transformer解码器层的编码器-解码器注意力子层），模型共叠加使用了 12 层的 Decoder。使用了从左向右的单向注意力机制&lt;/p&gt;
&lt;p&gt;Masked Multi Self-Attention的768维向量和12个attention head： 意思是12个独立的attention组件，每个组件的参数都独立，然后每个attention的Q向量都是768维，也可以理解为一个词在模型中的向量（或者说词嵌入）是768维]&lt;/p&gt;
&lt;p&gt;feedforward： 作用是提取更深层次的特征。在每个序列的位置单独应用一个全连接前馈网络，由两个线性层和一个激活函数组成。线性层将每个位置的表示扩展，为学习更复杂的特征提供可能性，激活函数帮助模型学习更复杂的非线性特征，第二个线性层将每个位置的表示压缩回原始维度。这样，位置特征敏感的部分就会被表达出来，提供给后续网络学习。&lt;/p&gt;
&lt;p&gt;就是十二个下图这样的小东西&lt;/p&gt;
&lt;p&gt;transformer输入有token embedding和position embedding&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704860957530.png&#34;
	width=&#34;1705&#34;
	height=&#34;653&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704860957530_hu8926197901563072272.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704860957530_hu6768007247378411936.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704860957530&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;261&#34;
		data-flex-basis=&#34;626px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对比一下transformer，transformer的decoder是6个右边的，少了一层multi-head attention的encoder-decoder注意力子层（cross-attention的那个子模块）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704861830326.png&#34;
	width=&#34;1745&#34;
	height=&#34;1215&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704861830326_hu1267590137560154136.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704861830326_hu490868326540095410.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704861830326&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;344px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;6层attention堆叠就是六个encoder就是个小的encoder，每个encoder里都有attention机制，上图N=6的意思。&lt;/p&gt;
&lt;p&gt;训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865198754.png&#34;
	width=&#34;1627&#34;
	height=&#34;1151&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865198754_hu18295127181586053728.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865198754_hu765383374413356849.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865198754&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;maximize负数=近0最小化&lt;/p&gt;
&lt;p&gt;与下游任务对接：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865491544.png&#34;
	width=&#34;1715&#34;
	height=&#34;1093&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865491544_hu7727105172578725578.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865491544_hu17796126958344873400.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865491544&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;376px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;把多序列通过一些特定的规则拼成一个单序列。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865594321.png&#34;
	width=&#34;1645&#34;
	height=&#34;1097&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865594321_hu14186312628356567511.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704865594321_hu4799036538612108705.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865594321&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;微调：&lt;/p&gt;
&lt;p&gt;任务微调有2种方式 ：① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务&lt;/p&gt;
&lt;p&gt;举例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866344121.png&#34;
	width=&#34;1631&#34;
	height=&#34;777&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866344121_hu14184095227643443681.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866344121_hu6724736624953045186.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866344121&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;503px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866727437.png&#34;
	width=&#34;1667&#34;
	height=&#34;1263&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866727437_hu2685094725974759744.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866727437_hu5484079396662048645.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866727437&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;131&#34;
		data-flex-basis=&#34;316px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这儿的$L_1(C)$是上面提到的预训练过程中的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866802266.png&#34;
	width=&#34;1043&#34;
	height=&#34;247&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866802266_hu10841169866586460984.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704866802266_hu8734186239891054702.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866802266&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;422&#34;
		data-flex-basis=&#34;1013px&#34;
	
&gt;&lt;/p&gt;
&lt;h6 id=&#34;bert训练和对接&#34;&gt;BERT训练和对接
&lt;/h6&gt;&lt;p&gt;用了transformer的encoder再加FFN（前馈神经网络，FFN 层有助于学习序列中的非线性关系和模式）层&lt;/p&gt;
&lt;p&gt;【但是transformer的encoder不是带FeedForward吗？】FFN仅在MLM过程中有用，而BERT的最终输出是模型在整个预训练过程中学到的表示的某种组合。这些表示在后续的任务中可以进一步微调或者用作特征。（BYD，原来只是训练过程中的一个b东西）&lt;/p&gt;
&lt;p&gt;下图中一个trm是一个子层，&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867651617.png&#34;
	width=&#34;1699&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867651617_hu15852950417108706317.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867651617_hu18055933110186604633.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704867651617&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867919696.png&#34;
	width=&#34;1743&#34;
	height=&#34;1183&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867919696_hu12126545177696037022.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704867919696_hu11709496378341585097.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704867919696&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在BERT模型中，输入的每个单词都会通过三种嵌入（embedding）进行编码&lt;/p&gt;
&lt;p&gt;Token Embedding：是将每个单词或者词片映射到一个向量，这个向量能够捕获该单词的语义信息。在BERT中，使用了WordPiece标记化，其中输入句子的每个单词都被分解成子词标记。这些标记的嵌入是随机初始化的，然后通过梯度下降进行训练。&lt;/p&gt;
&lt;p&gt;Segment Embedding：是用来区分不同的句子的。在处理两个句子的任务（如自然语言推理）时，BERT需要知道每个单词属于哪个句子。&lt;/p&gt;
&lt;p&gt;Position Embedding：由于Transformer模型并没有像循环神经网络那样的顺序性，因此需要显式地向模型添加位置信息，以保留句子中单词的顺序信息&lt;/p&gt;
&lt;p&gt;训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871798829.png&#34;
	width=&#34;1501&#34;
	height=&#34;1293&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871798829_hu8610259492780851718.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871798829_hu6056564884473094718.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704871798829&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;116&#34;
		data-flex-basis=&#34;278px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;MLM：把一个序列的几个word给mask了让模型猜的训练方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880632493.png&#34;
	width=&#34;1733&#34;
	height=&#34;1355&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880632493_hu7472892352835275265.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880632493_hu5891896495687017913.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880632493&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;(2).句子顺序模型训练&lt;/p&gt;
&lt;p&gt;凑一些下一句不是下一句的负样本来训练预训练模型对句子顺序的敏感。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871959170.png&#34;
	width=&#34;1653&#34;
	height=&#34;1143&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871959170_hu10496531795912410771.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704871959170_hu9268893923717152002.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704871959170&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对接：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704872111654.png&#34;
	width=&#34;1641&#34;
	height=&#34;1135&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704872111654_hu4750092083308442896.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704872111654_hu5822185689071956132.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704872111654&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;微调同样有两种① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务&lt;/p&gt;
&lt;h6 id=&#34;其他&#34;&gt;其他
&lt;/h6&gt;&lt;p&gt;RoBERTa：把BERT使用Adam默认的参数改为使用更大的batches，训练时把静态mask改为动态mask。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704873163103.png&#34;
	width=&#34;1581&#34;
	height=&#34;451&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704873163103_hu3963849609435003651.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704873163103_hu11226990512560464996.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704873163103&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;350&#34;
		data-flex-basis=&#34;841px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;BART：GPT只用了transformer的decoder，BERT只用了transformer的encoder。导致&lt;/p&gt;
&lt;p&gt;BERT具备双向语言理解能力的却不具备做生成任务的能力。GPT拥有自回归特性的却不能更好的从双向理解语言.&lt;/p&gt;
&lt;p&gt;（模型的&amp;quot;自回归&amp;quot;特性指的是，当前的观察值是过去观察值的加权平均和一个随机项）&lt;/p&gt;
&lt;p&gt;BART使用标准的Transformer结构为基础，吸纳BERT和GPT的优点，使用&lt;strong&gt;多种噪声破坏原文本&lt;/strong&gt;，再将残缺文本通过序列到序列的任务重新复原（降噪自监督）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880759669.png&#34;
	width=&#34;1643&#34;
	height=&#34;937&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880759669_hu731822508231366444.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880759669_hu8761662701064972921.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880759669&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;BERT在预测时加了额外的FFN, 而BART没使用FFN.&lt;/p&gt;
&lt;p&gt;（还记得这个Beyond吗）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880695071.png&#34;
	width=&#34;783&#34;
	height=&#34;565&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880695071_hu3131767436758927975.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704880695071_hu9286743150284470608.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880695071&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;T5&lt;/p&gt;
&lt;p&gt;给整个 NLP 预训练模型领域提供了一个通用框架，把所有NLP任务都转化成一种形式(Text-to-Text)，通过这样的方式可以用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务。以后的各种NLP任务，只需针对一个超大预训练模型，考虑怎么把任转换成合适的文本输入输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881664053.png&#34;
	width=&#34;1737&#34;
	height=&#34;1085&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881664053_hu7964880614041283345.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881664053_hu6445767833377681690.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881664053&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881694268.png&#34;
	width=&#34;885&#34;
	height=&#34;581&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881694268_hu2482336117203447256.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881694268_hu14203005275127060711.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881694268&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881712622.png&#34;
	width=&#34;1377&#34;
	height=&#34;1167&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881712622_hu16680869906482496881.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881712622_hu2376424321398539483.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881712622&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;283px&#34;
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;第四范式预训练提示预测范式pre-trainpromptpredict&#34;&gt;第四范式：预训练，提示，预测范式（Pre-train,Prompt,Predict）
&lt;/h5&gt;&lt;p&gt;prompt挖掘工程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881803249.png&#34;
	width=&#34;1213&#34;
	height=&#34;457&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881803249_hu13436209395521074123.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704881803249_hu400279335658290653.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881803249&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;265&#34;
		data-flex-basis=&#34;637px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;特点：不通过目标工程使预训练的语言模型（LM）适应下游任务，而是将下游任务建模的方式重新定义（Reformulate），通过利用合适prompt实现不对预训练语言模型改动太多，尽量在原始 LM上解决任务的问题。&lt;/p&gt;
&lt;p&gt;实现方法eg：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882346953.png&#34;
	width=&#34;1153&#34;
	height=&#34;903&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882346953_hu7586107885839066189.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882346953_hu18395355479921493855.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882346953&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882361576.png&#34;
	width=&#34;1551&#34;
	height=&#34;1067&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882361576_hu7561047822722738764.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882361576_hu17707468956273518316.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882361576&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882378196.png&#34;
	width=&#34;1743&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882378196_hu11233012488109695091.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882378196_hu18192133209930565210.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882378196&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882674432.png&#34;
	width=&#34;1663&#34;
	height=&#34;1101&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882674432_hu13587885341611662869.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882674432_hu6574086317527920283.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882674432&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;362px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882723263.png&#34;
	width=&#34;1499&#34;
	height=&#34;1165&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882723263_hu15934204195131867694.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882723263_hu8130282241809025503.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882723263&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;308px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;要素：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882803241.png&#34;
	width=&#34;1605&#34;
	height=&#34;1131&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882803241_hu11243186754304041644.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882803241_hu13242270450688689761.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882803241&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;输入端&lt;/p&gt;
&lt;p&gt;prompt工程&lt;/p&gt;
&lt;p&gt;完形填空和前缀提示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882845802.png&#34;
	width=&#34;1663&#34;
	height=&#34;1033&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882845802_hu5360227835705963459.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882845802_hu16844713461182656559.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882845802&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;386px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882862203.png&#34;
	width=&#34;1575&#34;
	height=&#34;935&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882862203_hu4923635207357229542.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704882862203_hu8729982319631759787.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882862203&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;模板创建&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883318434.png&#34;
	width=&#34;1649&#34;
	height=&#34;1057&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883318434_hu2031580311689760221.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883318434_hu13604712488439325149.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883318434&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;输出端&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883343584.png&#34;
	width=&#34;1501&#34;
	height=&#34;961&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883343584_hu10159854810570904305.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883343584_hu1409993135558243725.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883343584&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883381023.png&#34;
	width=&#34;1511&#34;
	height=&#34;493&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883381023_hu14144485234484677976.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883381023_hu3096513643290003920.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883381023&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;306&#34;
		data-flex-basis=&#34;735px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;微调&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883499101.png&#34;
	width=&#34;1721&#34;
	height=&#34;721&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883499101_hu16376653817208783887.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704883499101_hu11276345419966855526.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883499101&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;572px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;生成类任务用法与第五范式相同&lt;/p&gt;
&lt;h5 id=&#34;第五范式大模型&#34;&gt;第五范式：大模型
&lt;/h5&gt;&lt;p&gt;大语言模型 (Large Language Model，LLM) 通常指由大量参数（通常数十亿个权重或更多）组成的人工神经网络预训练语言模型，使用大量的计算资源在海量数据上进行训练。&lt;/p&gt;
&lt;p&gt;大型语言模型是通用的模型，在广泛的任务（例如情感分析、命名实体识别或数学推理）中表现出色，具有与人类认证对齐的特点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890488901.png&#34;
	width=&#34;1745&#34;
	height=&#34;1177&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890488901_hu10754120118364663090.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890488901_hu4510787958984072316.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704890488901&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;355px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890734795.png&#34;
	width=&#34;511&#34;
	height=&#34;1397&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890734795_hu13613932846090635315.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704890734795_hu9129181308305722667.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704890734795&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;36&#34;
		data-flex-basis=&#34;87px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891371789.png&#34;
	width=&#34;1031&#34;
	height=&#34;367&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891371789_hu1688961027346648908.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891371789_hu12704699035741257340.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891371789&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;280&#34;
		data-flex-basis=&#34;674px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891495297.png&#34;
	width=&#34;1289&#34;
	height=&#34;689&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891495297_hu4784850340188345910.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891495297_hu16529497992066648152.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891495297&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;187&#34;
		data-flex-basis=&#34;448px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891515801.png&#34;
	width=&#34;1071&#34;
	height=&#34;381&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891515801_hu4630454084288122190.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891515801_hu938004935632843284.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891515801&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;281&#34;
		data-flex-basis=&#34;674px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891548168.png&#34;
	width=&#34;1689&#34;
	height=&#34;1053&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891548168_hu623298517607735383.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704891548168_hu5124767124848243865.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891548168&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;不需要任务模型的意思是只要有预训练就行&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892005089.png&#34;
	width=&#34;1367&#34;
	height=&#34;1053&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892005089_hu6490540247374804975.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892005089_hu13217813354372715640.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892005089&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;311px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892027838.png&#34;
	width=&#34;1331&#34;
	height=&#34;759&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892027838_hu15547625226989968101.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892027838_hu9706457580677688775.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892027838&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;（我靠，这要传统注意力算死了）&lt;/p&gt;
&lt;p&gt;学习方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892066074.png&#34;
	width=&#34;1807&#34;
	height=&#34;1279&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892066074_hu18266335367921637302.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892066074_hu18370171913557314051.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892066074&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因为上下文学习，在使用的时候也可以用zero-shot, one-shot和few-shot。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892306373.png&#34;
	width=&#34;1701&#34;
	height=&#34;1129&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892306373_hu13173518649217814168.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892306373_hu7431608758380709257.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892306373&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892321288.png&#34;
	width=&#34;1479&#34;
	height=&#34;593&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892321288_hu5752156193903261076.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892321288_hu5650352027565034447.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892321288&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;598px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;chain-of-thought&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892359407.png&#34;
	width=&#34;1833&#34;
	height=&#34;1243&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892359407_hu7192301161315741032.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892359407_hu13613672281411163690.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892359407&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892375228.png&#34;
	width=&#34;1669&#34;
	height=&#34;887&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892375228_hu2209185251116944039.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892375228_hu9886370453575105156.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892375228&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892402637.png&#34;
	width=&#34;1693&#34;
	height=&#34;1069&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892402637_hu2964137903259344483.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892402637_hu3870580862370189520.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892402637&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892464014.png&#34;
	width=&#34;1571&#34;
	height=&#34;945&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892464014_hu15004423879866489559.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892464014_hu7225937244381743371.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892464014&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;398px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892496826.png&#34;
	width=&#34;1663&#34;
	height=&#34;1063&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892496826_hu1895004684451289270.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892496826_hu6750107482027946969.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892496826&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;375px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;与人类对齐：RLHF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892569477.png&#34;
	width=&#34;1573&#34;
	height=&#34;993&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892569477_hu7586391511272646600.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704892569477_hu8914919997900683168.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892569477&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;简而言之：1、在人工标注数据上SFT（有监督微调）模型&lt;/p&gt;
&lt;p&gt;2、多模型给标注人员做排序，用来训练奖励模型（RM）&lt;/p&gt;
&lt;p&gt;3、使用强化学习PPO算法，交互地优化模型参数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894207867.png&#34;
	width=&#34;1399&#34;
	height=&#34;759&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894207867_hu1332232816591556732.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894207867_hu8745407441154241832.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894207867&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;184&#34;
		data-flex-basis=&#34;442px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;文本分类在各个范式上的例子&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894268859.png&#34;
	width=&#34;1503&#34;
	height=&#34;1237&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894268859_hu8771123388892522953.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894268859_hu14078665449389149866.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894268859&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;121&#34;
		data-flex-basis=&#34;291px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894283289.png&#34;
	width=&#34;1597&#34;
	height=&#34;1147&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894283289_hu14537641212256313193.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894283289_hu3089359921073626067.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894283289&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;334px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894294431.png&#34;
	width=&#34;1573&#34;
	height=&#34;1239&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894294431_hu7514295359827945179.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894294431_hu3157246104607377348.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894294431&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;304px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894305898.png&#34;
	width=&#34;1519&#34;
	height=&#34;1217&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894305898_hu7120198363506593785.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894305898_hu8510343385631668390.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894305898&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;299px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894318376.png&#34;
	width=&#34;1537&#34;
	height=&#34;1197&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894318376_hu9107351236652778168.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894318376_hu17683178309041845731.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894318376&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;308px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894335472.png&#34;
	width=&#34;1569&#34;
	height=&#34;1177&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894335472_hu18407232659461768239.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894335472_hu12232235605293595467.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894335472&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;319px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894354621.png&#34;
	width=&#34;1707&#34;
	height=&#34;1257&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894354621_hu8169211974197776655.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704894354621_hu18266681990100247118.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894354621&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;方面级情感分类&#34;&gt;方面级情感分类
&lt;/h2&gt;&lt;p&gt;方面级情感分类（Aspect-Level Sentiment Classification）是自然语言处理（NLP）中的一个任务，它的目标是识别文本中特定方面的情感倾向。例如，在产品评论中，“这款手机的电池寿命很长，但屏幕质量差。”这句话中，“电池寿命”这个方面的情感是积极的，而“屏幕质量”这个方面的情感是消极的。所以，方面级情感分类不仅要识别出文本中的各个方面，还要判断这些方面的情感倾向。这个任务在许多领域都有应用，比如产品评论分析、社交媒体监控等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946258919.png&#34;
	width=&#34;1121&#34;
	height=&#34;609&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946258919_hu2222420591439563545.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946258919_hu11358273111127950303.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946258919&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;184&#34;
		data-flex-basis=&#34;441px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946290972.png&#34;
	width=&#34;1717&#34;
	height=&#34;1197&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946290972_hu12577265251290657369.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946290972_hu6465901452069974406.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946290972&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;344px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;问题定义&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946620060.png&#34;
	width=&#34;1741&#34;
	height=&#34;1071&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946620060_hu14806983785370900191.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946620060_hu9789760354206344488.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946620060&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946709801.png&#34;
	width=&#34;1605&#34;
	height=&#34;1001&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946709801_hu14181402794970104860.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946709801_hu10926658505811714054.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946709801&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946746917.png&#34;
	width=&#34;1601&#34;
	height=&#34;1029&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946746917_hu14458908010627886301.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946746917_hu12137286081988276040.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946746917&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;373px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;基本方法原理&#34;&gt;基本方法、原理
&lt;/h3&gt;&lt;p&gt;子任务等：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946917658.png&#34;
	width=&#34;1655&#34;
	height=&#34;1089&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946917658_hu17729044232348888486.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704946917658_hu6695596692297444939.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946917658&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;364px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity/Target&lt;/strong&gt;：评论的对象或者物品是什么，例如某个餐厅，某款手机。&amp;ldquo;Target&amp;quot;这个词用的比较模糊，其既可以被当作Entity，又可以当作Aspect Term。和在AE里提到的opinion target是一个意思。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Aspect&lt;/strong&gt;：隶属于某个Entity的属性。在这里其因为学者提出的任务类型不同，又分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Aspect Term&lt;/strong&gt;：存在在句子中的Aspect。例如例句中的”拍照“、”电池“、”外观“。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aspect Category&lt;/strong&gt;：预先给定的Aspect。例如，我们想知道评论对”华为手机“的”外观“、”售后服务“、”便携性“三个aspect的情感极性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;lstm&#34;&gt;LSTM
&lt;/h4&gt;&lt;p&gt;LSTM 方法先将所有变长的句子均表示为一种固定长度的向量，具体做法是将最后一个word对应的计算得到的 hidden vector 作为整句话的表示（sentence vector）。之后，将最后得到的这个 sentence vector 送入一个 linear layer，使其输出为一个维度为情绪种类个数。最后对 linear layer 得出的结果做 softmax 并依次为依据选出该句（同时也是 target）的情绪分类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968549919.png&#34;
	width=&#34;1903&#34;
	height=&#34;489&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968549919_hu5455216471110153373.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968549919_hu806877054696094480.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704968549919&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;389&#34;
		data-flex-basis=&#34;933px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;td-lstm&#34;&gt;TD-LSTM
&lt;/h4&gt;&lt;p&gt;将输入的句子根据 aspect 分为两部分，两边都朝着 aspect 的方向分别同时把 words 送入两个 LSTM 中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968574703.png&#34;
	width=&#34;1915&#34;
	height=&#34;593&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968574703_hu8833283802558480150.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704968574703_hu11176999820145122604.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704968574703&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;322&#34;
		data-flex-basis=&#34;775px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;tc-lstm&#34;&gt;TC-LSTM
&lt;/h4&gt;&lt;p&gt;与 TD-LSTM 唯一的不同就是在 input 时在每个 word embedding vector 后面拼接上 aspect vector（如果 aspect 中有多个 word，则取平均）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704963182156.png&#34;
	width=&#34;1881&#34;
	height=&#34;757&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704963182156_hu14641494358648621056.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704963182156_hu113868132794345503.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;img&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;248&#34;
		data-flex-basis=&#34;596px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;at-lstm&#34;&gt;AT-LSTM
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970432154.png&#34;
	width=&#34;1589&#34;
	height=&#34;533&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970432154_hu14167089383799661096.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970432154_hu14022688230935692713.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704970432154&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;298&#34;
		data-flex-basis=&#34;715px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对隐藏状态h和aspect的词嵌入后施加attention&lt;/p&gt;
&lt;h4 id=&#34;atae-lstm&#34;&gt;ATAE-LSTM
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970484908.png&#34;
	width=&#34;1559&#34;
	height=&#34;613&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970484908_hu6125976050558981422.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704970484908_hu15514222291754135708.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704970484908&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;610px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在LSTM的输入方面在concat一个aspect的词向量，说明aspect的重要性&lt;/p&gt;
&lt;h4 id=&#34;ian&#34;&gt;IAN
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704971332861.png&#34;
	width=&#34;1535&#34;
	height=&#34;1061&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704971332861_hu14070070489939502786.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704971332861_hu7985691024448634525.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704971332861&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;IAN 模型由两部分组成，两部分分别对 Target 和 Context 进行建模。每一部分都以词嵌入作为输入，再通过 LSTM 获取每个词的隐藏状态，最后取所有隐藏向量的平均值，用它来监督另一部分注意力向量的生成。attention学习隐藏状态和对应词向量序列的相关性。&lt;/p&gt;
&lt;p&gt;attention部分是$h_t^i$&amp;amp;$avg(h_c)$在target上做注意力，$h_c^i$&amp;amp;$avg(h_t)$在context上做注意力&lt;/p&gt;
&lt;h2 id=&#34;实体和关系联合抽取&#34;&gt;实体和关系联合抽取
&lt;/h2&gt;&lt;p&gt;信息抽取：从自然语言文本中抽取指定类型的实体、 关系、 事件等事实信息，并形成结构化数据输出的文本处理技术。一般情况下信息抽取别是知识抽取等其他任务的基础。主要在对无结构数据的抽取出现问题&lt;/p&gt;
&lt;h3 id=&#34;基本方法原理-1&#34;&gt;基本方法原理
&lt;/h3&gt;&lt;h4 id=&#34;名词解释&#34;&gt;名词解释
&lt;/h4&gt;&lt;p&gt;span：指的是文本中的一段连续的子串，这段子串对应于某个&lt;em&gt;&lt;strong&gt;实体&lt;/strong&gt;&lt;/em&gt;或者&lt;em&gt;&lt;strong&gt;关系&lt;/strong&gt;&lt;/em&gt;的具体文本表述。&lt;/p&gt;
&lt;h4 id=&#34;dygie&#34;&gt;DyGIE
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704981462026.png&#34;
	width=&#34;1761&#34;
	height=&#34;1223&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704981462026_hu16291791765519612408.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704981462026_hu4093291640552130305.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704981462026&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;345px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;问题定义：&lt;/p&gt;
&lt;p&gt;输入：所有句中可能的spans序列集合。&lt;/p&gt;
&lt;p&gt;输出三种信息：实体类型，关系分类（同一句），指代链接（跨句）；&lt;/p&gt;
&lt;p&gt;Token Representation Layer（Token表示层）：BiLSTM&lt;/p&gt;
&lt;p&gt;Span Representation Layer（span表示层）： 初始化来自BiLSTM输出联合起来，加入基于注意力模型。&lt;/p&gt;
&lt;p&gt;Coreference Propagation Layer（指代传播层）：N次传播处理，跨span共享上下文信息&lt;/p&gt;
&lt;p&gt;Relation Propagation Layer（关系传播层）：与指代传播层相似&lt;/p&gt;
&lt;p&gt;Final Prediction Layer（最终预测层）：去预测任务—实体任务，关系任务&lt;/p&gt;
&lt;h4 id=&#34;oneie&#34;&gt;OneIE
&lt;/h4&gt;&lt;p&gt;任务定义：给定一个输入的句子，输出一个图，图中节点(含节点类型)代表实体提及或者触发词，图中的边表示表示节点之间的关系&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982012782.png&#34;
	width=&#34;1903&#34;
	height=&#34;1313&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982012782_hu5235937576886163447.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982012782_hu4909130293108348477.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982012782&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982419293.png&#34;
	width=&#34;1873&#34;
	height=&#34;733&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982419293_hu17728031895853845399.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982419293_hu13549578310736543633.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982419293&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;613px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;条件随机场（Conditional Random Field，CRF）是一种在自然语言处理（NLP）中广泛使用的模型。CRF的主要作用是解决序列数据的标注问题，它能够考虑整个序列的上下文信息，以做出更准确的预测。&lt;/p&gt;
&lt;p&gt;Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。Beam Search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。&lt;/p&gt;
&lt;p&gt;在这里只保留最好的&lt;/p&gt;
&lt;h4 id=&#34;uie&#34;&gt;UIE
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982684822.png&#34;
	width=&#34;1471&#34;
	height=&#34;1295&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982684822_hu9458506524386010053.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982684822_hu18365285391092397666.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982684822&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;113&#34;
		data-flex-basis=&#34;272px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982702205.png&#34;
	width=&#34;1847&#34;
	height=&#34;1305&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982702205_hu2297697458979006195.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982702205_hu2391283701938374471.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982702205&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;uniex&#34;&gt;UniEX
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982730929.png&#34;
	width=&#34;1771&#34;
	height=&#34;1089&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982730929_hu13948239325332942244.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982730929_hu11513530179906298511.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982730929&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;390px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982749870.png&#34;
	width=&#34;1877&#34;
	height=&#34;1129&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982749870_hu13646308732225976319.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1704982749870_hu10501103273093165471.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982749870&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;399px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;检索式问答系统&#34;&gt;检索式问答系统
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210507003.png&#34;
	width=&#34;1619&#34;
	height=&#34;1114&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210507003_hu5728466859855221245.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210507003_hu14015467137243180911.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210507003&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;1、问题分析模块：问题分类和关键词提取&lt;/p&gt;
&lt;p&gt;问题分类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210654986.png&#34;
	width=&#34;1467&#34;
	height=&#34;755&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210654986_hu13617186629204057358.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210654986_hu4522445902139057376.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210654986&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;466px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;关键词提取：根据问题分类，用&lt;strong&gt;序列标注法&lt;/strong&gt;抽取相应类别的&lt;strong&gt;实体&lt;/strong&gt;做为检索关键词&lt;/p&gt;
&lt;p&gt;2、检索模块：检索问题答案所在文档与段落&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210788393.png&#34;
	width=&#34;1750&#34;
	height=&#34;819&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210788393_hu826958126325044927.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210788393_hu3866198452579352400.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210788393&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;213&#34;
		data-flex-basis=&#34;512px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;3、 答案抽取模块：在相关片段中抽取备选答案，并对备选答案进行排序&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210839822.png&#34;
	width=&#34;1441&#34;
	height=&#34;793&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210839822_hu863186486261353328.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210839822_hu14699342959208430024.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210839822&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;436px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;实现方法：&lt;/p&gt;
&lt;h3 id=&#34;流水线方式&#34;&gt;流水线方式
&lt;/h3&gt;&lt;p&gt;Document Retriever + Reading Comprehension Reader框架&lt;/p&gt;
&lt;h4 id=&#34;drqa&#34;&gt;DrQA
&lt;/h4&gt;&lt;p&gt;TF-IDF：（Term Frequency-Inverse Document Frequency，词频-逆文件频率）是一种用于信息检索和数据挖掘的常用加权技术。它是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210960856.png&#34;
	width=&#34;1655&#34;
	height=&#34;648&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210960856_hu17213876916534315208.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210960856_hu17284829679722648237.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210960856&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;612px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用TF-IDF获取与问题topK相关的文档&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210980785.png&#34;
	width=&#34;1496&#34;
	height=&#34;1016&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210980785_hu7708197675707807797.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705210980785_hu8505320205850320887.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210980785&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然后将对topK使用抽取式阅读理解，从原文中抽取出可以回答的文本&lt;/p&gt;
&lt;h4 id=&#34;evidence-aggregation-for-answer-re-ranking-in-open-domain-question-answering&#34;&gt;Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering
&lt;/h4&gt;&lt;p&gt;有些问题需要来自不同来源的证据相结合才能正确回答。解决方法：strength-based re-ranker&amp;amp;coverage-based re-ranker&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235444754.png&#34;
	width=&#34;1872&#34;
	height=&#34;868&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235444754_hu4957348848114218103.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235444754_hu8252926922647068916.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235444754&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;517px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;strength-based re-ranker的基本思想是，正确的答案通常会被更多的段落反复提及&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235457636.png&#34;
	width=&#34;1883&#34;
	height=&#34;886&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235457636_hu11473308949179497263.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235457636_hu14652746093115460702.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235457636&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;510px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;coverage-based re-ranker考虑每个答案在覆盖不同证据方面的能力，这里用一个BiLSTM来计算答案支撑片段的相似表征【指一个答案和它的支撑片段在表征空间中的相似度】，在垮文本上的相似表征很很高说明这个答案更可靠&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235473330.png&#34;
	width=&#34;1817&#34;
	height=&#34;720&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235473330_hu4130355319735489963.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235473330_hu11541485704517458918.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235473330&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;605px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235487967.png&#34;
	width=&#34;1730&#34;
	height=&#34;632&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235487967_hu9818936321527014481.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235487967_hu5798320088600986527.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235487967&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;656px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;端到端方式&#34;&gt;端到端方式
&lt;/h3&gt;&lt;h4 id=&#34;retriever-reader的联合学习&#34;&gt;Retriever-Reader的联合学习
&lt;/h4&gt;&lt;h5 id=&#34;orqa-open-retriever-question-answering&#34;&gt;ORQA: Open-Retriever Question Answering
&lt;/h5&gt;&lt;p&gt;问题引入：&lt;/p&gt;
&lt;p&gt;1）需要具有强监督的支持证据：监督数据难以获得&lt;/p&gt;
&lt;p&gt;2）利用IR（信息检索）系统检索候选证据：QA与IR存在一定差异性，IR更关注词法或语义相似性，QA对于语言理解层次更丰富&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235654530.png&#34;
	width=&#34;1749&#34;
	height=&#34;1145&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235654530_hu10296767104456587737.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235654530_hu14368177086762838328.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235654530&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;366px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;就是一个S是打分函数。评价retrieval和评价reader是两个不同的，$s_{retr}$是评价这个block和问题的相关性的，$S_{read}$是评价块儿里的文本和q的相关性的。这个里面的bert是用来理解retrieval和question的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235732678.png&#34;
	width=&#34;1865&#34;
	height=&#34;1006&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235732678_hu1307206400050938910.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235732678_hu4961983462613438784.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235732678&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;444px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;每个块通过BERT和权重矩阵b生成隐藏向量h，问题通过BERT和权重矩阵q生成隐藏向量h，通过点积判断相关性&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235784735.png&#34;
	width=&#34;1764&#34;
	height=&#34;1138&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235784735_hu3295368086085503393.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705235784735_hu16728383327305885491.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235784735&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;372px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;BERT_R+MLP生成s，给S_read来评分&lt;/p&gt;
&lt;p&gt;有监督训练，需要手标与a有关的s&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705239164034.png&#34;
	width=&#34;1751&#34;
	height=&#34;1096&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705239164034_hu6946245623843056269.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705239164034_hu6158909432621952815.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705239164034&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;383px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;有挑战，但是懒得管了&lt;/p&gt;
&lt;h4 id=&#34;基于预训练的retriever-free方法&#34;&gt;基于预训练的Retriever-Free方法
&lt;/h4&gt;&lt;p&gt;对预训练模型进行微调，使其能够在没有任何外部上下文或知识的情况下回答问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705240830964.png&#34;
	width=&#34;1582&#34;
	height=&#34;933&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705240830964_hu17582698358078323255.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705240830964_hu15553741384045805649.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705240830964&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;169&#34;
		data-flex-basis=&#34;406px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用span corruption来预训练&lt;/p&gt;
&lt;p&gt;Span Corruption是T5模型预训练任务中的一种方法。它将完整的句子根据随机的span进行掩码。例如，原句：“Thank you for inviting me to your party last week”，Span Corruption之后可能得到输入：“Thank you [X] me to your party [Y] week”，目标：“[X] for inviting [Y] last [Z]”。其中[X]等一系列辅助编码称为sentinels。&lt;/p&gt;
&lt;p&gt;这种方法的目标是让模型学习如何从被打乱或被掩码的句子中恢复出原始的句子。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705234885580.png&#34;
	width=&#34;1598&#34;
	height=&#34;737&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705234885580_hu4110775518237848858.png 480w, https://blog2.pillar.fun/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/1705234885580_hu7362483006742806818.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1705234885580&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;LLM在问答任务上与有监督微调效果不相上下&lt;/p&gt;
&lt;p&gt;LLM在计数、多跳推理、日期、因果等类型上的性能较弱&lt;/p&gt;
&lt;h1 id=&#34;最后一节课&#34;&gt;最后一节课
&lt;/h1&gt;&lt;p&gt;讲了一节课的对话系统（不考）&lt;/p&gt;
&lt;p&gt;参考：https://blog.csdn.net/ld326/article/details/112802292&lt;/p&gt;
</description>
        </item>
        <item>
        <title>网络认证技术作业三</title>
        <link>https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/</link>
        <pubDate>Wed, 29 Nov 2023 19:56:28 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255501352.png" alt="Featured image of post 网络认证技术作业三" /&gt;&lt;p&gt;为了避免跨平台的问题，直接用choco在windows上安装openssl 3.1.1&lt;/p&gt;
&lt;p&gt;&lt;code&gt;choco install openssl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;首先生成私钥&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openssl genrsa -aes256 -out private.pem &lt;span style=&#34;color:#ae81ff&#34;&gt;4096&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;genrsa&lt;/code&gt;是openssl的一个命令，用于生成RSA私钥。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-aes256&lt;/code&gt;表示在输出私钥之前，使用AES 256加密&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-out private.pem&lt;/code&gt; 表示将生成的私钥输出到名为private.pem的文件中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;4096&lt;/code&gt;表示生成的私钥的位数，即私钥的长度为4096位&lt;/p&gt;
&lt;p&gt;然后使用私钥生成证书&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openssl req -new -x509 -days &lt;span style=&#34;color:#ae81ff&#34;&gt;365&lt;/span&gt; -key .&lt;span style=&#34;color:#ae81ff&#34;&gt;\p&lt;/span&gt;rivate.pem -out cacert.crt -config .&lt;span style=&#34;color:#ae81ff&#34;&gt;\s&lt;/span&gt;mime.cnf -extensions smime
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;req&lt;/code&gt;是openssl的一个命令，用于创建和处理PKCS#10格式的证书请求。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-new&lt;/code&gt;表示创建一个新的证书请求。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-x509&lt;/code&gt;表示生成一个自签名的证书，而不是生成一个证书请求。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-days 365&lt;/code&gt;表示生成的证书的有效期为365天。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-key .\private.pem&lt;/code&gt;表示使用名为private.pem的文件中的私钥来签署证书。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-out cacert.crt&lt;/code&gt;表示将生成的证书输出到名为cacert.crt的文件中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-config .\smime.cnf&lt;/code&gt;表示使用名为smime.cnf的文件作为配置文件。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-extensions smime&lt;/code&gt;表示应该包含配置文件中名为smime的部分中指定的扩展。&lt;/p&gt;
&lt;p&gt;smime的部分为&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;smime&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;basicConstraints &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CA:FALSE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;keyUsage &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nonRepudiation, digitalSignature, keyEncipherment
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;extendedKeyUsage &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; emailProtection
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;subjectKeyIdentifier &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hash
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;authorityKeyIdentifier &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; keyid:always, issuer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;subjectAltName &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; email:copy
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;basicConstraints = CA:FALSE&lt;/code&gt;指定证书不能用作CA（证书颁发机构）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;keyUsage = nonRepudiation, digitalSignature, keyEncipherment&lt;/code&gt;指定证书的公钥可以用于哪些用途。这个证书可以用于非否认（nonRepudiation）、数字签名（digitalSignature）和密钥封装（keyEncipherment）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;extendedKeyUsage = emailProtection&lt;/code&gt;用于电子邮件保护（emailProtection）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;subjectKeyIdentifier = hash&lt;/code&gt;用公钥的hash值唯一地标识证书中的公钥。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;authorityKeyIdentifier = keyid:always, issuer&lt;/code&gt;用于标识签署此证书的CA的公钥。这个扩展通常包含CA公钥的keyid（一个唯一标识符），以及CA的名称（issuer）。&lt;code&gt;keyid:always&lt;/code&gt;表示总是包含keyid，无论是否需要&lt;/p&gt;
&lt;p&gt;&lt;code&gt;subjectAltName = email:copy&lt;/code&gt;用于指定证书的主题可选名称（Subject Alternative Name）。主题可选名称是电子邮件地址，该地址从证书的主题名称字段中复制&lt;/p&gt;
&lt;p&gt;生成的时候国家地区公司啥的都不重要，我直接敲回车按默认了。邮箱写自己的就行了。&lt;/p&gt;
&lt;p&gt;直接用windwos自带的证书查看器查看这个证书。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257323787.png&#34;
	width=&#34;847&#34;
	height=&#34;573&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257323787_hu17165879921753680436.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257323787_hu4806887702423259926.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701257323787&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;354px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;版本v3是指我们使用了x.509第3版本，然后序列号是有证书生成算法生成的，唯一的指定这个证书，像身份证号似的。签名算法和哈希算法是一个声明，颁发者是我们刚才在生成证书时写的。有效期由我们刚才的 &lt;code&gt;-days&lt;/code&gt; 参数指明，使用者和颁发者一样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257667795.png&#34;
	width=&#34;827&#34;
	height=&#34;675&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257667795_hu7533296930055372906.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257667795_hu17530596613425736032.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701257667795&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;122&#34;
		data-flex-basis=&#34;294px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;公钥直接在证书文件里保存。公钥参数0500表示NULL，这是因为RSA的公钥的参数（模数和公开指数）已经在公钥字段中给出，所以不需要在公钥参数字段中再给出，如果是其他的加密算法，可能会包含其他信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257930426.png&#34;
	width=&#34;819&#34;
	height=&#34;491&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257930426_hu2024155453600559689.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701257930426_hu4201524837540034718.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701257930426&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;基本约束&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subject Type=End Entity&lt;/strong&gt; ：这表示该证书是一个终端实体证书，而不是CA（证书颁发机构）证书。也就是说这个证书不能用于签发/创建其他证书。&lt;strong&gt;Path Length Constraint=None&lt;/strong&gt; ：这表示路径长度没有设置，准许其签发多级的数字证书。然而，由于Subject Type=End Entity，这个证书不能用于签发其他证书，所以这个设置在这种情况下没有意义。这是由于我们使用了 &lt;code&gt;basicConstraints = CA:FALSE&lt;/code&gt;的选项。&lt;/p&gt;
&lt;p&gt;下面的其他拓展都在-extension部分说过了，这里就不多赘述了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701253924073.png&#34;
	width=&#34;855&#34;
	height=&#34;417&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701253924073_hu17716889339716606819.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701253924073_hu10499399039886456992.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701253924073&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;205&#34;
		data-flex-basis=&#34;492px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然后安装这个证书，并且选择保存路径为受信任的根证书颁发机构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254826717.png&#34;
	width=&#34;1073&#34;
	height=&#34;571&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254826717_hu10368783417683325174.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254826717_hu5072019229119659573.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254826717&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;187&#34;
		data-flex-basis=&#34;450px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;我使用的客户端是outlook。使用的邮箱服务是qq邮箱。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701253859046.png&#34;
	width=&#34;1211&#34;
	height=&#34;1081&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701253859046_hu10963510743842251483.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701253859046_hu16824046125666459864.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701253859046&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;268px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在outlook里添加我的证书&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254522179.png&#34;
	width=&#34;943&#34;
	height=&#34;403&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254522179_hu1591807652405323081.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254522179_hu14302989938282512054.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254522179&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;561px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254551598.png&#34;
	width=&#34;1187&#34;
	height=&#34;1061&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254551598_hu15348383641582059596.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254551598_hu9657815826302673246.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254551598&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;111&#34;
		data-flex-basis=&#34;268px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254581460.png&#34;
	width=&#34;1705&#34;
	height=&#34;1009&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254581460_hu6951950976345695763.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254581460_hu12745949766548365465.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254581460&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;405px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254667232.png&#34;
	width=&#34;879&#34;
	height=&#34;655&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254667232_hu18342330981869491276.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701254667232_hu8299742400613984236.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254667232&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;134&#34;
		data-flex-basis=&#34;322px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这里outlook只支持导入pfx，所以我们需要把生成的证书格式转换&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;openssl pkcs12 -export -out cacert.pfx -inkey .\private.pem -in .\cacert.crt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来是导入助教的证书，首先在outlook里新建一个联系人&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255095693.png&#34;
	width=&#34;1547&#34;
	height=&#34;659&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255095693_hu6224654897561945954.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255095693_hu17036281567946463483.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701255095693&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;234&#34;
		data-flex-basis=&#34;563px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;导入，这里又只支持.cer了，我的windows下的openssl好像缺了库没法转，所以用wsl里的openssl转了一下&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;openssl pkcs12 -in limengjie22\@mails.ucas.ac.cn.pfx -nokeys -out output.cer
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;openssl pkcs12 -nokeys&lt;/code&gt;命令用于从PKCS#12文件（通常具有.pfx或.p12扩展名）中提取证书，-nokeys指定不包含私钥，这样生成的output.cer不能做任何需要私钥的操作（我们也没有要用私钥的操作）&lt;/p&gt;
&lt;p&gt;import password即使提供的私钥.txt的内容。然后就可以成功导入了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255140765.png&#34;
	width=&#34;853&#34;
	height=&#34;459&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255140765_hu11289906702990029714.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255140765_hu633056975698361235.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701255140765&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;446px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然后在发送邮件的时候，在选项里把加密和签署都点了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255501352.png&#34;
	width=&#34;1159&#34;
	height=&#34;155&#34;
	srcset=&#34;https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255501352_hu11940644482753663051.png 480w, https://blog2.pillar.fun/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255501352_hu13369538901111674843.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1701255501352&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;747&#34;
		data-flex-basis=&#34;1794px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>IKEv2标准阅读</title>
        <link>https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/</link>
        <pubDate>Thu, 19 Oct 2023 16:31:55 +0800</pubDate>
        
        <guid>https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/</guid>
        <description>&lt;img src="https://blog2.pillar.fun/img/placeholder.jpeg" alt="Featured image of post IKEv2标准阅读" /&gt;&lt;h1 id=&#34;导语&#34;&gt;导语
&lt;/h1&gt;&lt;p&gt;ucas 2023秋 网络认证技术&lt;/p&gt;
&lt;p&gt;作业2：任选一个标准（口令鉴别协议），书写阅读报告。报告内容要求描述基本原理，解决了什么问题，可能存在什么问题。&lt;/p&gt;
&lt;h1 id=&#34;概述&#34;&gt;概述
&lt;/h1&gt;&lt;p&gt;我选择阅读&lt;a class=&#34;link&#34; href=&#34;https://www.rfc-editor.org/rfc/rfc7296.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RFC 7296&lt;/a&gt;，该标准是互联网密钥交换 （IKE） 协议的第二个版本。本标准使RFC 5996废弃， IKEv2是当前的互联网标准。&lt;/p&gt;
&lt;h1 id=&#34;解决了什么问题&#34;&gt;解决了什么问题
&lt;/h1&gt;&lt;p&gt;IKEv2（Internet Key Exchange version 2）是一种用于建立虚拟专用网络（VPN）连接的协议，它解决了许多与安全通信和远程访问有关的问题。包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安全性：IKEv2提供了强大的安全性，通过使用加密算法来保护数据的机密性和完整性。它还允许身份验证，以确保通信双方是合法的，并可以抵御各种网络攻击，如中间人攻击和数据篡改。&lt;/li&gt;
&lt;li&gt;移动性：IKEv2支持移动设备的连接，允许用户从一个网络切换到另一个网络时保持连接的连续性。这对于移动工作人员或在不同网络环境中工作的人员非常有用。&lt;/li&gt;
&lt;li&gt;多平台兼容性：IKEv2是一种通用的VPN协议，支持多种操作系统和设备，包括Windows、macOS、iOS、Android和Linux。这使得它成为广泛使用的VPN协议，能够在不同平台之间建立安全的连接。&lt;/li&gt;
&lt;li&gt;快速重新连接：IKEv2具有快速重新连接的能力，可以在断开连接后快速重新建立连接，而不需要用户手动干预。这对于移动设备或不稳定的网络连接非常有用。&lt;/li&gt;
&lt;li&gt;支持IPv6：随着IPv6的推广，IKEv2也提供了对IPv6的良好支持，使其适用于新一代互联网协议。&lt;/li&gt;
&lt;li&gt;NAT穿透：IKEv2能够穿越网络地址转换（NAT）设备，这使得它在各种网络环境中都能够正常工作，包括家庭网络和企业网络。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在基本原理-1.1节也简述了IKEv2在特定场景下解决了什么问题。&lt;/p&gt;
&lt;h1 id=&#34;基本原理&#34;&gt;基本原理
&lt;/h1&gt;&lt;h2 id=&#34;11-使用场景&#34;&gt;1.1 使用场景
&lt;/h2&gt;&lt;p&gt;IP 安全性 （IPsec） 为 IP 数据报提供机密性、数据完整性、访问控制和数据源身份验证，这些服务是通过维护 IP 数据报的源和接收方之间的共享状态来提供的。以手动方式建立此共享状态不能很好地扩展。IKEv2正是这样一个动态建立此状态的协议。IKE 在双方之间执行相互身份验证，并建立 IKE 安全关联 （SA），该关联包含共享机密信息，可用于高效建立用于封装安全有效负载 （ESP） [ESP] 或身份验证标头 （AH） [AH] 的 SA，以及一组加密算法，供 SA 用于保护其承载的流量。IKE 用于在许多不同的场景中协商 ESP 或 AH SA，每种方案都有自己的特殊要求。&lt;/p&gt;
&lt;h3 id=&#34;111-隧道模式下的安全网关到安全网关&#34;&gt;1.1.1 隧道模式下的安全网关到安全网关
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697641620088.png&#34;
	width=&#34;1300&#34;
	height=&#34;339&#34;
	srcset=&#34;https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697641620088_hu17835943804313113804.png 480w, https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697641620088_hu13568277230924474737.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1697641620088&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;383&#34;
		data-flex-basis=&#34;920px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在此方案中，IP 连接的两个endpoint都不实现 IPsec，但它们之间的网络节点会保护部分方式的流量。保护对endpoint是透明的，并且依赖于普通路由通过隧道终结点发送数据包进行处理。每个endpoint将宣布其后subnet的地址集，数据包将以隧道模式发送，其中内部 IP 标头将包含实际端点的 IP 地址。&lt;/p&gt;
&lt;h3 id=&#34;112-端点到端点传输模式&#34;&gt;1.1.2 端点到端点传输模式
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697642019874.png&#34;
	width=&#34;1306&#34;
	height=&#34;327&#34;
	srcset=&#34;https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697642019874_hu13499073414692899883.png 480w, https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697642019874_hu960953422566784343.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1697642019874&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;399&#34;
		data-flex-basis=&#34;958px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在此方案中，IP 连接的两个终结点都实现 IPsec，这是 [IPSECARCH] 中主机的要求。该模式通常使用没有内部 IP 标头。将协商一对地址，以便此 SA 保护数据包。这些endpoint可以根据参与者的 IPsec 身份验证身份实现应用层访问控制。此方案实现了端到端安全性。虽然此场景可能不完全适用于 IPv4 公网，但已在使用 IKEv1 的内网内的特定场景中成功部署。在向 IPv6 过渡期间和采用 IKEv2 期间，应该更广泛地启用它。&lt;/p&gt;
&lt;p&gt;在这种情况下，一个或两个受保护的端点可能位于网络地址转换 （NAT） 节点后面，在这种情况下，必须对隧道数据包进行 UDP 封装，以便 UDP 标头中的端口号可用于标识 NAT “后面”的各个endpoint。&lt;/p&gt;
&lt;h3 id=&#34;113隧道模式下的端点到安全网关&#34;&gt;1.1.3隧道模式下的端点到安全网关
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697642991491.png&#34;
	width=&#34;1309&#34;
	height=&#34;315&#34;
	srcset=&#34;https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697642991491_hu17265859170288725566.png 480w, https://blog2.pillar.fun/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/IKEv2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/1697642991491_hu2294255082020515239.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1697642991491&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;415&#34;
		data-flex-basis=&#34;997px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在此方案中，受保护的endpoint（通常是便携式计算机）通过受 IPsec保护的隧道连接回其企业网络。它可能仅使用此隧道访问公司网络上的信息，或者可能通过公司网络将其所有流量通过隧道传输回，以便利用公司防火墙提供的针对基于 Internet 的攻击的保护。在任一情况下，受保护端点都需要一个与安全网关关联的 IP 地址，以便返回到该网关的数据包将转到安全网关并用隧道传回。此 IP 地址可以是静态的，也可以由安全网关动态分配。为了支持后一种情况，IKEv2 包括一种机制（即配置有效负载），发起方请求安全网关拥有的 IP 地址，以便在其 SA 期间使用。&lt;/p&gt;
&lt;p&gt;在这种情况下，数据包将使用隧道模式。在来自受保护endpoint的每个数据包上，外部 IP 标头将包含与其当前位置关联的源 IP 地址（即，将流量直接路由到端点的地址），而内部 IP 标头将包含安全网关分配的源 IP 地址（即，将流量路由到安全网关以转发到端点的地址）。外部目标地址将始终是安全网关的地址，而内部目标地址将是数据包的最终目标。&lt;/p&gt;
&lt;p&gt;在这种情况下，受保护的终结点可能位于 NAT 后面。在这种情况下，安全网关看到的 IP 地址将与受保护端点发送的 IP 地址不同，并且必须对数据包进行 UDP 封装才能正确路由。&lt;/p&gt;
&lt;h2 id=&#34;12初始交换&#34;&gt;1.2初始交换
&lt;/h2&gt;&lt;p&gt;使用 IKE 的通信始终从IKE_SA_INIT和IKE_AUTH交换开始（在 IKEv1 中称为阶段 1）。这些初始交换通常由四条消息组成，但在某些情况下，该数字可能会增长。使用 IKE 的所有通信都由请求/响应对组成。我们将首先描述基础交换，然后是变体。第一对消息 （IKE_SA_INIT） 协商加密算法、交换随机数并进行 Diffie-Hellman 交换 [DH]。&lt;/p&gt;
&lt;p&gt;第二对消息 （IKE_AUTH） 对以前的消息进行身份验证，交换身份和证书，并建立第一个子 SA。这些消息的某些部分使用通过IKE_SA_INIT交换建立的密钥进行加密和完整性保护，因此身份对窃听者隐藏，并且所有消息中的所有字段都经过身份验证。有关如何生成加密密钥的信息，请参阅第 2.14 节。（无法完成IKE_AUTH交换的中间人攻击者仍可以看到发起者的身份。&lt;/p&gt;
&lt;p&gt;初始交换后的所有消息都使用在IKE_SA_INIT交换中协商的加密算法和密钥进行加密保护。这些后续消息使用第 3.14 节中描述的加密有效负载的语法，使用第 2.14 节中所述派生的密钥进行加密。所有后续消息都包含加密有效负载，即使它们在文本中称为“空”。对于CREATE_CHILD_SA、IKE_AUTH或信息交换，标头后面的消息是加密的，包含标头的消息是使用为 IKE SA 协商的加密算法进行完整性保护的。&lt;/p&gt;
&lt;p&gt;每个 IKE 消息都包含一个消息 ID 作为其固定标头的一部分。此消息 ID 用于匹配请求和响应，并标识消息的重新传输。&lt;/p&gt;
&lt;p&gt;一些简称如下&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
Notation        Payload 

----------------------------------------

AUTH            Authentication 

CERT            Certificate 

CERTREQ         Certificate Request 

CP              Configuration 

D               Delete 

EAP             Extensible Authentication 

HDR             IKE header (not a payload) 

IDi             Identification - Initiator 

IDr             Identification - Responder 

KE              Key Exchange 

Ni, Nr          Nonce 

N               Notify 

SA              Security Association 

SK              Encrypted and Authenticated

TSi             Traffic Selector - Initiator 

TSr             Traffic Selector - Responder 

V               Vendor ID
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;第 3 节介绍了每个有效负载的内容的详细信息。可能选择显示的有效负载将显示在括号中，例如 [CERTREQ];这表示可以选择包含证书请求有效负载。&lt;/p&gt;
&lt;p&gt;初始交流如下：&lt;/p&gt;
&lt;p&gt;发起方→接收方HDR, SAi1, KEi, Ni。&lt;/p&gt;
&lt;p&gt;HDR 包含安全参数索引 （SPI）、版本号、Exchange 类型、消息 ID 和各种标志。SAi1 有效负载声明发起方为 IKE SA 支持的加密算法。KE 有效负载发送发起方的 Diffie-Hellman 值。Ni是发起者的随机数。&lt;/p&gt;
&lt;p&gt;接收方→发起方HDR, SAr1, KEr, Nr, [CERTREQ]。&lt;/p&gt;
&lt;p&gt;响应方从发起方提供的选择中选择加密套件，并在 SAr1 有效负载中表达该选择，完成与 KEr 有效负载的 Diffie-Hellman 交换，并在 Nr 有效负载中发送其随机数。&lt;/p&gt;
&lt;p&gt;在协商的这一点上，每一方都可以生成一个名为 SKEYSEED 的数量（参见第 2.14 节），该 IKE SA 的所有密钥都从中派生出来。以下消息完全加密和完整性保护，邮件头除外。用于加密和完整性保护的密钥派生自 SKEYSEED，称为SK_e（加密）和SK_a（身份验证，又名完整性保护）;有关密钥派生的详细信息，请参见第 2.13 和 2.14 节。为每个方向计算单独的SK_e和SK_a。除了从 Diffie-Hellman 值派生的用于保护 IKE SA 的密钥SK_e和SK_a之外，还派生了另一个数量SK_d，并用于派生子 SA 的进一步密钥材料。符号 SK { &amp;hellip; } 表示这些有效负载已使用该方向的SK_e和SK_a进行加密和完整性保护。&lt;/p&gt;
&lt;p&gt;发起方→接收方HDR, SK {IDi, [CERT,] [CERTREQ,] [IDr,] AUTH, SAi2, TSi, TSr} 。发起方使用 IDi 有效负载断言其身份，证明与 IDi 对应的密钥的知识，完整性使用 AUTH 有效负载保护第一条消息的内容（请参阅第 2.15 节）。它还可能在 CERT 有效负载中发送其证书，并在 CERTREQ 有效负载中发送其信任锚的列表。如果包含任何 CERT 有效负载，则提供的第一个证书必须包含用于验证 AUTH 字段的公钥。可选的有效负载 IDr 使发起方能够指定要与响应方的哪个身份通信。当运行响应程序的计算机在同一 IP 地址上托管多个标识时，这很有用。如果发起方建议的 IDr 不被响应方接受，则响应方可能会使用其他某个 IDr 来完成交换。如果发起方随后不接受响应方使用的 IDr 与所请求的 IDr 不同的事实，则发起方可以在注意到这一事实后关闭 SA。发起方使用 SAi2 有效负载开始协商子 SA。最终字段（以 SAi2 开头）在CREATE_CHILD_SA交换的描述中描述。&lt;/p&gt;
&lt;p&gt;接收方→发起方HDR, SK {IDr, [CERT,] AUTH, SAr2, TSi, TSr}。响应方使用 IDr 有效负载断言其身份，可以选择发送一个或多个证书（再次使用包含用于验证 AUTH 的公钥的证书首先列出），使用 AUTH 有效负载验证其身份并保护第二条消息的完整性，并使用下面在CREATE_CHILD_SA交换中描述的其他字段完成子 SA 的协商。IKE_AUTH交换双方必须验证所有签名和消息身份验证代码 （MAC） 是否正确计算。如果任何一方使用共享密钥进行身份验证，则 ID 有效负载中的名称必须与用于生成 AUTH 有效负载的密钥相对应。由于发起方在IKE_SA_INIT中发送其 Diffie-Hellman 值，因此它必须猜测响应方将从其支持的组列表中选择的 Diffie-Hellman 组。如果发起方猜错了，响应方将使用类型 INVALID_KE_PAYLOAD 的通知有效负载进行响应，指示所选组。在这种情况下，发起方必须使用更正的 Diffie-Hellman 组重试IKE_SA_INIT。发起方必须再次提出其完整的可接受加密套件集，因为拒绝消息未经身份验证，否则主动攻击者可以诱使端点协商弱的套件。&lt;/p&gt;
&lt;p&gt;如果在IKE_AUTH交换期间创建子 SA 由于某种原因而失败，IKE SA 仍会照常创建。IKE_AUTH交换中不阻止设置 IKE SA 的通知消息类型列表至少包括以下内容：NO_PROPOSAL_CHOSEN、TS_UNACCEPTABLE、SINGLE_PAIR_REQUIRED、INTERNAL_ADDRESS_FAILURE和FAILED_CP_REQUIRED。&lt;/p&gt;
&lt;p&gt;如果失败与创建 IKE SA 有关（例如，返回AUTHENTICATION_FAILED通知错误消息），则不会创建 IKE SA。请注意，尽管IKE_AUTH消息已加密且完整性受到保护，但如果收到此通知错误消息的对等方尚未对另一端进行身份验证（或者如果对等方由于某种原因未能对另一端进行身份验证），则需要谨慎对待这些信息。更准确地说，假设MAC正确验证，则已知错误通知消息的发送方是IKE_SA_INIT交换的响应者，但无法保证发送方的身份。&lt;/p&gt;
&lt;p&gt;请注意，IKE_AUTH消息不包含 KEi/KEr 或 Ni/Nr 有效负载。因此，IKE_AUTH交换中的 SA 有效负载不能包含具有除 NONE 以外的任何值的转换类型 4（Diffie-Hellman 组）。实现应该省略整个转换子结构，而不是发送值 NONE。&lt;/p&gt;
&lt;h2 id=&#34;13-create_child_sa交换&#34;&gt;1.3 CREATE_CHILD_SA交换
&lt;/h2&gt;&lt;p&gt;CREATE_CHILD_SA交换用于创建新的子 SA，并重新生成 IKE SA 和子 SA 的密钥。此交换由单个请求/响应对组成，其某些功能在 IKEv1 中称为第 2 阶段交换。在初始交换完成后，它可以由IKE SA的任何一端发起。&lt;/p&gt;
&lt;p&gt;通过创建新 SA，然后删除旧 SA 来重新生成 SA 的密钥。本节介绍重新生成密钥的第一部分，即创建新 SA;第 2.8 节介绍了重新生成密钥的机制，包括将流量从旧 SA 移动到新 SA 以及删除旧 SA。必须一起阅读这两个部分才能理解重新生成密钥的整个过程。&lt;/p&gt;
&lt;p&gt;任一端点都可能发起CREATE_CHILD_SA交换，因此在本节中，术语发起方是指发起此交换的端点。实现可以拒绝 IKE SA 中的所有CREATE_CHILD_SA请求。&lt;/p&gt;
&lt;p&gt;CREATE_CHILD_SA请求可以选择包含用于额外 Diffie-Hellman 交换的 KE 有效负载，以便为子 SA 提供更强有力的前向保密保证。子 SA 的键控材料是 IKE SA 建立期间建立的SK_d、CREATE_CHILD_SA交换期间交换的随机数和 Diffie-Hellman 值（如果 KE 有效载荷包含在CREATE_CHILD_SA交换中）的函数。&lt;/p&gt;
&lt;p&gt;如果CREATE_CHILD_SA交换包含 KEi 有效载荷，则至少有一个 SA 报价必须包括 KEi 的 Diffie-Hellman 组。KEi的Diffie-Hellman组必须是发起者期望响应者接受的组的一个元素（可以提出其他Diffie-Hellman组）。如果响应方使用不同的 Diffie-Hellman 组（NONE 除外）选择提案，则响应方必须拒绝该请求，并在INVALID_KE_PAYLOAD Notify 有效负载中指示其首选的 Diffie-Hellman 组。有两个八位字节的数据与此通知相关联：接受的 Diffie-Hellman 组号，按大端序排列。在此类拒绝的情况下，CREATE_CHILD_SA交换失败，发起方可能会在响应者在INVALID_KE_PAYLOAD通知有效负载中给出的组中使用 Diffie-Hellman 提案和 KEi 重试交换。&lt;/p&gt;
&lt;p&gt;响应方发送NO_ADDITIONAL_SAS通知，以指示CREATE_CHILD_SA请求不可接受，因为响应方不愿意在此 IKE SA 上接受更多的子 SA。此通知还可用于拒绝 IKE SA 重新生成密钥。一些最小实现可能只接受初始 IKE 交换上下文中的单个子 SA 设置，并拒绝任何后续添加更多设置的尝试。&lt;/p&gt;
&lt;h3 id=&#34;131-通过create_child_sa交换创建新的子-sa&#34;&gt;1.3.1 通过CREATE_CHILD_SA交换创建新的子 SA
&lt;/h3&gt;&lt;p&gt;可以通过发送CREATE_CHILD_SA请求来创建子 SA。创建新子 SA 的CREATE_CHILD_SA请求是：&lt;/p&gt;
&lt;p&gt;发起方→接收方 HDR, SK {SA, Ni, [KEi,] TSi, TSr}。&lt;/p&gt;
&lt;p&gt;发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {SA, Nr, [KEr,]TSi, TSr}&lt;/p&gt;
&lt;p&gt;如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。&lt;/p&gt;
&lt;p&gt;要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。&lt;/p&gt;
&lt;p&gt;USE_TRANSPORT_MODE通知可以包含在请求消息中，该消息还包括请求子 SA 的 SA 有效负载。它要求子 SA 对创建的 SA 使用传输模式而不是隧道模式。如果请求被接受，则响应还必须包含类型 USE_TRANSPORT_MODE 的通知。如果响应方拒绝请求，子 SA 将在隧道模式下建立。如果发起方无法接受，则发起方必须删除 SA。注意：除非使用此选项协商传输模式，否则所有子 SA 都将使用隧道模式。&lt;/p&gt;
&lt;p&gt;ESP_TFC_PADDING_NOT_SUPPORTED通知断言发送终结点将不接受在正在协商的子 SA 上填充包含流量流机密性 （TFC） 填充的数据包。如果两个终结点都不接受 TFC 填充，则此通知将包含在请求和响应中。如果此通知仅包含在其中一条消息中，则仍可以在另一个方向发送 TFC 填充。&lt;/p&gt;
&lt;p&gt;NON_FIRST_FRAGMENTS_ALSO通知用于碎片控制。有关更全面的解释，请参见 [IPSECARCH]。双方需要同意在任何一方发送非第一个片段之前发送。仅当建议 SA 的请求和接受 SA 的响应中都包含通知NON_FIRST_FRAGMENTS_ALSO才会启用它。如果响应程序不想发送或接收非第一个片段，则它只会从响应中省略NON_FIRST_FRAGMENTS_ALSO通知，但不会拒绝整个子 SA 创建。&lt;/p&gt;
&lt;p&gt;第 2.22 节中涵盖的IPCOMP_SUPPORTED通知也可以包含在交易所中。&lt;/p&gt;
&lt;p&gt;创建子 SA 的失败尝试不应拆除 IKE SA：没有理由丢失为 IKE SA 所做的工作。有关创建子 SA 失败时可能出现的错误消息列表，请参阅第 2.21 节。&lt;/p&gt;
&lt;h3 id=&#34;132-使用create_child_sa交换机重新生成-ike-sa-的密钥&#34;&gt;1.3.2 使用CREATE_CHILD_SA交换机重新生成 IKE SA 的密钥
&lt;/h3&gt;&lt;p&gt;重新生成 IKE SA 密钥的CREATE_CHILD_SA请求是：&lt;/p&gt;
&lt;p&gt;发起方→接收方HDR, SK {SA, Ni, KEi}&lt;/p&gt;
&lt;p&gt;发起方在 SA 有效负载中发送 SA 产品/服务，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值。必须包括 KEi 有效负载。新的发起方 SPI 在 SA 有效负载的 SPI 字段中提供。一旦对等方收到重新生成 IKE SA 密钥的请求或发送重新生成 IKE SA 的请求，它就不应在正在重新生成密钥的 IKE SA 上发起任何新的CREATE_CHILD_SA交换。&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {SA, Nr, KEr}&lt;/p&gt;
&lt;p&gt;如果所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。新的响应程序 SPI 在 SA 有效负载的 SPI 字段中提供。&lt;/p&gt;
&lt;p&gt;新的 IKE SA 将其消息计数器设置为 0，无论它们在早期的 IKE SA 中是什么。来自新 IKE SA 上双方的第一个 IKE 请求的消息 ID 为 0。旧的 IKE SA 保留其编号，因此任何进一步的请求（例如，删除 IKE SA）都将具有连续编号。新的 IKE SA 的窗口大小也重置为 1，并且此重新密钥交换中的发起方是新 IKE SA 的新“原始发起方”。&lt;/p&gt;
&lt;h3 id=&#34;133-使用-create_child_sa-交换重新生成子-sa-的密钥&#34;&gt;1.3.3. 使用 CREATE_CHILD_SA 交换重新生成子 SA 的密钥
&lt;/h3&gt;&lt;p&gt;重新生成子 SA 密钥CREATE_CHILD_SA请求是：&lt;/p&gt;
&lt;p&gt;发起方→接收方 HDR, SK {N(REKEY_SA), SA, Ni, [KEi,] TSi, TSr}&lt;/p&gt;
&lt;p&gt;发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。第 1.3.1 节中描述的通知也可以在重新生成密钥交换中发送。通常，这些通知与原始交换中使用的通知相同;例如，重新生成传输模式 SA 的密钥时，将使用USE_TRANSPORT_MODE通知。如果交换的目的是替换现有的 ESP 或 AH SA，则必须将REKEY_SA通知包含在CREATE_CHILD_SA交换中。正在重新生成密钥的 SA 由通知有效负载中的 SPI 字段标识;这是交换发起方在入站 ESP 或 AH 数据包中期望的 SPI。没有与此通知消息类型关联的数据。REKEY_SA通知的协议 ID 字段设置为与我们要重新生成密钥的 SA 的协议匹配，例如，3 表示 ESP，2 表示 AH。&lt;/p&gt;
&lt;p&gt;重新生成子 SA 密钥CREATE_CHILD_SA响应为：&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {SA, Nr, [KEr,] TSi, TSr}&lt;/p&gt;
&lt;p&gt;如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受报价、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。&lt;/p&gt;
&lt;p&gt;要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。&lt;/p&gt;
&lt;h2 id=&#34;14-信息交换&#34;&gt;1.4. 信息交换
&lt;/h2&gt;&lt;p&gt;在 IKE SA 运行过程中的不同点，对等方可能希望相互传达有关某些事件的错误或通知的控制消息。为了实现这一点，IKE 定义了一个信息交换。信息交换必须仅在初始交换之后进行，并使用协商密钥进行加密保护。请注意，某些信息性消息（而非交换）可以在 IKE SA 的上下文之外发送。第 2.21 节还详细介绍了错误消息。&lt;/p&gt;
&lt;p&gt;与 IKE SA 相关的控制消息必须在该 IKE SA 下发送。与子 SA 相关的控制消息必须在生成它们的 IKE SA（如果 IKE SA 已重新生成密钥，则为其后续消息）的保护下发送。&lt;/p&gt;
&lt;p&gt;信息交换中的消息包含零个或多个通知、删除和配置有效负载。信息交换请求的接收者必须发送一些响应;否则，发送方将假定消息在网络中丢失并重新传输。该响应可能是一条空消息。信息交换中的请求消息也可能不包含有效负载。这是终结点可以要求另一个终结点验证其是否处于活动状态的预期方式。&lt;/p&gt;
&lt;p&gt;信息交换定义为：&lt;/p&gt;
&lt;p&gt;发起方→接收方 HDR, SK {[N,] [D,] [CP,] &amp;hellip;}&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {[N,] [D,] [CP,] &amp;hellip;}&lt;/p&gt;
&lt;p&gt;信息交换的处理由其组件有效载荷决定。&lt;/p&gt;
&lt;h3 id=&#34;141-删除具有信息交换的-sa&#34;&gt;1.4.1. 删除具有信息交换的 SA
&lt;/h3&gt;&lt;p&gt;ESP 和 AH SA 始终成对存在，每个方向上有一个 SA。关闭 SA 时，必须关闭（即删除）对的两个成员。每个终结点必须关闭其传入的 SA，并允许另一个终结点关闭每对中的另一个 SA。要删除 SA，将发送具有一个或多个 Delete 有效负载的信息交换，列出要删除的 SA 的 SPI（正如入站数据包标头中预期的那样）。收件人必须关闭指定的 SA。请注意，从不在单个消息中发送 SA 两端的删除有效负载。如果要同时删除多个 SA，则在信息交换中包括每个 SA 对的入站部分的删除有效负载。&lt;/p&gt;
&lt;p&gt;通常，信息交换中的响应将包含向另一个方向的配对 SA 的删除有效负载。有一个例外。如果一组 SA 的两端偶然独立决定关闭它们，则每个 SA 都可能发送 Delete 有效负载，并且这两个请求可能会在网络中交叉。如果节点收到已发出删除请求的 SA 的删除请求，则必须在处理请求时删除传出 SA，在处理响应时删除传入 SA。在这种情况下，响应不得包含已删除 SA 的删除有效负载，因为这会导致重复删除，并且理论上可能会删除错误的 SA。&lt;/p&gt;
&lt;p&gt;与 ESP 和 AH SA 类似，IKE SA 也通过发送信息交换来删除。删除 IKE SA 会隐式关闭根据该 IKE SA 协商的任何剩余子 SA。对删除 IKE SA 的请求的响应是空的信息响应。&lt;/p&gt;
&lt;p&gt;半闭合 ESP 或 AH 连接是异常的，具有审计功能的节点如果它们仍然存在，则可能应该审计它们的存在。请注意，此规范未指定时间段，因此由各个终结点决定等待多长时间。节点可以拒绝接受半闭合连接上的传入数据，但不得单方面关闭它们并重用 SPI。如果连接状态变得足够混乱，节点可能会关闭 IKE SA，如上所述。然后，它可以在新的 IKE SA 下干净的基础上重建所需的 SA。&lt;/p&gt;
&lt;h2 id=&#34;15-ike-sa-之外的信息性消息&#34;&gt;1.5 IKE SA 之外的信息性消息
&lt;/h2&gt;&lt;p&gt;在某些情况下，节点收到无法处理的数据包，但它可能希望将这种情况通知发送方。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 ESP 或 AH 数据包到达时带有无法识别的 SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。&lt;/li&gt;
&lt;li&gt;如果加密的 IKE 请求数据包到达端口 500 或 4500，并且具有无法识别的 IKE SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。&lt;/li&gt;
&lt;li&gt;如果 IKE 请求数据包到达时的主版本号高于实现支持的版本号。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在第一种情况下，如果接收节点有一个活动的 IKE SA 到数据包来自的 IP 地址，它可能会在信息交换中通过该 IKE SA 发送任性数据包的INVALID_SPI通知。通知数据包含无效数据包的 SPI。此通知的接收者无法判断 SPI 是针对 AH 还是 ESP，但这并不重要，因为在许多情况下，两者的 SPI 会有所不同。如果不存在合适的 IKE SA，则节点可能会向源 IP 地址发送没有加密保护的信息性消息，如果数据包是 UDP（UDP 封装的 ESP 或 AH），则使用源 UDP 端口作为目标端口。在这种情况下，它应该只被收件人用作可能出错的提示（因为它很容易被伪造）。此消息不是信息交换的一部分，接收节点不得响应它，因为这样做可能会导致消息循环。消息构造如下：没有对此类通知的接收者有意义的 IKE SPI 值;使用零值或随机值都是可以接受的，这是第 3.1 节中禁止零 IKE 发起方 SPI 的规则的例外。发起方标志设置为 1，响应标志设置为 0，版本标志以正常方式设置;这些标志在第 3.1 节中描述。&lt;/p&gt;
&lt;p&gt;在第两种和第三种情况下，消息始终在没有加密保护的情况下发送（在 IKE SA 外部），并且包括INVALID_IKE_SPI或INVALID_MAJOR_VERSION通知（没有通知数据）。该消息是响应消息，因此它被发送到带有相同 IKE SPI 的 IP 地址和端口，并且消息 ID 和交换类型是从请求中复制的。响应标志设置为 1，版本标志以正常方式设置。&lt;/p&gt;
&lt;h1 id=&#34;可能存在的问题&#34;&gt;可能存在的问题
&lt;/h1&gt;&lt;p&gt;参考发表在27th USENIX Security Symposium (USENIX Security 18), 2018的&lt;a class=&#34;link&#34; href=&#34;https://usenix.org/conference/usenixsecurity18/presentation/felsch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Dangers of Key Reuse: Practical Attacks on IPsec IKE&lt;/a&gt;可IKEv1、v2如果重用密钥可能导致跨协议身份验证绕过，从而使攻击者能够冒充受害者主机或网络。在IKEv1模式下利用Bleichenbacher预言机，其中RSA加密的随机数用于身份验证。利用此漏洞打破了基于 RSA 加密的模式，此外还破坏了 IKEv1 和 IKEv2 中基于 RSA 签名的身份验证。此外，还存在针对基于 PSK（预共享密钥）的 IKE 模式的离线字典攻击，从而涵盖了 IKE 的所有可用身份验证机制。在思科（CVE-2018-0131）、华为（CVE2017-17305）、Clavister（CVE-2018-8753）和合勤科技（CVE-2018-9129）的IKEv1实现中找到了Bleichenbacher预言机。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
