[{"content":"导语 IEEE S\u0026amp;P 2024, doi is here\nmotivation 简单来说还是对效率问题提出了一个解，针对大的溯源图，开发了一个嵌入回收数据库来存储在训练阶段生成的节点嵌入，检测的时候读嵌入数据库即可，不用重新算嵌入\nQ:为什么训练阶段的节点嵌入在检测时能用到，覆盖率怎么搞？\nIntroduction 现有的溯源图IDS的存在局限\n通常会忽略有价值的语义数据，例如来源图中的进程名称、命令行参数、文件路径和 IP 地址。【Q：但是不是说这些内容容易造成混淆吗？把这些通过word2vec编码成向量不是还是容易造成混淆吗？】（A：意思是不带这些可以提升泛化性，对于没见过的攻击比较管用，但是可能因为学的不够多而有误报，我觉得还是泛化性更重要一点。差评) 一些IDS忽略了系统事件的时间顺序和因果顺序的重要性。 可扩展性问题，尤其是在处理大型来源图时。 粗粒度检测：许多 IDS 识别恶意子图而不是单个恶意节点，这使得警报验证和攻击重建对安全分析师来说既耗时又容易出错。 GNN技术不可扩展，而且速度非常慢 现有方法都是学习良性行为 因此，这篇文章\n采用基于Word2Vec的嵌入技术，将来源图中存在的各种节点属性（如进程名称、命令行参数、文件路径和IP地址）编码为语义丰富的特征向量。 修改了 Word2Vec 技术以获得时间敏感的嵌入，解决了忽略事件之间时间顺序的问题。(咋修改的？) 通过仅选择与威胁识别相关的边来提高图表示学习中图遍历的效率【Q：how？】，设计了一个GNN嵌入数据库，该数据库的灵感来自以前应用于语言模型的嵌入回收技术（2022) 报告危险节点（node)而非危险子图（graph)，能够对adversarial mimicry attacks on provenance-based IDSes【Q：这是啥？】（A：http://dx.doi.org/10.1145/586110.586145)鲁棒。根据检测生成的演变图（AEG)来帮助溯源。说是检测危险边（edge)的消耗太大，node是最好的trade off 这篇文章首次在Darpa OpTC数据集上进行了评估，该数据集是DARPA迄今为止发布的最大的系统日志数据集。这些数据集涵盖了广泛的攻击场景和系统行为。说是比SOTA快三倍\n作者的小结：\n• 我们提出了一种基于来源的IDS，即FLASH，它利用来源图中的上下文和结构信息来增强其检测能力。\n• 我们引入了一个两步过程，分别使用 Word2Vec 和 GNN 生成语义和上下文嵌入。在此过程之后，通过轻量级分类器模型进行实时异常检测，确保系统的可扩展性和效率。\n• 我们提供两种方案——选择性图遍历和嵌入回收数据库——使图表示学习在 IDS 设置中变得实用。\n• 我们在真实世界的数据集上对我们的技术进行全面评估。结果突出了FLASH在识别恶意活动方面的有效性，其对对抗性模拟攻击的弹性，以及加速警报验证过程的能力\nmethodlogy 嵌入回收数据库存储在训练阶段生成的节点嵌入，用轻量级分类器 [结合了GNN和word2vec] 在检测阶段通过数据库省去嵌入计算步骤。\n通过图神经网络（GNN）在数据溯源图上利用图表示学习 使用基于Word2Vec的语义编码器来捕获基本的语义属性和时间顺序 采用了基于GNN的上下文编码器，可以有效地将局部和全局图结构编码为富有表现力的节点嵌入。 ","date":"2024-04-18T10:51:57+08:00","permalink":"http://localhost:1313/p/flash/","title":"FLASH"},{"content":"文章可以在这里获取\nAbstract APT攻击越来越成为普遍的威胁，然而，以前的工作表现出几个缺点：\n需要包含攻击的数据和APT的先验知识 无法提取隐藏在来源图中的丰富上下文信息 由于其令人望而却步的计算开销和内存消耗而变得不切实际。 本文介绍一种自监督APT检测方法MAGIC，能够在不同级别的监督下进行多粒度检测。利用掩码图表示学习对良性系统实体和行为进行建模，对来源图进行高效的深度特征提取和结构抽象。通过异常值检测方法检测异常系统行为，MAGIC 能够执行系统实体级和批处理日志级 APT 检测。\nMAGIC是专门为处理概念漂移而设计的，具有模型适配机制，并成功应用于通用条件和检测场景。我们在三个广泛使用的数据集上评估了MAGIC，包括真实世界和模拟攻击。评估结果表明，MAGIC在所有场景中都取得了令人鼓舞的检测结果，并且在性能开销方面比最先进的APT检测方法具有巨大的优势。\nIntroduction 高级持续性威胁（APTs）是由熟练的攻击者进行的蓄意和复杂的网络攻击，对企业和机构都构成巨大威胁。大多数 APT 都涉及零日漏洞，并且由于其隐蔽性和多变性而特别难以检测。\n最近关于APT检测的工作利用数据来源进行APT检测。数据来源将审计日志转换为来源图，从审计日志中提取丰富的上下文信息，为细粒度的因果关系分析和 APT 检测提供完美的平台。\n早期工作基于典型或特定的 APT 模式构建规则，并将审计日志与这些规则进行匹配，以检测潜在的 APT。\n最近的一些工作采用统计异常检测方法来检测APT，这些APT侧重于不同的来源图元素，例如系统实体、交互和社区。\n最近的研究基于深度学习的方法。他们利用各种深度学习 （DL） 技术对 APT 模式或系统行为进行建模，并以分类或异常检测方式执行 APT 检测。\n虽然这些现有方法已经证明了它们能够以合理的准确性检测 APT，但它们遇到了以下挑战的各种组合：\n监督方法存在缺乏数据 （LOD） 问题，因为它们需要有关 APT 的先验知识（即攻击模式或包含攻击的日志）。此外，当面对他们没有接受过处理培训的新型 APT 时，这些方法特别容易受到攻击。 基于统计的方法只需要良性数据即可发挥作用，但无法提取审计日志中埋藏的复杂良性活动的深层语义和相关性，导致误报率高。 基于深度学习的方法，特别是基于序列和基于图的方法，以沉重的计算开销为代价，取得了可观的有效性，使其在实际检测场景中不切实际 在本文中，我们通过引入MAGIC来解决上述三个问题，MAGIC是一种新颖的自监督APT检测方法，它利用掩码图表示学习和简单的异常值检测方法从海量审计日志中识别关键攻击系统实体。MAGIC首先通过简单而通用的步骤从审计日志中构建出处图。然后，MAGIC使用图表示模块，该模块通过以自我监督的方式合并图特征和结构信息来获得嵌入。该模型建立在图形掩蔽自编码器[19]之上，在掩蔽特征重建和基于样本的结构重建的共同监督下。采用无监督异常值检测方法对计算出的嵌入进行分析，并得到最终的检测结果。\nMAGIC首先通过简单而通用的步骤从审计日志中构建出处图。然后，MAGIC使用图表示模块，该模块通过以自我监督的方式合并图特征和结构信息来获得嵌入。该模型建立在图形掩蔽自编码器之上，在掩蔽特征重建和基于样本的结构重建的共同监督下。采用无监督异常值检测方法对计算出的嵌入进行分析，并得到最终的检测结果。\nMAGIC 旨在灵活且可扩展。根据应用程序背景，MAGIC 能够执行多粒度检测，即检测批处理日志中是否存在 APT 或定位实体级对手。虽然 MAGIC 旨在执行 APT 检测而不会受到攻击包含数据，但它非常适合半监督和完全监督的情况。此外，MAGIC还包含一个可选的模型适配机制，为其用户提供反馈渠道。这样的反馈对于MAGIC进一步提高性能、对抗概念漂移和减少误报非常重要。\n我们在三个不同的 APT 攻击数据集上评估了MAGIC的性能和开销：DARPA Transparent Computing E3 数据集、StreamSpot 数据集和 Unicorn Wget 数据集。DARPA 数据集包含真实世界的攻击，而 StreamSpot 和 Unicorn Wget 数据集则在受控环境中完全模拟。评估结果表明，MAGIC 能够以 97.26% 的准确率和 99.91% 的召回率执行实体级 APT 检测，并且开销最小，对内存的要求更低，并且比最先进的方法快得多\ncontribution总结\n提出了MAGIC，这是一种基于掩码图表示学习和异常值检测方法的通用APT检测方法，能够对海量审计日志进行多粒度检测。 通过扩展的图形掩码自动编码器将计算开销降至最低，从而确保 MAGIC 的实用性，即使在狭小的条件下，也能在可接受的时间内完成训练和检测。 通过各种努力确保MAGIC的普遍性。我们利用掩码图表示学习和异常值检测方法，使 MAGIC 能够在不同的监管级别、不同的检测粒度和来自不同来源的审计日志下进行精确检测。 在三个广泛使用的数据集上评估了 MAGIC，涉及真实世界和模拟的 APT 攻击。评估结果表明，MAGIC检测的APTs具有良好的结果和最小的计算开销。 提供 MAGIC 的开源实现，以造福社区未来的研究，并鼓励进一步改进我们的方法。 Background 攻击场景 在这里，我们提供了我们在整篇文章中使用的 APT 场景的详细说明。带有 Drakon Dropper 的 Pine 后门是来自 DARPA Engagement 3 Trace 数据集的 APT 攻击 [20]。在攻击过程中，攻击者构建恶意可执行文件 （/tmp/tcexec） 并通过网络钓鱼电子邮件将其发送到目标主机。然后，用户会无意识地下载并打开电子邮件。电子邮件中包含的可执行文件旨在执行用于内部侦测的端口扫描，并在目标主机和攻击者之间建立静默连接。\n图 1 显示了我们的动机示例的出处图。图中的节点表示系统实体，箭头表示实体之间的定向交互。显示的图是通过删除大多数与攻击无关的实体和交互，从完整的来源图中抽象出来的子图。不同的节点形状对应不同类型的实体。被条纹覆盖的实体被视为恶意实体。\nPrior Research and their Limitations 监督方法：对于早期作品，需要构建特殊的启发式规则来涵盖所有攻击模式。许多基于深度学习的APT检测方法基于良性和攻击性数据构建来源图，并以分类方式检测APT。这些监督方法可以在学习的攻击模式上获得近乎完美的检测结果，但在面临概念漂移或看不见的攻击模式时尤其容易受到攻击。此外，对于基于规则的方法，启发式规则的构建和维护可能非常昂贵和耗时。对于基于深度学习的方法，包含攻击的数据的稀缺性阻碍了这些监督方法的实际部署。针对上述问题，MAGIC 采用完全自监督的异常检测方式，在有效处理不可见攻击模式的同时，允许不存在包含攻击的数据。 基于统计的方法：最新的基于统计学的方法通过识别系统实体、交互和社区的稀有性或异常分数来检测APT信号。然而，系统实体的稀有性并不一定表明它们的异常，通过因果分析或标签传播获得的异常评分是来源图上的浅层特征提取。为了说明这一点，在我们的攻击示例中，进程 tcexec 对不同的 IP 地址执行多个端口扫描操作（参见图 1），这可以被视为正常的系统行为。但是，考虑到源自外部网络的进程 tcexec 也会读取敏感的系统信息 （uname） 并与公共 IP 地址 （162.66.239.75） 建立连接，我们可以很容易地将 tcexec 识别为恶意实体。由于无法提取系统实体之间的深层语义和相关性，通常会导致基于统计的方法检测性能低下和误报率高。然而，MAGIC采用图表示模块对来源图进行深度图特征提取，从而产生高质量的嵌入。 基于 DL 的方法：最近，基于DL的APT检测方法，无论是有监督还是无监督，都产生了非常有希望的检测结果。然而，在现实中，中型企业每天会产生数百GB的审计日志。因此，基于深度学习的方法，特别是基于序列的和基于图形的方法，由于其计算开销而不可行。例如，ATLAS平均需要 1 小时才能在 676MB 的审计日志上进行训练，而 ShadeWatcher在具有 GPU 的 DARPA E3 Trace 数据集上训练平均需要 1 天。此外，一些基于图自编码器的方法在来源图规模扩大时会遇到爆炸性内存开销问题。MAGIC 通过引入图形掩码自动编码器避免了计算要求高，并在短短几分钟内完成了对 DARPA E3 Trace 数据集的训练。第 6.4 节中详细介绍了 MAGIC 的性能开销。 端到端方法：除了上面讨论的三个主要局限性之外，还值得一提的是，最新的APT检测方法是端到端检测器，并且专注于一个特定的检测任务。例如，ATLAS专注于端到端的攻击重建，而 Unicorn则从流日志中生成系统级警报。相反，MAGIC的方法是通用的，可以在各种检测场景下执行多粒度APT检测，也可以应用于从不同来源收集的审计日志。（什么叫通用的？预训练精调？还是知识说能检测多场景多力度就算通用了？） Threat Model and Definitions 威胁模型：我们假设攻击者来自系统外部，并以系统内的有价值信息为目标。攻击者可能会执行复杂的步骤来实现其目标，但在日志中留下可追踪的证据。系统硬件、操作系统和系统审计软件的组合是我们值得信赖的计算基础。在我们的威胁模型中不考虑毒物攻击和逃避攻击。\n出处图：来源图是从原始审计日志中提取的有向循环图。构建来源图是数据来源的常见做法，因为它连接系统实体并呈现它们之间的交互关系。来源图包含代表不同系统实体（例如，进程、文件和套接字）的节点，以及代表系统实体之间交互（例如，执行和连接）的边缘，并标有它们的类型。例如，/tmp/tcexec 是一个 FileObject 系统实体，而 /tmp/tcexec 和 tcexec 之间的边缘是 FileObject 面向 Process 的执行操作（参见图 1）。\n多粒度检测：MAGIC 能够执行两个粒度的 APT 检测：批处理日志级别和系统实体级别。MAGIC的多粒度检测能力催生了两阶段检测方法：首先对流式日志进行批量日志级检测，然后对正批次进行系统实体级检测，以识别详细的检测结果。将这种方法应用于实际环境将有效减少工作量、资源消耗和误报，同时产生详细的结果。\n批处理日志级别检测。在这种粒度的 APT 检测下，主要任务是给定来自一致来源的批量审核日志，如果在一批日志中检测到潜在的 APT，MAGIC 会发出警报。与Unicorn类似，MAGIC无法在这种检测粒度下准确定位恶意系统实体和交互。 系统实体级检测。在这种粒度的APT检测下，检测任务是给定来自一致来源的审计日志，MAGIC能够在这些审计日志中准确定位恶意系统实体。在APT期间识别关键系统实体对于后续任务（如攻击调查和攻击故事恢复）至关重要，因为它提供了可解释的检测结果，并减少了对领域专家的需求以及冗余的手动工作。 MAGIC是一种新型的自监督APT检测方法，它利用掩蔽图表示学习和异常值检测方法，能够对海量审计日志进行高效的多粒度检测。MAGIC的流水线由三个主要组件组成：（1）来源图构建，（2）图表示模块和（3）检测模块。它还提供了可选的 （4） 模型适配机制。在训练过程中，MAGIC 用 （1） 转换训练数据，用 （2） 学习图嵌入，用 （3） 记住良性行为。在推理过程中，MAGIC 使用 （1） 转换目标数据，使用训练的 （2） 获得图形嵌入，并通过 （3） 检测异常值。图 2 概述了 MAGIC 架构。\n系统审计软件收集的流式审计日志通常以批量方式存储。在来源图构建 （1） 期间，MAGIC 将这些日志转换为静态来源图。系统实体及其之间的交互被提取并分别转换为节点和边。使用几种降低复杂性的技术来删除冗余信息。 然后，将构建的出处图通过图表示模块（2）馈送，以获得输出嵌入（即对象的综合向量表示）。图表示模块基于图屏蔽自动编码器构建，并集成了基于样本的结构重构，将节点和边属性嵌入、传播和聚合到输出嵌入中，这些嵌入包含节点嵌入和系统状态嵌入。图形表示模块仅使用良性审核日志进行训练，以对良性系统行为进行建模。在对可能包含攻击的审计日志执行 APT 检测时，MAGIC 利用基于输出嵌入的异常值检测方法来检测系统行为中的异常值 （3）。根据任务的粒度，使用不同的嵌入来完成 APT 检测。在批处理日志级任务中，反映整个系统一般行为的系统状态嵌入是检测目标。此类嵌入中的异常值意味着其相应的系统状态是看不见的，并且可能是恶意的，这会显示该批次中的 APT 信号。在系统实体级任务中，检测目标是那些节点嵌入，它们表示系统实体的行为。节点嵌入中的异常值表示可疑的系统实体，并以更精细的粒度检测 APT 威胁。\n在实际检测设置中，MAGIC 有两个预先设计的应用程序。对于系统审计软件收集的每批日志，可以直接利用MAGIC的实体级检测来准确识别批次中的恶意实体，也可以按照第2.3节的规定进行两阶段检测。在这种情况下，MAGIC 首先扫描批次并查看批次中是否存在恶意信号（批处理日志级别检测）。如果警报为阳性，则 MAGIC 将执行实体级检测，以更精细的粒度识别恶意系统实体。与实体级检测相比，批量日志级别检测的计算要求要低得多。因此，这样的两阶段例程可以帮助MAGIC的用户节省计算资源，避免误报，同时不影响MAGIC的检测细度。但是，如果用户喜欢对所有系统实体进行细粒度检测，则前一个例程仍然是一个可访问的选项。\n为了应对概念漂移和看不见的攻击（unseen attack），采用了可选的模型适配机制为其用户提供反馈渠道（4）。由安全分析师检查和确认的检测结果将反馈给 MAGIC，帮助其以半监督的方式适应良性系统行为变化。在这种情况下，MAGIC获得了更有希望的检测结果，这将在第6.3节中讨论。此外，MAGIC 可以很容易地应用于现实世界的在线 APT 检测，这要归功于它能够适应概念漂移和最小的计算开销。\n1用的啥？2用的基于图屏蔽自动编码器，3用的啥？KNN? （4）是干啥的？\n（4）是靠人工标注进行监督学习的。\nunseen attack \u0026ldquo;Unseen attack\u0026rdquo;（不可见攻击）是一种高级持续性威胁（Advanced Persistent Threat，APT）攻击中的一种策略。这种攻击手段的目标是使攻击者的活动对受害者尽可能地不可见，让受害者很难察觉到自己受到了攻击。\n这种类型的攻击通常采取了多种隐蔽的方法，旨在规避传统安全监控和检测工具的检测。一些常见的不可见攻击技术包括：\n无文件攻击（Fileless Attacks）：这种攻击方式不会在受害者系统上留下可被传统防病毒软件等检测到的文件。攻击者通过利用系统内置的工具或脚本语言，例如 PowerShell 或 WMI (Windows Management Instrumentation)，在内存中执行恶意代码。 隐蔽通信：攻击者会使用加密或隐藏的通信渠道，以避免被网络监控和入侵检测系统检测到。这可能包括使用加密协议、隐蔽通信端口或者隐藏在合法网络流量中的恶意数据。 低频攻击：攻击者会在较长时间间隔内执行活动，以减少被检测到的风险。这种攻击方式通常不会引起系统管理员的注意，因为攻击活动没有频繁发生。 持续访问：攻击者在成功进入受害者网络后，会尽可能长时间地保持对系统的访问，以获取更多的信息和权限。他们可能会隐藏在系统的深层次，并悄悄地窃取数据或监视受害者的活动。 动态改变攻击模式：攻击者会不断地改变他们的攻击方式和工具，以规避传统的安全防御措施。这种变化性可以使传统的签名检测和规则检测方法失效。 Design Details Provenance Graph Construction MAGIC 首先从原始审计日志中构建出处图，然后再执行图表示和 APT 检测。我们遵循三个步骤来构建一致且优化的来源图，为图表示做好准备。\n日志解析：第一步是简单地解析每个日志条目，提取系统实体以及它们之间的系统交互。然后，可以构建一个原型出处图，以系统实体为节点，以交互为边。现在，我们提取有关节点和边的分类信息。对于提供实体和交互标签的简单日志格式，我们直接使用这些标签。对于提供这些实体和交互的复杂属性的某种格式，我们应用多标签哈希（例如，xxhash）将属性转换为标签。在这个阶段，来源图是有向多图。我们设计了一个示例来演示如何处理图 3 中日志解析后的原始来源图。\n初始嵌入：在这个阶段，我们将节点和边标签转换为维度 d 的固定大小的特征向量（即初始嵌入），其中 d 是图表示模块的隐藏维度。我们应用了查找嵌入，在节点/边标签和 d 维特征向量之间建立了一对一的映射。如图 3（I 和 II）所示，进程 a 和 b 共享相同的标签，因此它们映射到相同的特征向量，而 a 和 c 嵌入到不同的特征向量中，因为它们具有不同的标签。我们注意到，唯一节点/边缘标签的可能数量由数据源（即审计日志格式）决定。因此，查找嵌入在转导设置下工作，不需要学习看不见的标签的嵌入。\n降噪：我们的图表示模块的预期输入出处图将是简单图。因此，我们需要在节点对之间组合多个边。如果一对节点之间存在同一标签的多个边（也共享相同的初始嵌入），我们将删除多余的边，以便只保留其中一条。然后，我们将剩余的边合并为一条最终边。我们注意到，在一对节点之间，可能会保留几个不同标签的边缘。组合后，通过对剩余边的初始嵌入进行平均来获得所得唯一边的初始嵌入。为了说明这一点，我们在图3（II和III）中展示了我们的降噪如何结合多边，以及它如何影响边的初始嵌入。首先，对于每个标签，将 a 和 c 之间的 3 次读取和 2 次写入交互合并为一个。然后我们将它们组合在一起，形成一个边缘 eac，其初始嵌入等于其余边缘的平均初始嵌入（e′ 2 和 e′ 5）。我们在附录E中提供了我们的降噪步骤与以前工作的比较\n简单来说就是先对边和节点进行多标签哈希，目的是将多个类别标签映射到设计的那几个标签上（如果是有标签的日志格式则不需要这一步），这样每个边都有标签，标签就是边的含义（read,write等)。然后将节点和标签转为d维的初始嵌入，再将节点对之间嵌入值相同的边合并成一个边。\nGraph Representation Module MAGIC 采用图表示模块从特色出处图（featured provenance graphs）中获取高质量的嵌入。如图 4 所示，图表示模块由三个阶段组成：用于部分隐藏节点特征（即初始嵌入）以进行重建的掩码过程，通过传播和聚合图特征生成节点和系统状态输出嵌入的图编码器，图解码器通过掩码特征重建和基于样本的结构为图表示模块的训练提供监督信号重建。编码器和解码器形成图形掩码自动编码器，在生成快速且节省资源的嵌入方面表现出色。\nFeature Masking 在训练我们的图表示模块之前，我们对节点执行掩码，以便在重建这些节点时可以训练图掩码自动编码器。屏蔽节点是随机选择的，覆盖所有节点的一定比例。此类屏蔽节点的初始嵌入将替换为特殊的屏蔽令牌 xmask，以涵盖有关这些节点的任何原始信息。但是，边缘不会被屏蔽，因为这些边缘提供了有关系统实体之间关系的宝贵信息。总之，给定节点初始嵌入 $x_n$，我们按如下方式屏蔽节点：\n其中 $\\tilde{N}$ 是随机选择的掩码节点，$emb_n$ 是节点 n 的嵌入，准备训练图表示模块。此掩蔽过程仅在训练期间发生。在检测过程中，我们不会屏蔽任何节点。\nGraph Encoder 从图构建步骤中获得的初始嵌入仅考虑原始特征。然而，原始特征还远远不足以对系统实体的详细行为进行建模。实体的上下文信息，如其邻域、多跳关系以及与其他系统实体的交互模式，对于获得高质量的实体嵌入起着重要作用。在这里，我们采用并扩展了图形屏蔽自编码器，以自监督的方式生成输出嵌入。图形屏蔽自动编码器由编码器和解码器组成。编码器通过传播和聚合图特征来生成输出嵌入，解码器重建图特征以提供用于训练的监督信号。这种编码器-解码器架构在生成的嵌入中保留了上下文和语义信息，同时通过掩蔽学习显着降低了其计算开销。\n我们的图表示模块的编码器包含多个堆叠层的图注意力网络（GAT）。GAT层的功能是根据节点本身及其相邻节点的特征（初始嵌入）生成输出节点嵌入。与普通的GNN不同，GAT引入了一种注意力机制来衡量这些邻居的重要性。【GAT加了attention真的还好算吗？训练代价应该也不小吧】\n为了详细解释，GAT 的一层将前几层生成的节点嵌入作为输入，并将嵌入从源节点传播到目标节点，并沿交互传播到消息中。该消息包含有关源节点以及源节点和目标节点之间交互的信息：\n并采用注意力机制来计算消息源与其目的地之间的注意力系数：\n然后，对于目标节点，GAT 聚合来自传入边缘的消息，以通过计算所有传入消息的加权和来更新其节点嵌入。权重正是注意力系数：\n其中 $h^l_n$ 是 GAT 第 l 层节点 n 的隐藏嵌入，$h^{l-1}_n$ 是 l − 1 层的隐藏嵌入，$\\mathcal{N}n$ 是 n 的单跳邻域。第一个 GAT 层的输入是初始节点嵌入。Embe 是初始边缘嵌入，在整个图形表示模块中保持不变。$W_as$，$W_am$，$W{self}$ ，$W{msg}$ 是可训练的参数。更新的节点嵌入构成了节点单跳交互行为的一般抽象。\n将此类 GAT 的多层堆叠以获得最终的节点嵌入 h，该节点嵌入由原始节点嵌入和所有 GAT 层的输出连接起来：\n其中 ·||·表示串联操作。GAT堆叠的层数越多，相邻范围就越宽，节点的多跳交互模式能够表示的就越远。因此，图编码器有效地结合了节点初始特征和多跳交互行为，将系统实体行为抽象到节点嵌入中。图编码器还对所有节点嵌入应用平均池化，以生成图本身的全面嵌入，它概括了系统的整体状态：\n图编码器生成的节点嵌入和系统状态嵌入被视为图表示模块的输出，用于不同场景下的后续任务。\nGraph Decoder 图形编码器不提供支持模型训练的监督信号。在典型的图自编码器中，使用图解码器对节点嵌入进行解码，并通过特征重构和结构重构来监督模型训练。然而，图形屏蔽自编码器放弃了结构重建，以减少计算开销。我们的图解码器是两者的混合体，它集成了掩码特征重建和基于样本的结构重建，以构建优化图表示模块的目标函数。\n给定从图编码器获得的节点嵌入 $h_n$，解码器首先重新屏蔽这些屏蔽节点，并将它们转换为屏蔽特征重建的输入：\n随后，解码器使用上述类似的GAT层来重建掩码节点的初始嵌入，从而可以计算特征重建损失：\n【所以这个掩码节点是拿个seed全局保留的马？】\n其中 $L_{fr} $是通过计算掩码节点的初始嵌入和重建嵌入之间的缩放余弦损失获得的掩蔽特征重建损失。这种损失在简单样本和困难样本之间急剧增加，从而有效地加快了学习速度。这种缩放的程度由超参数γ控制。\n与此同时，基于样本的结构重建旨在重建图结构（即预测节点之间的边）。与重建整个邻接矩阵不同，后者具有O(N2)的复杂度，基于样本的结构重建在节点对上应用对比采样，并预测这些节点对之间的边概率。仅非掩蔽节点参与结构重建。正样本是由所有非掩蔽节点之间的所有现有边构建的，负样本则是在那些没有现有边的节点对中进行采样构建的。\n使用简单的两层 MLP 重建节点对样本之间的边，为每个样本生成一个概率。在这些样本上，重建损失的形式为简单的二元交叉熵损失：\n其中 （n， n−） 和 （n， n+） 分别是负样本和正样本，ˆ N =N− e N 是一组非掩码节点。基于样本的结构重建仅对输出嵌入进行监督。我们没有使用点积，而是使用 MLP 来计算边缘概率，因为交互实体的行为不一定相似。此外，我们并没有强迫模型学习预测边缘概率。这种结构重构的功能是最大化抽象节点嵌入中包含的行为信息，以便一个简单的MLP足以将这些信息合并并解释为边缘概率。\n最终的目标函数 L = $L_fr$ + $L_sr$ 结合了 $L_fr$ 和 $L_sr$，并为图表示模块提供监督信号，使其能够以自监督的方式学习参数。\nDetection Module 基于图表示模块生成的输出嵌入，利用异常值检测方法以无监督方式进行APT检测。如前几节所述，此类嵌入以不同的粒度总结了系统行为。我们的检测模型的目标是识别恶意系统实体或状态，前提是仅对良性系统行为有先验的了解。如果通过图表示学习生成的嵌入在图中具有相似的交互行为，则它们的相应实体往往会形成聚类。因此，系统状态嵌入中的异常值表示不常见和可疑的系统行为。基于这样的洞见，我们开发了一种特殊的异常值检测方法来进行APT检测。\n在训练过程中，首先从训练来源图中抽象出良性输出嵌入。在这个阶段，检测模块所做的只是记住这些嵌入，并将它们组织在一个K-D树中[33]。经过训练后，检测模块通过三个步骤揭示异常值：k-最近邻搜索、相似度计算和过滤。给定目标嵌入，检测模块首先通过 K-D 树搜索获得其 k 最近邻。这样的搜索过程只需要 log（N） 时间，其中 N 是记忆训练嵌入的总数。然后，应用相似性准则来评估目标嵌入与其邻居的接近程度并计算异常分数。如果其异常分数高于超参数 θ，则目标嵌入被视为异常值，其相应的系统实体或系统状态为恶意。检测模块的示例工作流程形式化如下，使用欧几里得距离作为相似性准则：\n其中 $\\overline{dist}$ 是训练嵌入与其 k 最近邻之间的平均距离。在执行批处理日志级别检测时，检测模块会记住反映系统状态的良性系统状态嵌入，并检测新到达的来源图的系统状态嵌入是否为异常值。在执行系统实体级检测时，检测模块会记住指示系统实体行为的良性节点嵌入，并给定一个新到达的来源图，它会检测所有系统实体嵌入中的异常值。\nModel Adaption 为了使APT检测器在实际检测场景中有效运行，必须考虑概念漂移。当面对良性但以前未见过的系统行为时，MAGIC 会产生误报检测结果，这可能会误导后续应用程序（例如攻击调查和故事恢复）。最近的工作通过忘记过时的数据[10]或通过模型适应机制[18]将他们的模型拟合到良性系统变化来解决这个问题。MAGIC还集成了模型适应机制，以对抗概念漂移，并从安全分析师识别的误报中学习。与其他仅使用误报来重新训练模型的作品略有不同，MAGIC可以使用所有反馈进行重新训练。如前几节所述，MAGIC 中的图形表示模块以自监督的方式将系统实体编码为嵌入，而无需知道其标签。任何看不见的数据，包括那些true negative，都是图表示模块的宝贵训练数据，以增强其对看不见的系统行为的表示能力。\n检测模块只能通过良性反馈进行重新训练，以跟上系统行为的变化。而且随着它记住越来越多的良性反馈，它的检测效率会降低。为了解决这个问题，我们还在检测模块上实现了折扣机制。当记忆嵌入的数量超过一定数量时，随着新到达的嵌入被学习，最早的嵌入被简单地删除。我们提供模型适配机制作为概念漂移和看不见的系统行为的可选解决方案。建议通过将确认的假阳性样本提供给 MAGIC 的模型适应机制来使 MAGIC 适应系统变化。\nImplementation 我们在 Python 3.8 中使用了大约 3,500 行代码实现了 MAGIC。我们开发了几个日志解析器来应对不同格式的审计日志，包括 StreamSpot [34]、Camflow [35] 和 CDM [36]。来源图是使用图处理库 Networkx [37] 构建的，并以 JSON 格式存储。图形表示模块是通过 PyTorch [38] 和 DGL [39] 实现的。该检测模块是用Scikit-learn[40]开发的。对于MAGIC的超参数，特征重建损失中的比例因子γ设置为3，相邻变量k设置为10，学习率为0.001，权重衰减因子等于5×10−4。我们在实验中使用了 3 层图形编码器和 0.5 的掩码率。在批处理日志级别检测和实体级检测两种检测方案中，输出嵌入维度 d 是不同的。我们在批处理日志级别检测中使用 d 等于 256，在实体级检测中使用 和 an 等于 64 以减少资源消耗。检测阈值 θ 是通过对每个数据集分别进行的简单线性搜索来选择的。超参数可能有其他选择。我们将在稍后的评估部分演示这些超参数对 MAGIC 的影响。在我们的超参数分析中，d 是从 {16， 32， 64， 128， 256} 中选出的，l 是从 {1， 2， 3， 4} 中选出的，r 是从 {0.3， 0.5， 0.7} 中选出的。对于阈值 θ，在批处理日志级别检测中选择介于 1 和 10 之间。有关实体级检测，请参阅附录 D。\nEvaluation 我们使用来自各种系统审计软件的 131GB 审计日志来评估 MAGIC 的有效性和效率。我们首先描述了我们的实验设置（第 6.1 节），然后详细说明了 MAGIC 在不同场景中的有效性（第 6.2 节），进行误报分析并评估模型适应机制的有用性（第 6.3 节），并分析了 MAGIC 的运行时性能开销（第 6.4 节）。MAGIC的不同组件和超参数的影响在第6.5节中进行了分析。此外，附录 C 中还对我们的动机示例进行了详细的案例研究，以说明 MAGIC 的管道如何用于 APT 检测。这些实验在相同的设备设置下进行。\nExperimental Settings 我们评估了MAGIC在三个公共数据集上的有效性：StreamSpot数据集[21]，Unicorn Wget数据集[22]和DARPA Engagement 3数据集[20]。这些数据集在数量、来源和粒度方面各不相同。我们相信，通过在这些数据集上测试MAGIC的性能，我们能够将MAGIC与尽可能多的最先进的APT检测方法进行比较，并探索MAGIC的普遍性和适用性。我们对这三个数据集的详细说明如下。\nStreamSpot 数据集：StreamSpot数据集（见表1）是由StreamSpot[34]使用审计系统SystemTap [41]收集并公开的模拟数据集。StreamSpot 数据集包含 600 批次审计日志，用于监控 6 个独特场景下的系统调用。其中五个方案是模拟的良性用户行为，而攻击方案模拟的是偷渡式下载攻击。该数据集被认为是一个相对较小的数据集，由于没有提供日志条目和系统实体的标签，因此我们对 StreamSpot 数据集执行批量日志级别检测，类似于以前的工作 [10， 15， 17]。\nUnicorn Wget 数据集：Unicorn Wget数据集（见表1）包含Unicorn[10]设计的模拟攻击。具体来说，它包含 Camflow [35] 收集的 150 批日志，其中 125 批是良性的，其中 25 批包含供应链攻击。这些攻击被归类为隐形攻击，经过精心设计，其行为类似于良性系统工作流程，预计很难识别。这个数据集被认为是我们实验数据集中最难的，因为它的体积大，日志格式复杂，而且这些攻击的隐蔽性。与最先进的方法相同，我们在此数据集上执行批处理日志级别检测。\nDARPA E3 数据集：DARPA Engagement 3 数据集（见表 2）作为 DARPA 透明计算计划的一部分，在对抗性参与期间在企业网络中收集。利用不同漏洞的 APT 攻击 [20] 由红队进行，以泄露敏感信息。蓝队试图通过审核网络主机并对其执行因果关系分析来识别这些攻击。Trace、THEIA 和 CADETS 子数据集包含在我们的评估中。这三个子数据集总共包含 51.69GB 的审计记录，包含多达 6,539,677 个系统实体和 68,127,444 次交互。因此，我们评估了MAGIC的系统实体级检测能力，并解决了这些数据集的开销问题。\n对于不同的数据集，我们采用不同的数据集拆分来评估模型，并且我们仅使用良性样本进行训练。对于 StreamSpot 数据集，我们从 500 个良性日志中随机选择 400 个批次进行训练，其余批次进行测试，从而形成一个平衡的测试集。对于 Unicorn Wget 数据集，选择了 100 批良性日志进行训练，其余用于测试。对于 DARPA E3 数据集，我们使用与 ThreaTrace [17] 相同的真值标签，并根据其出现的顺序拆分日志条目。最早的 80% 日志条目用于训练，其余条目保留用于测试。在评估过程中，MAGIC在100个全局随机种子下的平均性能被报告为最终结果，因此实验结果可能包含系统实体/日志批次的分数。\nEffectiveness conclusion 我们推出了 MAGIC，这是一种普遍适用的 APT 检测方法，它以最高的效率运行，开销很小。MAGIC 利用掩码图表示学习从原始审计日志中对良性系统行为进行建模，并通过异常值检测方法执行多粒度 APT 检测。在各种检测场景下对三个广泛使用的数据集进行评估表明，MAGIC以低误报率和最小的计算开销取得了良好的检测结果。\n知识补充 多标签哈希 多标签哈希（Multi-Label Hashing）是一种用于解决多标签分类问题的技术。在多标签分类问题中，每个样本可以属于一个或多个类别，而不是单一的类别。\n哈希是一种将数据映射到固定长度的二进制编码的方法。多标签哈希技术将这种哈希方法应用于多标签分类问题，将样本的多个标签映射为固定长度的二进制编码，从而方便快速的类别检索和处理。\n以下是多标签哈希的一些关键思想和方法：\n1. 单一哈希方法 Bit-Vector Hashing：最简单的多标签哈希方法之一是将每个标签映射为二进制编码的位向量。例如，如果有5个类别，则可以用5位二进制编码来表示每个标签，如 [1, 0, 1, 1, 0]。 2. 多哈希方法 Binary Relevance：这种方法将每个标签独立地进行哈希处理。对于每个标签，都使用一个单独的哈希函数，将其映射为固定长度的二进制编码。这种方法简单直观，但可能导致标签之间的相关性被忽略。 Multiple Hashing：这种方法使用多个哈希函数，将每个标签映射为多个不同的二进制编码。这可以捕捉到标签之间的一些相关性，提高多标签哈希的性能。 3. 哈希函数的选择 局部敏感哈希（Locality Sensitive Hashing，LSH）：LSH 是一种常用的哈希方法，它能够使相似的样本在哈希空间中映射为相邻的编码。这有助于快速的近似最近邻（ANN）搜索，对于多标签分类中的相似性检索很有用。 Deep Learning Hashing：使用深度学习模型学习哈希函数也是一种常见的方法。例如，可以使用卷积神经网络（CNN）或循环神经网络（RNN）来学习将标签映射为二进制编码的函数。 4. 检索和评估 哈希编码检索：一旦对样本进行了哈希处理，可以使用快速的哈希编码检索技术来查找与查询标签最相似的样本。 评估指标：对于多标签哈希，常用的评估指标包括 Hamming Loss、Hamming Distance、Precision、Recall、F1 Score 等，用于衡量模型的分类准确性和性能。 多标签哈希方法可以提高对于多标签数据的处理效率和准确性，尤其在大规模的多标签数据集中具有很好的应用前景。\n图形掩蔽自编码器 \u0026ldquo;Graph Masked Autoencoders\u0026rdquo; 是一种用于图形数据的自编码器模型，旨在学习图形数据的低维度表示。\n这个模型的主要思想是结合了自编码器（Autoencoder）的概念和图形数据的结构。自编码器是一种无监督学习算法，用于学习数据的紧凑表示，并在重建时最大程度地保留原始数据的信息。\n\u0026ldquo;Graph Masked Autoencoders\u0026rdquo; 在这个基础上加入了一种“掩蔽”机制，用于处理图形数据。这个“掩蔽”机制的目的是在训练过程中限制模型只能看到部分图形数据，从而强制模型学习到更加泛化的图形特征表示。\n具体来说，训练过程包含以下步骤：\n掩蔽图形数据（Masking Graph Data）：模型会随机地将一些节点或边从输入图中“掩蔽”（即隐藏），使得模型在训练时只能看到部分图形数据。 编码器（Encoder）：掩蔽后的图形数据通过编码器部分进行编码，将其映射到一个低维度的特征表示空间。 解码器（Decoder）：然后，模型尝试从这个低维度表示中重构原始的图形数据。 通过这个过程，模型被迫学习到不同节点和边之间的潜在关系，以及如何在只看到部分数据时对图形数据进行有效的编码和解码。\n这种方法的优势在于它能够提高模型对图形数据的泛化能力。因为模型只能看到部分数据，它不会过度依赖于特定的节点或边，从而可以更好地处理未见过的图形数据。\n\u0026ldquo;Graph Masked Autoencoders\u0026rdquo; 的应用包括图形节点分类、图形重构、图形生成等任务。它是图神经网络（Graph Neural Networks）领域中的一种重要技术，用于学习和处理复杂的图形结构数据。\n概念漂移 概念漂移是指机器学习模型在应用于新数据时性能下降的现象。这种现象通常发生在训练模型的数据分布与新数据的分布不同时。具体来说，概念漂移可能会导致模型在新数据上的预测准确性降低。\n概念漂移可以分为几种不同类型：\n特征漂移（Feature Drift）：特征漂移是指输入特征的分布在训练数据和测试数据中不同的情况。例如，训练数据中的特征范围可能与测试数据中的范围不同，这会导致模型无法准确地泛化到新数据。 标签漂移（Label Drift）：标签漂移是指目标变量的分布在训练数据和测试数据中不同的情况。换句话说，模型在训练数据中学到的标签分布可能与实际数据中的标签分布不同，从而影响模型的性能。 概念漂移（Concept Drift）：概念漂移是指预测变量和目标变量之间的关系在时间或数据分布上发生变化。这种情况下，模型在训练时学到的规律可能在应用到新数据时不再适用。 概念漂移是机器学习应用中一个重要的挑战，因为它可能导致模型的预测性能下降，需要采取一些方法来处理。一些应对概念漂移的方法包括：\n在线学习（Online Learning）：使用新数据不断更新模型，使其能够适应新的数据分布。 监督漂移检测（Supervised Drift Detection）：监测模型在新数据上的表现，当性能下降时触发模型的重新训练。 集成学习（Ensemble Learning）：结合多个模型的预测结果，以减少概念漂移对整体性能的影响。 领域适应（Domain Adaptation）：通过调整模型或数据来使其适应新的数据分布。 处理概念漂移是一个活跃的研究领域，因为许多现实世界的应用场景中数据分布经常发生变化。\nK-D tree K-D 树（K-Dimensional Tree，K-Dimensional Binary Tree）是一种用于高效处理k维空间的数据结构，用于解决近似最近邻搜索（Approximate Nearest Neighbor Search）等问题。它是一种二叉树结构，用于对 k 维数据进行分割和组织，以便快速地搜索最近的邻居。\n我的想法 和之前看的ATLAS比较起来，通过mask和减小模型复杂度来实现降低开销，并且没有造成结果的损失。好像就没啥更吊的地方了吧？\n加了个监督学习的步骤\n嵌入阶段用了mask和图注意力网络的技术手段，能获得质量更高，多跳敏感的嵌入。\n通过检测时候通过KD树找到K个临近的embedding，然后通过相似性计算， 设立一个阈值来判断是否具有恶意。再用decoder来还原出embedding所对应的边或者节点\n","date":"2024-03-02T20:19:11+08:00","permalink":"http://localhost:1313/p/magic/","title":"MAGIC"},{"content":"文章可以在这里获取\nintroduction APT攻击涉及长期的多个攻击步骤，其调查需要分析大量日志以确定其攻击步骤。因此提出ATLAS从现成的审计日志构建端到端攻击故事的框架。\nATLAS基于的观察：无论利用的漏洞和执行的有效载荷如何，不同的攻击可能具有相似的抽象攻击策略。\nATLAS利用因果关系分析、自然语言处理和机器学习技术的新颖组合来构建基于序列的模型，该模型从因果图中建立攻击和非攻击行为的关键模式。\n取证分析从多个主机、应用程序和网络接口收集各种审计日志。海量日志通常被离线分析或实时监控，以调试系统故障并识别复杂的威胁和漏洞。\n现有方法：\n从审计日志中构建因果依赖关系图，并使用查询系统来定位关键攻击阶段（例如，受损进程或恶意负载）。 扩展机器学习（ML）技术，从日志中提取特征/序列，以自动检测入侵和故障。 构建了通过事件关联发现不同日志事件之间关联的技术 这些方法在很大程度上无法精确定位关键攻击步骤，从而有效地突出端到端攻击故事。因此我们希望从审计日志中识别关键实体（节点），帮助网络分析师构建 APT 攻击的关键步骤。\nATLAS将自然语言处理 （NLP） 和深度学习技术集成到数据来源分析中，以识别攻击和非攻击序列。分为三个阶段\n处理系统日志并构建自己的优化因果依赖图 通过NLP技术从因果图中构建语义增强序列（时间戳事件） 学习表示攻击语义的基于序列的模型，这有助于在推理时恢复描述攻击故事的关键攻击实体。 ATLAS不会带来额外开销，不同的审计日志可以很容易地集成到 ATLAS 日志解析器中用来构建因果图并获得精确的序列和模型\n我们的方法基于：因果依赖关系图中不同攻击的关键步骤可能具有相似的模式。这些模式可以通过NLP技术（即词形还原和词嵌入）转换为序列，将攻击和非攻击实体之间各种变化形式的关系组合在一起。它为模型提供了具有不同因果关系的更深层次的记忆。，进而提高了序列模型从未知审计日志中识别攻击步骤的准确性。\n但是这样的方法面对以下三个挑战，相应的，ATLAS采取手段来应对：\n因果图通常庞大而复杂，这使得序列构建变得困难\u0026ndash;\u0026gt;采用定制化的图优化算法来降低图的复杂度 它需要一种方法来精确构建序列，以有效地模拟合法和可疑的活动\u0026ndash;\u0026gt;提出一种从事件中提取攻击模式序列的新技术 需要一种自动化方法来识别给定的攻击症状中的攻击事件\u0026ndash;\u0026gt;通过攻击症状进行攻击调查，以恢复攻击事件，帮助全面构建攻击故事。 总的来说，ATLAS做了\n引入了 ATLAS，这是一个用于攻击故事恢复的框架，它利用自然语言处理和基于序列的模型学习技术来帮助网络分析师从审计日志中恢复攻击步骤. 提出了一种新的序列表示，通过词形还原和词嵌入来抽象攻击和非攻击语义模式。这些序列使 ATLAS 能够构建一个有效的基于序列的模型，以识别构成攻击故事的攻击事件 我们在受控环境中通过其真实世界报告开发的 10 种现实 APT 攻击中验证了 ATLAS。结果表明，ATLAS能够高精度、最小开销地识别攻击故事的关键攻击条目。 Motivation and Definitions 整篇论文中设定了一种攻击场景：攻击者通过电子邮件向企业中的目标用户发送恶意Microsoft Word文件（contract.doc）。用户被欺骗使用 Firefox 从 Gmail 下载和打开 Word 文件。该文档包含一段恶意代码，该代码利用易受攻击的 Microsoft Word （winword.exe） 并发出 HTTPS 请求以下载恶意 Microsoft HTA 脚本 （template.hta）。此脚本执行恶意 Visual Basic 脚本 （maintenance.vbs），其中包含安装后门以泄露敏感文件的 PowerShell 命令。最后，攻击者横向移动到其他主机。\n调查这个场景通常从从审核日志中收集有关攻击的数据开始，例如系统事件、DNS 查询和浏览器事件。攻击调查工具通常以因果图（或来源图）的形式表示审核日志，该图用作取证工具，使安全调查人员能够执行根本原因分析，并更好地了解攻击的性质。大多数先前的研究将因果图中的攻击故事恢复为子图，其中该图中的节点和边与攻击症状具有因果关系。图 1 （a） 显示了由这些工具生成的示例攻击场景的因果关系图。红色虚线箭头表示从中启动攻击调查的警报事件（α，可疑网络连接），红色虚线矩形区域表示已恢复的攻击子图。\n但是即使应用了不同的图优化技术，这样的图仍然非常大，并且在实践中难以解释。这些工作很大程度上依赖于启发式或硬编码规则，这些规则的开发和维护非常耗时。领域知识专家需要不断更新这些规则，以涵盖新开发的攻击。而ATLAS只需要更多的攻击训练数据来学习新的攻击模式。\n其他人提出了基于异常的方法，该方法可以学习用户行为，并将任何偏离该行为的行为识别为异常。虽然基于异常的方法可以识别未知攻击，但随着用户行为随时间的变化，它们可能会出现许多误报。为了解决这个问题，ATLAS旨在学习攻击模式和用户行为，以确定两者之间的异同。\n与ATLAS类似，基于学习的方法使用ML算法从日志中对攻击事件进行建模。虽然这些方法可以有效地减少日志条目的数量，但仍需要大量的手动工作才能找到攻击事件的高级视图。为了解决这个问题，ATLAS调查旨在识别攻击关键实体（节点），使其能够自动识别相关攻击事件的子集。\nAPT攻击可以概括为从审计日志中获取的攻击阶段的时间序列， 例如图1（b）中所示的步骤1-14，类似于自然语言中描述的攻击步骤。这些攻击步骤通常适合在特定上下文中作为表示攻击语义的唯一序列，这可以与审核日志中的正常活动区分开来。\nATLAS 在推理时给定攻击症状节点（警报事件α包含的恶意 IP 地址），提取一组与症状节点关联的候选序列，并使用基于序列的模型来识别序列中的哪些节点参与了攻击。此后，它使用已识别的攻击节点来构建攻击故事，其中包括已识别的攻击节点的事件，从而使攻击调查更加简洁，更容易被调查人员解读。\n图 1 （c） 说明了 ATLAS 为激励示例恢复的攻击故事，其中包括示例攻击的完整关键攻击步骤。此过程大大减少了从大型因果图中进行攻击调查的手动工作，该图排除了对攻击没有影响的事件，并减少了调查大型因果图所需的时间。\ndefinition 因果图G：因果图是从审计日志中提取的数据结构，通常用于来源跟踪，指示主题（例如，流程）和对象（例如，文件或连接）之间的因果关系。因果图由节点组成，节点代表主体和客体，边缘连接，边缘代表主体和客体之间的动作（例如，读取或连接）。我们在这里考虑一个有向循环因果图，它的边缘从主体指向对象。\n实体e：实体是从因果图中提取的唯一系统主体或对象，在其中它表示为节点。我们考虑的实体包括进程、文件和网络连接（即IP地址和域名）\n邻域图。给定因果图，如果两个节点u和v通过一条边连接，则称它们为邻居。节点 n 的邻域是由节点 n 和连接相邻节点与节点 n 的边组成的 G 子图。类似地，给定一组节点 {n1,n2,\u0026hellip;,nn}，我们提取一个统一的邻域图，其中包括将它们连接到相邻节点的所有节点和边。\n事件：事件ε是一个四元组（src、action、dest、t），源 （src） 和目标 （dest） 是与动作相关的两个实体。t 是显示事件发生时间的事件时间戳。给定一个实体 e，可以从 e 邻域图中提取其事件，其中包括与 e 的邻居相关的所有操作。例如，给定一个实体Firefox.exe和一个邻域图，其中包含从节点 Firefox.exe 到节点 Word.doc 的操作 open 和时间戳 t，那么 （Firefox.exe， open， Word.doc， t） 是 Firefox 进程在时间 t 打开 Word 文件的事件\n序列：给定一个实体 e，可以从因果图中提取序列 S。序列 S 按时间顺序包括实体 e 的邻域图的所有事件，使得 S{e} ：= {ε1， ε2， . . . ， εn}。同样，如果给定一组实体，我们可以从它们的统一邻域图中提取一个包含所有事件的序列。\n图 2 （a） 说明了具有六个实体 {eA， eB， . . . ， eF} 的因果图。图 2 （b） 显示了 eB 的邻域图，其中包括节点 B、相邻节点 {A， C} 及其连接边 {EAB， EBC}。类似地，实体集 {eB， eC} 的邻域图包括节点 {A， B， C， D， E} 和边 {EAB， EBC， ECD， ECE}，如图 2 （b） 所示。实体 eB 的事件为 εAB =\u0026lt; eA， a1， eB， t1 \u0026gt; 和 εBC =\u0026lt; eB， a2， eC， t2 \u0026gt;如图 2 （c） 所示。图 2 （d） 显示了实体集 {eB， eC} 的事件序列。\n我的想法 利用NLP的方法，将问题转为序列识别问题。使用了新颖的数据增强方法。\n","date":"2024-03-02T20:16:17+08:00","permalink":"http://localhost:1313/p/atlas/","title":"ATLAS"},{"content":"导语 SP24\ndoi is here\nAbstract 溯源图是描述系统执行历史的结构化审计日志。最近的研究探索了多种技术来分析自动主机入侵检测的溯源图，特别关注高级持续威胁（APT）。通过筛选他们的设计文档，我们确定了推动基于溯源图的入侵检测系统 (PIDS) 开发的四个常见维度：范围（能否检测到渗透到应用程序边界的现代攻击？）、攻击不可知性（能否检测到新颖的攻击而无需攻击特征的先验知识？）、及时性（能否在主机系统运行时有效地监控主机系统？）以及攻击重建能力（能否从大型溯源图中提取攻击活动，以便系统管理员能够轻松理解并快速响应系统入侵？） 。我们提出了 KAIROS，这是第一个同时满足所有四个维度的需求的 PIDS，而现有方法至少牺牲了一个维度，并且难以实现可比的检测性能。 KAIROS 利用基于新型图神经网络的编码器-解码器架构，该架构可以学习溯源图结构变化的时间演化，以量化每个系统事件的异常程度。然后，基于这些细粒度信息，KAIROS 重建攻击足迹，生成紧凑的摘要图，准确描述系统审核日志流上的恶意活动。使用最先进的基准数据集，我们证明 KAIROS 优于以前的方法。\nsenario 来自DARPA E3-THEIA的溯源图的摘要，描述了一种攻击活动，由KAIROS自动生成。矩形、椭圆形和菱形分别表示进程、文件和套接字。R=读取，W=写入，O=打开，S=发送，Rc=接收，C=克隆，E=执行。为了清晰起见，我们添加了颜色和虚线元素，以突出显示 KAIROS 生成的输出。KAIROS从原始来源图中提取实体节点和边，以重建攻击。根据攻击地面真相，虚线粉红色节点和边缘是 KAIROS 错过的与攻击相关的活动。蓝色节点和边是基本事实中未明确提及但包含在 KAIROS 中的活动。\nchallenge 不可知性：有没见过的攻击，需要模型具有泛化性 攻击重建：很难通过溯源图得到并重现完整的攻击流程 及时性：作为入侵检测系统（IDS)需要及时性，这就要求模型性能比较高 Method KAIROS是一个基于异常的入侵检测和攻击调查系统。它通过来源图中的因果依赖关系，利用最先进的深度图学习和社区发现， 可以做到\n在事先不了解任何特定攻击特征的情况下检测异常系统行为。 根据信息流关联检测到的异常内核对象之间。 KAIROS 提供简洁且有意义的摘要图表，用于节省人力的人机交互取证分析。 下图描述了KAIROS的架构，由四个主要组件组成：\n图的构建和表示。 KAIROS以流式传输方式分析图表，按时间顺序摄取图表中出现的边。KAIROS 考虑三种类型的内核对象和九种类型的交互（即系统事件）。 KAIROS 将每个事件转换为有向、带时间戳的边，其中源节点代表事件的主体，目标节点代表所作用的对象。使用基于节点属性的分层特征哈希技术对节点的特征进行编码（但是感觉是先层次花，再用FeatureHasher，感觉也没有多次hash，而且他的形式化表达里映射正负的$\\mathcal{H}$也没看到啊。)\n图学习。 当图中出现新的边缘，KAIROS 使用编码器-解码器架构来重建边缘。编码器将边缘周围的邻域结构和邻域中节点的状态作为输入。节点的状态是与每个节点关联的特征向量，描述节点邻域的变化历史。然后，解码器根据编码器输出的边缘嵌入重建边缘。原始边缘和重建边缘之间的差异称为重建误差。在训练阶段，KAIROS同时训练编码器和解码器，以最大限度地减少良性边缘的重建误差。在部署过程中，各个边缘的重建误差被用作异常检测和调查的基础。此外，KAIROS更新新边的源节点和目标节点的状态。在encoder-decoder结构中，使用基于节点属性的分层特征哈希技术对节点的特征进行编码，利用时间图网络（TGN）对边进行嵌入，通过新的边（事件）动态更新节点特征，可以有效地将时序特征保存在节点特征中，同时边的嵌入也基于邻域内节点的特征向量。这样可以有效地保存事件的时序信息（感觉这个想法还挺常见的，就是要获取时间相关的特征)。\n异常检测。 KAIROS 构建时间窗口队列来检测部署期间的异常情况。为此，KAIROS根据边的重建误差在每个时间窗口中识别一组可疑节点。具有重叠可疑节点的两个时间窗口被排在一起。当新的时间窗口添加到队列时，KAIROS 也会根据重建错误更新队列的异常分数。如果分数超过阈值，KAIROS会认为队列异常并触发警报。因此，KAIROS以时间窗口的间隔定期执行异常检测。在图中，KAIROS 检测到由时间窗口 1、2 和 4 组成的异常队列。\n异常调查。 为了帮助系统管理员推理警报，KAIROS 自动从异常时间窗口队列生成紧凑的攻击摘要图。这涉及识别具有高重建误差的边缘社区以提高易读性。图形简化是必要的，因为与图像和文本不同，图形即使是人类专家也很难可视化和解释。在图中，系统管理员只需要了解 KAIROS 中的一个小的汇总图，而不是跟踪触发警报的异常时间窗口队列中的一个大得多的图。\n补充知识 分层特征哈希 在传统的哈希技术中，常见的方法是将特征空间的维度映射到固定大小的哈希表中。然而，当特征空间非常大时，这种方法可能会导致哈希冲突，进而影响模型的性能。\n分层特征哈希通过将特征空间分解为多个层级来解决这个问题。每个层级都具有不同的哈希函数，用于将特征映射到不同的桶中。通常，初始层级的哈希函数将原始特征映射到较小的中间空间，然后通过逐渐应用更多的哈希函数，将特征映射到最终的哈希桶中。\n分层特征哈希将高维输入向量投影到低维特征空间，同时保留原始输入之间的分层相似性。分层特征哈希在处理大规模高维数据时具有很好的效果，例如在文本分类、推荐系统和图像检索等任务中经常被使用。\nTGN TGN，全称Temporal Graph Networks，是一种针对时间图的网络嵌入方法。在许多实际应用中，包括社交网络、交通网络等，图的拓扑结构和节点的交互是随着时间发展而变化的，这种图被称为时间图。TGN的目标是为时间图中的节点和边生成嵌入向量，以便在这些向量上进行各种预测任务，如链接预测、节点分类等。\nTGN对边的嵌入主要涉及以下步骤：\n节点嵌入更新：当新的边事件（如用户间的交互）发生时，TGN会根据新的边事件信息更新对应节点的嵌入。具体来说，TGN采用了记忆化的节点嵌入更新机制，即根据节点的历史嵌入和新的边事件信息共同决定节点的新的嵌入。 边嵌入生成：在节点嵌入更新后，TGN会生成新的边的嵌入。具体来说，边的嵌入是由连接该边的两个节点的嵌入和边事件的时间信息共同决定的。例如，一种简单的方法是将两个节点的嵌入和边事件的时间信息拼接起来，然后通过一个全连接网络生成边的嵌入。 时间编码器：为了捕获边事件的时间信息，TGN引入了一个时间编码器，它可以将边事件的时间戳编码为一个连续的向量。在生成边的嵌入时，会将这个时间向量和节点的嵌入一起考虑。 通过以上步骤，TGN能够处理时间图中的边事件，并为每个边事件生成一个嵌入向量，这个嵌入向量同时考虑了边事件的拓扑结构信息和时间信息。这使得TGN能够适应图的动态变化，并进行各种预测任务。\n我的评价 贡献主要有：\n提出了PIDS的四个重要指标 提出了新的三对象九交互的建图模式（但是没怎么证明有效性) 提出了时间窗口的社区发现的攻击重建方法，比较紧凑，会更方便溯源人员重建攻击。 实验做的很丰富，包括对不同数据集的分析，以及在相应的数据集上和其他的现有方法的对比做的很好。 我的评价是，这篇文章最厉害的是实验部分，他把DARPA的数据集都做了检测，其他的感觉中规中矩也就，但是他能发SP你有意见吗？\n","date":"2024-03-02T20:15:55+08:00","permalink":"http://localhost:1313/p/kairos/","title":"KAIROS"},{"content":"abstract 目的：实现加密流量[VPN、tor]的分类\n现有局限性：只能提取低级别特征，基于统计的方法对短流无效，对header和payload采取不平等的处理，难以挖掘字节之间的潜在相关性。\n提出方法：\n基于逐点互信息(PMI)的字节级流量图构建方法 基于图神经网络(TFE-GNN)进行特征提取的时序融合编码器模型 引入了一个双嵌入层、一个基于GNN的流量图编码器以及一个交叉门控特征融合机制。[分别嵌入header和payload，然后通过融合实现数据增强] 结果：两个真实数据集（WWT和ISCX）优于SOTA\nintroduction 加密流量保存用户隐私同时也给了攻击者藏身的机会。\n传统的数据包检测（DPI）挖掘数据包中的潜在模式或关键词，面对加密数据包时耗时且准确性低。\n由于动态端口的应用，基于端口的工作不再有效。\n通过数据流的统计特征（e.g.数据包长度的平均值）采用机器学习分类器（e.g.随机森林）来实现分类的方法，需要手工制作的特征工程，并且在某些情况下可能会由于不可靠/不稳定的fow级统计信息而失败。与长流相比，短流的统计特征有更大的偏差（e.g.长度通常服从长尾分布）意味着不可靠的统计特征普遍存在。我们使用数据包字节而非统计特征。\nGNN 可以识别图中隐含的特定拓扑模式，以便我们可以用预测标签对每个图进行分类。目前大多数GNN根据数据包之间的相关性来构建图，这实际上是统计特征的另一种使用形式，导致上述问题。\n用了数据包字节的方法\n平等地对待header和payload，忽略了它们之间的含义差异。 原始字节利用不足，只是将数据包视为节点，将原始字节作为节点特征，不能充分利用。 本文提出了一种基于逐点互信息（PMI）的字节级图构建方法，一种基于图神经网络(TFE-GNN)进行特征提取的时序融合编码器模型。通过挖掘字节之间的相关性来构建流量图，用作TFE-GNN的输入。\nTFE-GNN由三大子模块组成（即双嵌入、流量图编码器和交叉门控特征融合机制）。双嵌入层分别嵌入header和payload；图编码器将图编码为高维图向量；交叉门控特征融合机制对header和payload的图向量融合，得到数据包的整体表示向量。\n使用端到端训练（从输入数据到最终输出直接进行训练，而无需将任务分解为多个独立的阶段或模块），采用时间序列模型，获得下游任务的预测结果。\n实验使用了自收集的WWT（WhatsApp、WeChat、Telegram）和公开的ISCX数据集，与十几个baseline比较得出TFE-GNN效果最好。\npreliminary 1、图的定义\nG = { $V,\\varepsilon,X$}表示一个图，V是节点集合，$\\varepsilon$是边集，X是节点的初始特征矩阵（每个节点的特征向量拼起来）\n$A$是大小为$\\lvert V \\lvert * \\lvert V \\lvert$图的邻接矩阵\n$N(v)$是节点v相邻的节点\n$d_l$是第l层的嵌入维度\nTS(traffic segment)=[$P_{t_1},P_{t_2}\u0026hellip;P_{t_n}$]是一段时间内的数据包的集合。$P_{t_i}$是时间戳为$t_i$的数据包，n是流量序列的长度。$t_1,t_2$是流量序列的开始和结束时间。\n2、加密流量分类\nM是训练样本数量\nN是分类类别\n$bs^j_i=[b^{ij}_1, b^{ij}_2,\u0026hellip;,b^{ij}_m]$，m是字节序列长度，$b^{ij}_k$是第i个流量样本第j个字节序列的第k个字节\n$s_i=[bs^i_1,bs^i_2,\u0026hellip;,bs^i_n)]$，n是序列长度$bs^i_j$为第i个样本的第j个字节序列，可以理解为就是TS\n3、 MP-GNN\nMP-GNN 是 Message Passing Graph Neural Network（消息传递图神经网络）的简称，节点嵌入向量可以通过特定的聚合策略将节点的嵌入向量集成到邻域中，从而迭代更新节点嵌入向量。\n第l层 MP-GNN 可以形式化为两个过程\n其中$h^{(l)}_u,h^{(l)}_v$是节点u和v在第l层的嵌入向量，$m^{(l)}_u$是l层中节点u的计算信息，$MSG^{(l)}$是消息计算函数，$AGG^{(l)}$是消息聚合函数，$\\theta$是他们对应的参数\nmethodology 字节级流量图构造 Byte-level Traffic Graph Construction 节点：某一个字节，注意相同的字节值共享同一个节点，因此节点个数不会超过256，这样能够保持图在一定的规模下，不会太大。\n字节之间的相关性表示：采用点互信息（PMI）来建模两个字节之间的相似性，字节i和字节j的相似性用$PMI(i,j)$表示。\n边：根据PMI值来构造边，PMI值为正：表示字节之间的语义相关性高；而PMI值为零或负值：表示字节之间的语义相关性很小或没有。因此，我们只在PMI值为正的两个字节之间创建一条边。\n节点特征：每个节点的初始特征为字节的值，维度为1，范围为[0,255]\n图构建：由于$PMI(i,j)=PMI(j,i)$，因此该图是个无向图。\nPMI PMI：是一种用于衡量两个事件之间相关性的统计量。\n$$ PMI(A, B) = \\log \\frac{P(A, B)}{P(A) \\cdot P(B)} $$\n值大于零，则表示 A 和 B 之间有正相关性；如果值等于零，则表示它们之间没有关联；如果值小于零，则表示它们之间有负相关性。\n双嵌入 dual embedding 原因：字节值通常用作进一步向量嵌入的初始特征。具有不同值的两个字节对应两个不同的嵌入向量。然而，字节的含义不仅随字节值本身而变化，还随它所在的字节序列的部分而变化。换句话说，在数据包的header和payload中，具有相同值的两个字节的表示含义可能完全不同。对于header和payload，使用两个不共享参数的嵌入层的双嵌入，嵌入矩阵分别是$E_{header}$和$E_{payload}$\n交叉门控特征融合的流量图编码器 Traffic Graph Encoder with Cross-gated Feature Fusion 因为要double embedding，所以encoder也要两个。\n这里堆叠了N个GraphSAGE\nGraphSAGE 对于图 G 中的每个节点 v，GraphSAGE 通过使用节点 v 的度数归一化其嵌入向量，计算来自每个相邻节点的消息。\n通过逐元素均值运算（element-wise mean operation）计算所有相邻节点$N(v)$的整体消息，并通过串联运算聚合整体消息以及节点v的嵌入向量\n对节点A的嵌入向量进行非线性变换，完成一个GraphSAGE层的正向过程\nGraphSAGE的消息聚合和计算可以描述为\n$m^{(l)}_{N(v)}$是将v节点所有临接节点的上一层嵌入向量求平均的结果\n$h_v^{(l)}$是本层v节点的嵌入向量，$\\sigma(\\cdot)$是激活函数，$CONCAT(\\cdot)$是连接函数。然后通过BatchNorm对h进行批量归一化\n激活函数选择PReLU，将每个负元素值按不同因子缩放，不但引入了非线性，还由于每个负元素的缩放因子的不同而起到类似于注意力机制的作用。\n由于深度GNN模型中的过度平滑问题，我们最多只堆叠GraphSAGE4层，并将输出拼接起来。这与跳转知识网络（JKL）相类似\n最后，通过对每个节点的最终嵌入使用meanpooling来得到图嵌入\n用$g_h,g_p$表示header和payload得到的图嵌入\nCross-gated Feature Fusion 交叉门控特征融合，这个模块的目标是将$g_h,g_p$融合，获取门控矢量$s_h,s_p$。\n如上图，我们用了两个filter，每个filter的组成都是线性层、PReLU、线性层、Sigmoid。\n其中w和b分别是线性层的weight和bias。z 是数据包字节的整体表示向量。\n端到端的下游任务训练 End-to-End Training on Downstream Tasks 由于我们已经将流量段中每个数据包的原始字节编码为表示向量z，因此可以将段级分类任务视为时间序列预测任务。\n这里我们使用双层Bi-LSTM作为baseline，他的输出喂给一个带PReLU的两层线性分类器，使用交叉熵作为损失函数\nn是长度，CE是交叉熵，y是ground truth（标注数据的分类标签）\n实验部分还有一个用transformer的\nexperiments 介绍了实验设置，在很多数据集和baseline上做了对比实验，做了消融实验（good）\n还分析了TFE-GNN的灵敏度，回答了\n每个组件的功能 哪个GNN架构效果最好 TFE-GNN的复杂度如何 超参数的变化在多大程度上会影响TFE-GNN的有效性 实验设置 数据集：ISCX VPN-nonVPN , ISCX Tor-nonTor , self-collected WWT datasets.\n预处理：对于每个数据集，筛除\n空流或空段：所有数据包都没有有效负载（用于establish 连接） 超长流或超长段：长度（即数据包数）大于 10000 然后对于每个数据包，删掉以太网标头，源 IP 地址和目标 IP 地址以及端口号都将被删除，以消除IP地址和端口号的敏感信息的干扰##\n细节：一个样本的最大数据包数设置为50，最大有效负载字节长度和最大标头字节长度分别设置为 150 和 40，PMI 窗口大小设置为 5\nepoch设置为120，lr为1e-2，用Adam优化器分512批次将lr从1e-2衰减到1e-4。warmup为0.1，droupout为0.2.\\\n运行了10次实验。\n用AC、PR、RC和F1做评估\n和基于传统特征工程的方法（即 AppScanner [31]、CUMUL [23]、K-FP （K-Fingerprinting） [8]、FlowPrint [32]、GRAIN [43]、FAAR [19]、ETC-PS [40]）、基于深度学习的方法（即 FS-Net [18]、 EDC [16]、FFB [44]、MVML [4]、DF [30]、ET-BERT [17]）和基于图神经网络的方法（即 GraphDApp [29]、ECD-GNN [11]）做比较。\n消融实验 在 ISCX-VPN 和 ISCX-Tor 数据集上对 TFE-GNN 进行了消融实验，分别将头、有效载荷、双嵌入模块、跳跃知识网络式串联、交叉门控特征融合和激活函数以及批量归一化分别表示为 \u0026lsquo;H\u0026rsquo;、\u0026lsquo;P\u0026rsquo;、\u0026lsquo;DUAL\u0026rsquo;、\u0026lsquo;JKN\u0026rsquo;、\u0026lsquo;CGFF\u0026rsquo; 和 \u0026lsquo;A\u0026amp;N\u0026rsquo;。不仅验证了TFE-GNN中每个组件的有效性，而且还测试了一些替代模块或操作的影响，用sum、max替换mean，用GRU、transformer替换LSTM\n（只用H或P就不需要DUAL和CGFF）\n得出结论\n数据包标头在分类中起着比数据包有效载荷更重要的作用，不同的数据集具有不同级别的标头和有效载荷重要性 使用双嵌入使 f1 分数分别提高了 3.63% 和 0.95%，这表明其总体有效性。JKN样串联和交叉门控特征融合在两个数据集上都以相似的幅度增强了TFE-GNN的性能。 缺少激活函数和批量归一化在两个数据集上都可以看到显著的性能下降，证明了其必要性 用sum替换mean在两个数据集上分别差了11.1%和29.64%，用max替换mean在VPN上差很多在tor上只差一点 用GRU替换LSTM导致两个都差10%左右，transformer替换LSTM导致VPN差了40%左右，tor上只差一点 换GNN架构为GAT, GIN, GCN and SGC，还是GraphSAGE最好\n此外，通过TFE-GNN可以快捷的拓展一个segment级的全局特征\n复杂度\nTFE-GNN 在模型复杂度相对较小的情况下，在公共数据集上实现了最显着的改进。虽然ET-BERT在ISCX-nonVPN数据集上达到了可比的结果，但ET-BERT的FLOP大约是TFEGNN的五倍，模型参数的数量也增加了一倍，这通常表明模型推理时间更长，需要更多的计算资源。此外，ETBERT的预训练阶段非常耗时，由于预训练期间有大量的额外数据，并且模型复杂度高，因此成本很高。相比之下，TFE-GNN可以实现更高的精度，同时降低训练或推理成本。\n双重嵌入维度的影响。为了研究双嵌入层隐藏维度的影响，我们进行了灵敏度实验，结果如图a所示。正如我们所看到的，当嵌入维度低于 100 时，f1 分数会迅速增加。在此之后，随着维度的变化，模型性能趋于稳定。为了减少计算消耗，选取50作为默认维度\nPMI窗口大小的影响。从图 b 中可以看出，较小的窗口大小通常会导致更好的 f1 分数。窗口越大图中添加的边就越多，由于图太密集，模型将更难分类。\n段长的影响。从图 c 中，我们可以得出一个结论，即用于训练的短段长度通常会使性能更好。\nconclusion and future work 我们提出了一种构建字节级流量图的方法和一个名为 TFE-GNN 的模型，用于加密流量分类。字节级流量图构造方法可以挖掘原始字节之间的潜在相关性并生成判别性流量图。TFE-GNN 旨在从构建的流量图中提取高维特征。最后，TFE-GNN可以将每个数据包编码为一个整体表示向量，该向量可用于一些下游任务，如流量分类。选择了几个基线来评估 TFE-GNN 的有效性。实验结果表明，所提模型全面超越了WWT和ISCX数据集上的所有基线。精心设计的实验进一步证明了TFE-GNN具有很强的有效性。\n将来，我们将尝试在以下限制方面改进 TFE-GNN。（1）有限的图构建方法。所提模型的图拓扑结构是在训练过程之前确定的，这可能会导致非最佳性能。此外，TFE-GNN无法应对每个数据包的原始字节中隐含的字节级噪声。（2） 字节序列中隐含的未使用的时间信息。字节级 trafc 图的构造没有引入字节序列的显式时间特征。\n","date":"2024-02-16T16:28:03+08:00","permalink":"http://localhost:1313/p/tfe-gnn/","title":"TFE GNN"},{"content":"导语 ucas2023秋 林璟锵、马存庆 网络认证技术笔记\n说是开课以来从未有过挂科选手，但是想得不错的分数还是要努努力，进自己脑子的知识才是最好的知识\n笔记 网络认证技术 ≈ 密码学+计算机网络\n网络认证：在信息系统/网络环境中，实现身份的确认。目标：在不可信的网络环境中确认主体是谁，有什么属性、权限、能力\n身份确认的主体：人、设备、软件服务……\nPKI：公钥基础设施\nCA：认证中心，生成数字证书\nCA是PKI的核心组成成分，但是在很多地方把CA和PKI混用了。\n考试用 两个半小时\n题型：\n判断 2分 简答 5-8分 建议不要空这 关键是原理之类的要记住的\n01 导言（意义不大） 相关概念\n02 密码学基础（会涉及题目，需要复习复习） 对称：加解密\n非对称：签名\n光看密钥长度不能知道强度，RSA1024bits=ECC160bits。短密钥可以达到高强度\n哈希：验证\n消息鉴别码：MAC=C(K,M)，K为密钥M为消息，把密钥跟着一块哈希了\n可鉴别加密CCM、GCM、AEAD（简单了解）\n国外的密码基本原理不细说了\n国产的了解一下\nSM2 非对称 ECC \u0026mdash; 椭圆曲线 知道基于椭圆曲线域上的离散对数困难问题。 替换RSA\nSM3 哈希 256bit和sha256差不多 分组长度512bit，摘要值长度256bit\nSM4 分组工作模式\nECB：对每个块独立加密：明文同样的块会加密成同样的密文\nCBC：明文先与上一个密文异或在加密，需要初始化向量\nOFB：将块密码转为流密码，生成密钥流的块\nCTR（ICM、SIC）：将块密码变为流密码，通过递增加密计数器产生密钥流\nZUC 流密码\n128位的初始密钥key和128位的初始向量iv来作为输入。每个时钟周期能生成32bit\n共享密钥问题-\u0026gt;为什么要有非对称的原因-\u0026gt;数字签名\n03 口令鉴别 client 用复杂口令，不要告诉别人，次数限制\n传输 使用已被验证的安全信道\nserver 存储，验证\n04 基于密码技术的鉴别 两大类：\n对称： 有没有密钥\n提一个协议框架，让你看有没有什么错，一些参数有什么用【用什么方式可以抵抗什么攻击】\nreplay attack（重放攻击）：通过加一个nonce抵抗\noracle session attack（就是攻击者使另一方帮自己来计算）：让u和v不同。比如u为加密，v为解密，被挑战方只能加密，就不能被当成解密服务器了。\nParallel Session attack：p(), q()与方向有关。从而攻击者不能利用服务器的计算。比如发起者会加一个xor，被挑战者会加一个左移\noffset attack：\n把返回的东西改为E(f()#E(g))\n可信第三方，kobras\n非对称：数字签名和验证\n单向（带一个时间戳之类的约定好的东西）、双向（A发给B后B还要发给A）\nPPT标红好好看看\n05+06 PKI技术 CA：认证机构，权威第三方，公钥（证书）可信发布[根CA、子CA]\nRA：注册机构，审查信息，防止CA职能太多导致一个出问题导出都出问题\nrepository（存数据的吧）\nCRL（Certificate Revocation List，证书撤销列表）\nOnline Certificate Status Protocol（OCSP）一种通信协议，专门用于检查证书是否已经被撤销 相应的服务器称为OCSP Server-\u0026gt;（证书有三种状态）Good、Revoked、Unknown ：未撤销、已经撤销、未知\nASN.1-基本数据类型-DER编码-sequence-implicit/explicit tag 稍微看一下\n07 证书拓展 证书基本域\n证书扩展域 X.509版本3 18种，了解功能即可\n拓展有关键和非关键，如果关键出了错（识别不出来），直接认定证书非法。非关键出错则忽略拓展\nBasic Constraints：区分是否是CA证书（能否签发其他证书）以及路径的深度（说明CA可以有多少层次的下级）\nAuthority Key Identifier：证书链中可能有多个公钥，这个确定哪个是用来验证证书的颁发者（CA）的公钥\nSubject Key Identifier：证书链中可能有多个公钥，确定哪个是证书自己的公钥\nKey Usage：密钥的用途。7种+2种辅助用途\nPrivate Key Usage Period：给出证书有效的开始到结束的时间\nIssuer Alternative Name：放置签发者（CA）的消息（DN存放CA信息，子CN没法用DN，就用这来放）\nSubject Alternative Name：放置证书拥有者的消息\nSubject Directory Attributes：可加入任何与Subject有关的信息，例如，民族、生日等\nName Constraints：限制下级CA所能够签发证书的订户的名字空间（只在下级CA中有用）\nCertificate Policies（CP）：区分不同证书的安全等级\nInhibit Any-Policy：（CP的Any-Policy指对于该CA所签发的订户证书的CP没有限制），值是整数N，表示：在证书路径中，本证书之下的N个证书可带有Any-Policy的证书\nPolicy Mappings：说明了不同CA域之间的CP等级的相互映射关系\nPolicy Constraints：对于证书认证路径的策略映射过程中，有关CP的处理，进行限制。N：在N个证书后，不允许再进行策略映射；M：在M个证书后，就必须要有认识的、明确的CP\nExtended Key Usage：证书/密钥可用的用途（拓展）\nCRL Distribution Points：和应用系统约定在哪儿获取CRL\nFreshest CRL：增量CRL情况下，获取最新的增量CRL的地址\nAuthority Information Access：如何在Internet上面，访问一些CA的信息（目前只有 1、上级CA的情况 2、OCSP服务器的情况两个信息）\nSubject Information Access： l如何在Internet上面，访问一些用户的信息 （目前只有 1、资料库的地址（针对CA）2、TSA服务地址（针对TSA服务器））\n08 PKI信任体系 信任模型\n单根CA\n多根CA 根之间要互相通信-CTL（用户自主+权威发布）沟通方式、原理、优缺点，应用\nCTL（信任锚）由权威机构统一地发布1个可信的信任锚列表（Certificate Trust List）包括多个根CA证书文件的HASH结果和受信任CA对其签名\n【信任锚里有根CA证书的hash、其他CA证书、CRL、信任策略和规则等。然后由一个我信任的CA对CTL签名，一般不用CTL里信任的CA来签名。】\n方式：1、不同PKI域用同一个CTL 2、加一个ACA的信任锚说明哪些根CA是可以信任的\n交叉认证-网状mesh-\u0026gt;桥CA\n相当于将对方CA认作是我的子CA。\nmesh-\u0026gt;信任链变成信任网\n桥CA-\u0026gt;不同域之间的证书传递\n09 证书撤销 验证签名-验证有效期-验证撤销状态\n撤销状态 CRL、OCSP、CRT 原理\nCA/CRL Issuer定期地签发CRL CRL，certificate revocation list\n完全CRL－Complete CRL：所有CRL信息一次发布\n增量CRL－Delta CRL：发布新增的CRL信息发布\n直接CRL－Direct CRL：证书签发者签发CRL\n间接CRL－Indirect CRL：使用CRL issuer签发CRL\nOCSP在线证书状态协议 Online Certificate Status Protocol 在线服务器\nCRT：证书撤销树，对于各证书序列号进行一定的结构化，形成了HASH链\n使用了merkle hash tree【区块链信任算法】\n拿加粗的子哈希算哈希，就可以推出根hash，验证起来需要更少的那啥\n10 TLS handshake怎么shake的\n增加一个server对client的鉴别\n如果server证书只能签名不能加密，则要生成一个临时公钥，签名后发给client【ServerKeyExchange】\n两张图里的消息有什么含义 1.3和1.2的区别\n直接在client hello中发了选择算法的key（用server 公钥加密）\n10.5wifi认证 WPA-PSK共享口令 （路由器上做）\nWPA-802.1X 基于账号的身份鉴别 （身份鉴别server）\n客户端或网页 （微信、短信）\n11 不考 12 PKI安全增强 入侵容忍 解决了什么问题？怎么解决的 原理 解决了在入侵场景下的高可用。黑客侵入了其中一个PKI节点无法获利，同时PKI系统任然保持可用性\n【门限密码学：把密钥分成L份，当有其中f+1份时可以解密，否则解密不了】\neg. Shamir\u0026rsquo;s Secret Sharing 基于拉格朗日插值法\neg2. ITTC 基于离散对数的子密钥分配\nserver上用密码算，CA来整合\n也就是说黑客就算攻入了一个节点，他仍然无法获取PKI用来签名证书的私钥，同时其他节点还能继续工作。\n信任增强 解决方式原理 信任机制基本假设：1、CA行为不会出错，证书中的信息不会出错【只有可能是错误操作导致的签发给错误的人】 2、无限制权利\n三个思路：\n1、 浏览器端实施检测：\n（1）浏览器维护证书信息\n（2）多个会话之间互相比较\n2、限制CA权利\n（1）假定server只会向同一个国家的CA申请证书\n（2）限定CA能签发的顶级域名范围\n（3）域名拥有者可以控制哪个CA给他签发证书\n（4）server再次确认机制：\nserver在多一个sovereign Key的公私钥对挂在timeline上，浏览器看到timeline上有sovereign Key，会要求server再次拿sovereign Key私钥签名，黑客控制了CA，却无法获取server的sovereign Key私钥，因此仍然无法伪造身份\n3、证书透明化：\n假定CA也会出错，审计CA\n13 证书透明化 虚假证书：证书可以被严格验证通过，但是证书对应的私钥并不被证书主体拥有，而是被其他人拥有（CA被人黑了，一顿乱发）\n透明化增加哪些步骤SCT相关特点弄清楚一点\n增加\n公开日志服务器（Public Log Server）：保存和维护记录证书的公开日志（Public Log）\n收到证书并验证通过后，公开日志服务器会向提交者返回一个凭据（SCT）Signed Certificate Timestamp。（有可能多个公开日志服务器，就会返回多个SCT）用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）\n用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展） 怎么获得SCT呢？ 1.从X.509证书扩展项获得SCT 2.从连接建立时TLS扩展项获得SCT -\u0026gt;TLS客户端要支持 3.从OCSP stapling的扩展项获得SCT 下面不重要\n监视员（Monitor）：周期性的访问公开日志服务器，寻找和发现可疑的证书 审计员（Auditor）：审计公开日志的行为 14 隐式证书 传统和隐式的结构和使用的区别\n在带宽、计算能力、存储资源有限制的环境下，隐式证书是传统X.509证书的一种高效替代\nX.509证书基本内容：订户身份信息+公钥数据+CA数字签名\n隐式证书基本内容：中间公钥数据$P_U$ + 订户身份标识。 最终公钥 P=$P_{CA}$+$P_U$以及身份信息也有关 $P_{CA}$：CA证书公钥\n使用：\nX.509： 需要对订户证书进行CA签名的验证\n隐式证书：需要重构出订户公钥，在对消息的验签时同时完成对证书本身的验证\n隐式证书中，没有对CA数字签名的验证，取而代之的是，重构公钥的计算，后者的计算量较小。\n假名证书不考\n15 kerberos 可信第三方TTP，基于对称密码，也支持在某些过程使用非对称\n获得一个TGT，用TGT和要访问的目，请求问kerberos服务器，来获取访问目标的票据（不是TGT，TGT只是告诉kerberos我已经被验证过了）\nkerberos票据流程\n长期密钥（主密钥）Long-term Key/Master Key： 长期保持不变的密钥。被长期密钥（主密钥）加密的数据尽量不在网络上传输。（防止暴力破解、分析）\n短期密钥（会话密钥）Short-term Key/Session Key： 加密需要进行网络传输的数据。只在一段时间内有效，即使被加密的数据包被黑客截获并破解成功后，这个Key早就已经过期了。\nKDC（Key Distribution Center）：kerberos server作为可信第三方，维护所有帐户（client、server）的注册信息、用户名、口令、用户主密钥、服务器主密钥\nServer 与Client之间基于共享秘密短期密钥key实现身份鉴别\nKDC仅仅是允许进入应用系统，至于有什么权限、由应用系统自主决定\n获取TGT：\nclient发请求，KDC用client的master key加密一个会话密钥$S_{KDC-Client}$，用KDC的master key加密TGT，TGT里包含会话密钥和client信息（让client 鉴别KDC是KDC而非被伪造）\n获取ST：\n这个图有问题，KDC还给client的不是用clinet的master key，而是用session key。\n当client要访问server的时候，给KDC自己的TGT和要访问的server。\nKDC根据TGT来对client进行认证，生成$S_{Server-Client}$和ST(session ticket)\n$S_{Server-Client}$：用client的主密钥加密一个会话密钥，\nST：用server的主密钥加密，ST包含会话密钥和client的信息。\n将这两个被加密的Copy一并发送给Client\nclient得到会话密钥后，用session key解密，创建Authenticator（Client Info + Timestamp）并用会话密钥加密\nclient将ST和Authenticator访问server，server用自己的主密钥解密ST得到会话密钥，在用会话密钥解密Authenticator，比较Authenticator里的client info和ST里的client info来确定client就是client\n那如果TGT没过期，session key过期了呢？可以用TGT再申请一个，因为TGT用KDC的master key加密，KDC可以得到旧的session key和client info，进而再发一个session key。由于session key是TGT的一部分，这其实也就相当于重新申请了TGT\nclient鉴别server（双向鉴别）：\n在Authenticator里在加一个flag要求server自证。\nserver看到后，用ST里得到的会话密钥解密Authenticator，把里面的timestamp用会话密钥加密发给client\n16 OAuth\u0026amp;OIDC 单点登录(Single Sign on)在某个地方认证了之后，在整个域里都不用再认证了。\nSSO 口令记录器-\u0026gt;保存在edge/chrome\nOAuth 协议流程图，理解认证的流程，有那几个角色，分别做了什么\nOIDC 协议流程图，理解认证的流程\n17 FIDO 在服务器端将用户与移动终端的可信环境进行身份绑定 将用户与服务器之间的直接鉴别转变为两段式鉴别 1 移动终端鉴别用户主要是靠生物特征 2 服务器端鉴别移动终端主要是靠数字签名\n","date":"2023-12-27T19:07:10+08:00","permalink":"http://localhost:1313/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","title":"网络认证技术笔记"},{"content":"导语 ucas 2023秋 自然语言处理基础 胡玥、曹亚男\n期末全是开放问题，因此弄清楚各种模型的优劣非常有必要。\n笔记 model 常见的模型有DNN、CNN、RNN、GNN、LSTM、\ntask NLP的经典问题有\n在课程中，我们主要学习了\n属性抽取（AE） opinion target和aspect的区别：opinion target是被评价对象，aspect是对象的属性\neg\u0026quot;这个手机的摄像头很出色，但电池寿命较短。\u0026ldquo;手机是opinion target，而摄像头和电池寿命是手机的两个aspect。\n目标：抽取对象。eg：华为技术遥遥领先！-\u0026gt; 抽取出“华为”\nAspect Term Extraction 论文阅读（一） - 知乎 (zhihu.com)\nTHA是用了attention机制的LSTM，用于获得上文（单向）已经标注过的aspect的信息，来指导当前aspect标注。\nSTN是LSTM，用于获得opinion的摘要信息。首先，STN单元获得基于给定aspect的opinion的表示，接下来利用attention机制来获得基于全局的opinion的表示。自此就可以获得基于当前aspect的opinion摘要。将aspect的表示和opinion摘要拼接作为特征，用于标注。\n（表示就是一个框里三个圆圆）\n将ATE形式化为一个seq2seq的学习任务。在这个任务中，源序列和目标序列分别由单词和标签组成。为了使Seq2Seq学习更适合ATE,作者设计了门控单元网络和位置感知注意力机制。门控单元网络用于将相应的单词表示融入解码器，而位置感知注意力机制则用于更多地关注目标词的相邻词。\ndecoder包含一个门控单元，用于控制编码器和解码器产生的隐状态。当解码标签时，这个门控单元可以自动的整合来自编码器和解码器隐状态的信息。\nmasked seq2seq。首先，对输入句子的连续几个词进行掩码处理。然后，encoder接收部分掩码的句子及其标签序列作为输入，decoder尝试根据编码上下文和标签信息重建句子原文。要求保持opinion target位置不变\n观点抽取（OE） 一般都是先抽取aspect，在对aspect进行情感预测的流水线方式\nIMN使用非流水线方式。与传统的多任务学习方法依赖于学习不同任务的共同特征不同，IMN引入了一种消息传递体系结构，通过一组共享的潜在变量将信息迭代地传递给不同的任务\n它接受一系列tokens{x1，…，xn}作为特征提取组件fθs的输入，该组件在所有任务之间共享。该组件由单词嵌入层和几个特征提取层（好多个CNN）组成。输出所有任务共享的潜在向量{hs1，hs2，…，hsn}的序列。该潜在向量序列会根据来自不同任务组件传播来的信息来更新。\n$hi^{s(T)}$ 表示为t轮消息传递后Xi对应的共享潜在向量的值。\n共享潜在向量序列用作不同任务特定组件的输入。每个特定于任务的组件都有自己的潜在变量和输出变量集。输出变量对应于序列标签任务中的标签序列；在AE中，我们为每个令牌分配一个标签，表明它是否属于任何aspect或opinion，而在AS中，我们为每个单词加上它的情感标签。在分类任务中，输出对应于输入实例的标签：情感分类任务(DS)的文档的情感，以及领域分类任务(DD)的文档域。在每次迭代中，适当的信息被传递回共享的潜在向量以进行组合；这可以是输出变量的值，也可以是潜在变量的值，具体取决于任务。 此外，我们还允许在每次迭代中在组件之间传递消息（opinion transmission）。\n感觉有点训练词向量的感觉，像是预处理一下得到向量序列来方便其他任务。\n【超，好像这些都不是考试重点】\n属性级情感分类 For exam 试卷题型：简答题 40 分（5*8）好多个问号（内容为胡老师讲的基础部分）+ 综合题 60 分（内容为曹老师讲的核心应用部分）\n简答题重点章节：\n什么是语言模型、神经网络语言模型、几种、特点（优点）\n概念性的简答题， 不难+\n第4章 语言模型+词向量 （要求掌握：语言模型概念，神经网络语言模型 ）\n第 5章 NLP中的注意力机制 （全部要求掌握）概念、用处\n第 7 章 预训练语言模型（全部要求掌握）[主要掌握GPT，BERT 是 怎么训练的，与下游任务是如何对接的]prompt，inconcert learning，思维链【建模的几种范式】\n主观题重点章节： 设计东西\n第9章 情感分析（要求掌握：方面级情感分析基本方法原理）\n第10章 信息抽取 （要求掌握：实体和关系联合抽取基本方法原理）\n第 11章 问答系统（要求掌握：检索式问答系统基本方法原理）\n语言模型概念 神经网络语言模型 统计的方法使用最大似然估计，需要数据平滑否则会出现0概率问题。\n神经网络使用DNN和RNN\n利用RNN 语言模型可以解决以上概率语言模型问题，在神经网络一般用RNN语言模型\n一些（我不会的）背景知识 梯度下降算法 梯度下降法是一种常用的优化算法，主要用于找到函数的局部最小值。它的基本思想是：在每一步迭代过程中，选择函数在当前点的负梯度（即函数在该点下降最快的方向）作为搜索方向，然后按照一定的步长向该方向更新当前点，不断迭代，直到满足停止准则。\n具体来说，假设我们要最小化一个可微函数$f(x)$，我们首先随机选择一个初始点$x_0$，然后按照以下规则更新$x$：\n$$ x_{n+1} = x_n - \\alpha \\nabla f(x_n) $$\n其中，$\\nabla f(x_n)$是函数$f$在点$x_n$处的梯度，$\\alpha$是步长（也称为学习率），控制着每一步更新的幅度。\n梯度下降法只能保证找到局部最小值\n双曲正切函数 它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在\n$\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\nBP算法 反向传播算法，当正向传播得到的结果和预期不符，则反向传播，修改权重\nBPTT算法 是BP算法的拓展，可以处理具有时间序列结构的数据，用于训练RNN\nBPTT的工作原理如下：\n正向传播 ：在每个时间步，网络会读取输入并计算输出。这个过程会持续进行，直到处理完所有的输入序列。 反向传播 ：一旦完成所有的正向传播步骤，网络就会计算最后一个时间步的误差（即网络的预测与实际值之间的差距），然后将这个误差反向传播到前一个时间步。这个过程会持续进行，直到误差被传播回第一个时间步。 参数更新 ：在误差反向传播的过程中，网络会计算误差关于每个参数的梯度。然后，这些梯度会被用来更新网络的参数。 one-hot编码 独热编码是一种将离散的分类标签转换为二进制向量的方法\n假设我们要做一个分类任务，总共有3个类别，分别是猫、狗、人。那这三个类别就是一种离散的分类：它们之间互相独立，不存在谁比谁大、谁比谁先、谁比谁后的关系。\n在神经网络中，需要一种数学的表示方法，来表示猫、狗、人的分类。最容易想到的，便是以 0 代表猫，以 1 代表狗，以 2 代表人这种简单粗暴的方式。但这样会导致分类标签之间出现了不对等的情况。（2比1大……）\n而进行如下的编码的话就可以解决这个问题：\n猫：[1, 0, 0] 狗：[0, 1, 0] 人：[0, 0, 1] 这就是独热码\npairwise \u0026ldquo;Pairwise\u0026quot;是一种常用于排序和推荐系统的方法。它的主要思想是将排序问题转换为二元分类问题。每次取一对样本，预估这一对样本的先后顺序，不断重复预估一对对样本，从而得到某条查询下完整的排序。如果文档A的相关性高于文档B，则赋值+1，反之则赋值-1。这样，我们就得到了二元分类器训练所需的训练样本\nPairwise方法也有其缺点。例如，它只考虑了两篇文档的相对顺序，没有考虑他们出现在搜索结果列表中的位置。\n除了Pairwise，还有其他的方法如Pointwise和Listwise。Pointwise方法每次仅仅考虑一个样本，预估的是每一条和查询的相关性，基于此进行排序。而Listwise方法则同时考虑多个样本，找到最优顺序。这些方法各有优缺点，选择哪种方法取决于具体的应用场景和需求。\nzero-shot \u0026ldquo;Zero-shot learning\u0026rdquo;（零样本学习）是一种机器学习范式，它允许模型在没有先前训练过相关数据集的情况下，对不包含在训练数据中的类别或任务进行准确的预测或推断。这种能力是由先进的深度学习模型和迁移学习方法得以实现的。\n举个例子，假设我们的模型已经能够识别马，老虎和熊猫了，现在需要该模型也识别斑马，那么我们需要告诉模型，怎样的对象才是斑马，但是并不能直接让模型看见斑马。所以模型需要知道的信息是马的样本、老虎的样本、熊猫的样本和样本的标签，以及关于前三种动物和斑马的描述。\n这种方法的优点是可以极大地节省标注量。不需要增加样本，只需要增加描述即可。\nPPO PPO（Proximal Policy Optimization，近端策略优化）是一种强化学习算法，由OpenAI在2017年提出。PPO算法的目标是解决深度强化学习中策略优化的问题。\nPPO算法的核心在于更新策略梯度，主流方法有两种，分别是KL散度做penalty，另一种是Clip剪裁，它们的主要作用都是限制策略梯度更新的幅度，从而推导出不同的神经网络参数更新方式。\nPPO算法具备Policy Gradient、TRPO的部分优点，采样数据和使用随机梯度上升方法优化代替目标函数之间交替进行，虽然标准的策略梯度方法对每个数据样本执行一次梯度更新，但PPO提出新目标函数，可以实现小批量更新。\nDNN（NNLM） 2-gram（bigram） 其中$\\theta$就是训练过程中要学习的参数，有了这些参数就可以直接的到 $p(w_i|w_{i-1})$， 找到一组足够好的参数，就能让得到的$p(w_i|w_{i-1})$最接近训练语料库\n这里的最大化就是损失函数最小（最接近0），因为P永远小于1，所以log永远是负数，他们加起来永远小于0，让log最大，也就是让log最接近0\nn-gram 拓展一下罢了\nRNN（RNNLM） 词向量 自然语言问题要用计算机处理时，第一步要找一种方法把这些符号数字化，成为计算机方便处理的形式化表示\nNNLM模型词向量 RNNLM模型词向量 C\u0026amp;W 模型词向量 CBOW 模型词向量 Skip-gram模型词向量 不同模型的词向量之间的主要区别在于它们捕获和编码词义和上下文信息的方式。以下是一些常见模型的词向量特点：\n神经网络语言模型（NNLM） ：NNLM通过学习预测下一个词的任务来生成词向量。这种方法可以捕获词义和词之间的关系，但是它通常无法捕获长距离的依赖关系，因为它只考虑了固定大小的上下文。 循环神经网络语言模型（RNNLM） ：RNNLM使用循环神经网络结构，可以处理变长的输入序列，并能捕获长距离的依赖关系。因此，RNNLM生成的词向量可以包含更丰富的上下文信息。 Word2Vec ：Word2Vec是一种预训练词向量的方法，它包括两种模型：Skip-gram和CBOW。Skip-gram模型通过一个词预测其上下文，而CBOW模型则通过上下文预测一个词。Word2Vec生成的词向量可以捕获词义和词之间的各种关系，如同义词、反义词、类比关系等。 GloVe ：GloVe（Global Vectors for Word Representation）是另一种预训练词向量的方法，它通过对词-词共现矩阵进行分解来生成词向量。GloVe生成的词向量可以捕获词义和词之间的线性关系。 BERT ：BERT（Bidirectional Encoder Representations from Transformers）使用Transformer模型结构，并通过预训练任务（如Masked Language Model和Next Sentence Prediction）来生成词向量。BERT生成的词向量是上下文相关的，也就是说，同一个词在不同的上下文中可能有不同的词向量。 总的来说，不同模型的词向量之间的区别主要在于它们捕获和编码词义和上下文信息的方式。选择哪种词向量取决于具体的任务需求和计算资源。\nNNLM的词向量 解决办法\n通过一个|D| * |V|的矩阵，额可以将one-shot的编码转为D维的稠密的词向量，所以管他叫lookup表\nNNLM 语言模型在训练语言模型同时也训练了词向量\nRNNLM的词向量 C\u0026amp;W C\u0026amp;W模型是靠两边猜中间的一种模型，输入层是wi上下文的词向量\nscore是wi中间这个word在这个位置有多合理，越高越合理。\n正样本通常是指在实际语料库中出现过的词语及其上下文。负样本则是人为构造的，通常是将一个词与一个随机的上下文配对。\nPairwise方法在训练C\u0026amp;W词向量时，主要是通过比较一对词的上下文来进行训练的。具体来说，对于每一对词（一个正样本和一个负样本），我们都会计算它们的词向量，并通过比较这两个词向量的相似度来更新我们的模型。\n在训练过程中，我们首先需要选择一个损失函数，这里是修改后的HingeLoss\n然后，我们会使用一种优化算法来最小化这个损失函数，这里是梯度下降，在每一次迭代中，我们都会根据当前的损失来更新我们的词向量。\n训练的目标是在正样本中的score高，负样本的score低，然后score差的越大效果越好\nCBOW CBOW也是靠两边猜中间，输入层是wi上下文词向量的平均值，目标是最小化（最收敛与0）上下文词的平均与目标词之间的距离。输出是\nskip-gram skip-gram是知道中间猜两边，训练最小化（最收敛于0）目标词与上下文词之间的距离。\n注意力机制 概述 在注意力机制中，Q、K、V 分别代表查询（Query）、键（Key）和值（Value）。\n注意力机制的工作过程可以简单概括为：对于每一个查询，计算它与所有键的匹配程度（通常使用点积），然后对这些匹配程度进行归一化（通常使用 softmax 函数），得到每个键对应的权重。最后，用这些权重对所有的值进行加权求和，得到最终的输出。\n这种机制允许模型在处理一个元素时，考虑到其他相关元素的信息，从而捕捉输入元素之间的依赖关系。在自然语言处理、计算机视觉等领域，注意力机制已经被广泛应用，并取得了显著的效果。\nW是权重，都是学来的。\n参考\nKQV矩阵： https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web\u0026vd_source=2fbfeabcc6cdd857dcd6247eb0154d83\nAttention机制： https://www.bilibili.com/video/BV1YA411G7Ep\nK、V都是经过线性变换的词向量集合（矩阵）\nQ是隐藏状态（隐藏向量）\nA是一个注意力值，就是我们设置的这个字的注意力值\n通过attention的学习，可以得到a1、a2……，这些就是K中各个向量对Q的权重\n步骤1：计算 f ( Q ,Ki )\n步骤2：计算对于Q 各个 Ki 的权重\n步骤3：计算输出 Att-V值（各 Ki 乘以自己的权重，然后求和 ）\n举例1， seq2seq（RNN2RNN）的机器翻译中 seq2seq做机器翻译的过程，需要大量的两种语言的平行语料，就是意思相同的语言的一一对应的关系。\n其中x为词向量，A为权重矩阵，h为隐藏状态（隐藏向量）。\nRNN是用预训练的词向量，然后通过学习权重矩阵A来微调，得到隐藏状态，可以理解为隐藏状态是带了上下文的更加符合RNN的词的向量表示。\n每一个时间步中，A都被微调， 因此x1、x2、x3的A可能都是不一样的。在大量预料的训练下会获得表现比较好的A和A'\nh可以表示为$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$其中$h_t$ 是当前时间步的隐藏状态，$h_{t-1}$ 是前一时间步的隐藏状态，$x_t$ 是当前时间步的输入，$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b_h$ 是偏置项，$f$ 是激活函数\n不加注意力机智的seq2seq模型，encoder是RNN，decoder也是RNN，在encoder接受了$x_1$到$x_m$的词向量序列后，得到最终的隐藏状态$h_m$， 也就是$s_0$，作为decoder的初始状态。\n如果不加注意力机制，decoder那边也就是靠隐藏状态、x和参数（A矩阵，偏置值b）来继续进行RNN的步骤。\n现在我们引入注意力机制，也就是图上的$c_0$，权重的计算按照上面所说的KQV计算方法，这里K是词向量集合x1,x2\u0026hellip; Q是隐藏状态。也就是对于每一个隐藏状态，都可以求一个关于词向量序列的权重值$\\alpha$。\n通过求出这一系列的$\\alpha$，就可以加权求出上下文矩阵$c$，c知道当前隐藏状态和词向量矩阵的全部关系。\n加了注意力机制之后，decoder的各个隐藏状态求解过程就会向之前提到的那样变得更复杂\n而每一个步骤的c都不一样，比如\nc0是s0对于x1,x2\u0026hellip;的att-V，也就是hm对于x1,x2\u0026hellip;的att-V；c1是隐藏状态s1对于x1,x2\u0026hellip;的att-V，c2是隐藏状态s2对于x1,x2\u0026hellip;的att-V这些c都需要花算力来算\n注意力机制的问题是时间复杂度太大了。如果是简单的RNN2RNN，如果encoder词向量矩阵大小为m，decoder词向量矩阵大小为n，所需的时间复杂度为O(m+n)，而使用注意力机制之后就会变成O(mn)，还是打分函数比较简单的情况下。\n注意力编码机制 attention机制还可以将不同序列融合编码（将多个序列经过某种处理或嵌入方式，转换为一个固定长度的向量或表示形式。）\n就是给每个词向量乘个权重加起来，被称作注意力池化（Attention Pooling）或加权求和（Weighted Sum）。这个操作的含义是将注意力权重分配给输入序列中的不同部分，从而形成一个汇聚了注意力的向量表示。\n这个操作的效果是聚焦于输入序列中具有更高注意力权重的部分，形成一个综合的表示，其中对于重要的部分有更大的贡献。这对于处理序列数据中的上下文信息，关注重要元素，以及实现对不同部分不同程度的关注都非常有用，特别是在自然语言处理中的任务中。\n预训练语言模型 迁移学习 迁移学习（Transfer Learning）是一种机器学习方法，其核心思想是利用已有的知识来辅助学习新的知识。例如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#。\n迁移学习通常会关注有一个源域（源任务） $D_ {s}$ 和一个目标域（目标任务） $D_ {t}$ 的情况.\n迁移方式分为两种\n几个范式 第三范式：预训练-精调范式 自回归：预测序列的下一个或者上一个\n自编码：预测序列中的某一个或某几个\n广义自回归：和自回归主要区别在于他们处理输入数据的方式。自回归预训练语言模型在生成序列时，会一个接一个地生成新的词，每个新词都依赖于前面的词。如GPT，而广义自回归预训练语言模型则更为灵活，它们可以在生成序列时考虑更多的上下文信息，模型不仅可以查看前面的词，还可以查看后面的词或者整个序列。如XLNet\nGPT训练和对接 GPT 采用了 Transformer 的 Decoder 部分，并且每个子层只有一个 Masked Multi Self-Attention（768 维向量和 12 个 Attention Head）和一个FeedForward （无普通transformer解码器层的编码器-解码器注意力子层），模型共叠加使用了 12 层的 Decoder。使用了从左向右的单向注意力机制\nMasked Multi Self-Attention的768维向量和12个attention head： 意思是12个独立的attention组件，每个组件的参数都独立，然后每个attention的Q向量都是768维，也可以理解为一个词在模型中的向量（或者说词嵌入）是768维]\nfeedforward： 作用是提取更深层次的特征。在每个序列的位置单独应用一个全连接前馈网络，由两个线性层和一个激活函数组成。线性层将每个位置的表示扩展，为学习更复杂的特征提供可能性，激活函数帮助模型学习更复杂的非线性特征，第二个线性层将每个位置的表示压缩回原始维度。这样，位置特征敏感的部分就会被表达出来，提供给后续网络学习。\n就是十二个下图这样的小东西\ntransformer输入有token embedding和position embedding\n对比一下transformer，transformer的decoder是6个右边的，少了一层multi-head attention的encoder-decoder注意力子层（cross-attention的那个子模块）\n6层attention堆叠就是六个encoder就是个小的encoder，每个encoder里都有attention机制，上图N=6的意思。\n训练：\nmaximize负数=近0最小化\n与下游任务对接：\n把多序列通过一些特定的规则拼成一个单序列。\n微调：\n任务微调有2种方式 ：① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务\n举例：\n这儿的$L_1(C)$是上面提到的预训练过程中的\nBERT训练和对接 用了transformer的encoder再加FFN（前馈神经网络，FFN 层有助于学习序列中的非线性关系和模式）层\n【但是transformer的encoder不是带FeedForward吗？】FFN仅在MLM过程中有用，而BERT的最终输出是模型在整个预训练过程中学到的表示的某种组合。这些表示在后续的任务中可以进一步微调或者用作特征。（BYD，原来只是训练过程中的一个b东西）\n下图中一个trm是一个子层，\n在BERT模型中，输入的每个单词都会通过三种嵌入（embedding）进行编码\nToken Embedding：是将每个单词或者词片映射到一个向量，这个向量能够捕获该单词的语义信息。在BERT中，使用了WordPiece标记化，其中输入句子的每个单词都被分解成子词标记。这些标记的嵌入是随机初始化的，然后通过梯度下降进行训练。\nSegment Embedding：是用来区分不同的句子的。在处理两个句子的任务（如自然语言推理）时，BERT需要知道每个单词属于哪个句子。\nPosition Embedding：由于Transformer模型并没有像循环神经网络那样的顺序性，因此需要显式地向模型添加位置信息，以保留句子中单词的顺序信息\n训练：\nMLM：把一个序列的几个word给mask了让模型猜的训练方法。\n(2).句子顺序模型训练\n凑一些下一句不是下一句的负样本来训练预训练模型对句子顺序的敏感。\n对接：\n微调同样有两种① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务\n其他 RoBERTa：把BERT使用Adam默认的参数改为使用更大的batches，训练时把静态mask改为动态mask。\nBART：GPT只用了transformer的decoder，BERT只用了transformer的encoder。导致\nBERT具备双向语言理解能力的却不具备做生成任务的能力。GPT拥有自回归特性的却不能更好的从双向理解语言.\n（模型的\u0026quot;自回归\u0026quot;特性指的是，当前的观察值是过去观察值的加权平均和一个随机项）\nBART使用标准的Transformer结构为基础，吸纳BERT和GPT的优点，使用多种噪声破坏原文本，再将残缺文本通过序列到序列的任务重新复原（降噪自监督）\nBERT在预测时加了额外的FFN, 而BART没使用FFN.\n（还记得这个Beyond吗）\nT5\n给整个 NLP 预训练模型领域提供了一个通用框架，把所有NLP任务都转化成一种形式(Text-to-Text)，通过这样的方式可以用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务。以后的各种NLP任务，只需针对一个超大预训练模型，考虑怎么把任转换成合适的文本输入输出。\n第四范式：预训练，提示，预测范式（Pre-train,Prompt,Predict） prompt挖掘工程\n特点：不通过目标工程使预训练的语言模型（LM）适应下游任务，而是将下游任务建模的方式重新定义（Reformulate），通过利用合适prompt实现不对预训练语言模型改动太多，尽量在原始 LM上解决任务的问题。\n实现方法eg：\n要素：\n输入端\nprompt工程\n完形填空和前缀提示\n模板创建\n输出端\n方法\n微调\n生成类任务用法与第五范式相同\n第五范式：大模型 大语言模型 (Large Language Model，LLM) 通常指由大量参数（通常数十亿个权重或更多）组成的人工神经网络预训练语言模型，使用大量的计算资源在海量数据上进行训练。\n大型语言模型是通用的模型，在广泛的任务（例如情感分析、命名实体识别或数学推理）中表现出色，具有与人类认证对齐的特点。\n不需要任务模型的意思是只要有预训练就行\n（我靠，这要传统注意力算死了）\n学习方法\n因为上下文学习，在使用的时候也可以用zero-shot, one-shot和few-shot。\nchain-of-thought\n与人类对齐：RLHF\n简而言之：1、在人工标注数据上SFT（有监督微调）模型\n2、多模型给标注人员做排序，用来训练奖励模型（RM）\n3、使用强化学习PPO算法，交互地优化模型参数。\n文本分类在各个范式上的例子\n方面级情感分类 方面级情感分类（Aspect-Level Sentiment Classification）是自然语言处理（NLP）中的一个任务，它的目标是识别文本中特定方面的情感倾向。例如，在产品评论中，“这款手机的电池寿命很长，但屏幕质量差。”这句话中，“电池寿命”这个方面的情感是积极的，而“屏幕质量”这个方面的情感是消极的。所以，方面级情感分类不仅要识别出文本中的各个方面，还要判断这些方面的情感倾向。这个任务在许多领域都有应用，比如产品评论分析、社交媒体监控等。\n问题定义\n基本方法、原理 子任务等：\nEntity/Target：评论的对象或者物品是什么，例如某个餐厅，某款手机。\u0026ldquo;Target\u0026quot;这个词用的比较模糊，其既可以被当作Entity，又可以当作Aspect Term。和在AE里提到的opinion target是一个意思。\nAspect：隶属于某个Entity的属性。在这里其因为学者提出的任务类型不同，又分为两类：\nAspect Term：存在在句子中的Aspect。例如例句中的”拍照“、”电池“、”外观“。 Aspect Category：预先给定的Aspect。例如，我们想知道评论对”华为手机“的”外观“、”售后服务“、”便携性“三个aspect的情感极性。 LSTM LSTM 方法先将所有变长的句子均表示为一种固定长度的向量，具体做法是将最后一个word对应的计算得到的 hidden vector 作为整句话的表示（sentence vector）。之后，将最后得到的这个 sentence vector 送入一个 linear layer，使其输出为一个维度为情绪种类个数。最后对 linear layer 得出的结果做 softmax 并依次为依据选出该句（同时也是 target）的情绪分类。\nTD-LSTM 将输入的句子根据 aspect 分为两部分，两边都朝着 aspect 的方向分别同时把 words 送入两个 LSTM 中\nTC-LSTM 与 TD-LSTM 唯一的不同就是在 input 时在每个 word embedding vector 后面拼接上 aspect vector（如果 aspect 中有多个 word，则取平均）\nAT-LSTM 对隐藏状态h和aspect的词嵌入后施加attention\nATAE-LSTM 在LSTM的输入方面在concat一个aspect的词向量，说明aspect的重要性\nIAN IAN 模型由两部分组成，两部分分别对 Target 和 Context 进行建模。每一部分都以词嵌入作为输入，再通过 LSTM 获取每个词的隐藏状态，最后取所有隐藏向量的平均值，用它来监督另一部分注意力向量的生成。attention学习隐藏状态和对应词向量序列的相关性。\nattention部分是$h_t^i$\u0026amp;$avg(h_c)$在target上做注意力，$h_c^i$\u0026amp;$avg(h_t)$在context上做注意力\n实体和关系联合抽取 信息抽取：从自然语言文本中抽取指定类型的实体、 关系、 事件等事实信息，并形成结构化数据输出的文本处理技术。一般情况下信息抽取别是知识抽取等其他任务的基础。主要在对无结构数据的抽取出现问题\n基本方法原理 名词解释 span：指的是文本中的一段连续的子串，这段子串对应于某个实体或者关系的具体文本表述。\nDyGIE 问题定义：\n输入：所有句中可能的spans序列集合。\n输出三种信息：实体类型，关系分类（同一句），指代链接（跨句）；\nToken Representation Layer（Token表示层）：BiLSTM\nSpan Representation Layer（span表示层）： 初始化来自BiLSTM输出联合起来，加入基于注意力模型。\nCoreference Propagation Layer（指代传播层）：N次传播处理，跨span共享上下文信息\nRelation Propagation Layer（关系传播层）：与指代传播层相似\nFinal Prediction Layer（最终预测层）：去预测任务—实体任务，关系任务\nOneIE 任务定义：给定一个输入的句子，输出一个图，图中节点(含节点类型)代表实体提及或者触发词，图中的边表示表示节点之间的关系\n条件随机场（Conditional Random Field，CRF）是一种在自然语言处理（NLP）中广泛使用的模型。CRF的主要作用是解决序列数据的标注问题，它能够考虑整个序列的上下文信息，以做出更准确的预测。\nBeam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。Beam Search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。\n在这里只保留最好的\nUIE UniEX 检索式问答系统 1、问题分析模块：问题分类和关键词提取\n问题分类：\n关键词提取：根据问题分类，用序列标注法抽取相应类别的实体做为检索关键词\n2、检索模块：检索问题答案所在文档与段落\n3、 答案抽取模块：在相关片段中抽取备选答案，并对备选答案进行排序\n实现方法：\n流水线方式 Document Retriever + Reading Comprehension Reader框架\nDrQA TF-IDF：（Term Frequency-Inverse Document Frequency，词频-逆文件频率）是一种用于信息检索和数据挖掘的常用加权技术。它是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度\n使用TF-IDF获取与问题topK相关的文档\n然后将对topK使用抽取式阅读理解，从原文中抽取出可以回答的文本\nEvidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering 有些问题需要来自不同来源的证据相结合才能正确回答。解决方法：strength-based re-ranker\u0026amp;coverage-based re-ranker\nstrength-based re-ranker的基本思想是，正确的答案通常会被更多的段落反复提及\ncoverage-based re-ranker考虑每个答案在覆盖不同证据方面的能力，这里用一个BiLSTM来计算答案支撑片段的相似表征【指一个答案和它的支撑片段在表征空间中的相似度】，在垮文本上的相似表征很很高说明这个答案更可靠\n端到端方式 Retriever-Reader的联合学习 ORQA: Open-Retriever Question Answering 问题引入：\n1）需要具有强监督的支持证据：监督数据难以获得\n2）利用IR（信息检索）系统检索候选证据：QA与IR存在一定差异性，IR更关注词法或语义相似性，QA对于语言理解层次更丰富\n就是一个S是打分函数。评价retrieval和评价reader是两个不同的，$s_{retr}$是评价这个block和问题的相关性的，$S_{read}$是评价块儿里的文本和q的相关性的。这个里面的bert是用来理解retrieval和question的。\n每个块通过BERT和权重矩阵b生成隐藏向量h，问题通过BERT和权重矩阵q生成隐藏向量h，通过点积判断相关性\nBERT_R+MLP生成s，给S_read来评分\n有监督训练，需要手标与a有关的s\n有挑战，但是懒得管了\n基于预训练的Retriever-Free方法 对预训练模型进行微调，使其能够在没有任何外部上下文或知识的情况下回答问题\n使用span corruption来预训练\nSpan Corruption是T5模型预训练任务中的一种方法。它将完整的句子根据随机的span进行掩码。例如，原句：“Thank you for inviting me to your party last week”，Span Corruption之后可能得到输入：“Thank you [X] me to your party [Y] week”，目标：“[X] for inviting [Y] last [Z]”。其中[X]等一系列辅助编码称为sentinels。\n这种方法的目标是让模型学习如何从被打乱或被掩码的句子中恢复出原始的句子。\nLLM在问答任务上与有监督微调效果不相上下\nLLM在计数、多跳推理、日期、因果等类型上的性能较弱\n最后一节课 讲了一节课的对话系统（不考）\n参考：https://blog.csdn.net/ld326/article/details/112802292\n","date":"2023-12-27T19:07:00+08:00","permalink":"http://localhost:1313/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/","title":"自然语言处理笔记"},{"content":"导语 本笔记是ucas李炼编译程序高级教程课程的笔记。属于知识点碎碎念，不知道对作业、考试有没有作用。\n文法 文法：终结符、非终结符、产生式、开始符号（或者识别符号）的四元组\n产生式：可以理解为推导规则\n终结符、非终结符：\n非终结符：能出现在推导左边的就是非终结符 终结符：不能出现在推导左边的局势终结符 即能否进一步推导细分是区分终结符和非终结符的关键\n句型、句子：\n句型：符号串x是从开始符号S推导出来的，则x是文法G[S]的句型 句子：若推导出的符号串x仅由终结符号组成，则x为G[S]的句子 语言：从S推导出的所有句子的集合，也就是从S开始推导出的只有终结符的东西 G产生的语言记作L（G）\n文法的等价： 如果L(G)==L(G2)，则G1等价于G2\n3型文法==有限自动机： 如果文法的产生式P都有A→aB或A→a的形式，其中A、B∈$V_N$，a∈$V_T$。3型文法的许多性质是可判定的常用于编译器的词法分析程序的构造\n左右线性文法：\n右线性文法：形如A→aB或A→a 左线性文法：形如A→Ba或A→a 词法 字母表Σ\n正规式==3型文法（感觉正规式就是正则表达式）\n正规式和正规集：\n正规集可以用正规式表示 正规式是表示正规集一种方法 一个字集合是正规集当且仅当它能用正规式表示 正规集是正规式表示的语言 正规式可以递归定义\n且正规式有运算优先级\n感觉正规式和正则表达式没啥区别\n（这他妈不就是正则表达式吗）\n有限状态自动机\n五元式M = (S, ∑, δ, s0, F)，其中\nS是状态集合 ∑是字母表 s0为开始状态，属于S F是接受（或终止）状态的集合，属于S δ是转换函数 只有转换函数的表示稍微难理解一点，下面是一个例子\n也可以转成转换表\n只要有从s0到F内元素的路径，NFA就接受这个串（串也被称为语言）\n根据是否有ε转换细分成两种\n二型文法 设G=(VN,Vt,P,S)，如果它的每个产生式有如下形式，A→β，A∈VN，β∈(VN∪VT)*，则G是二型文法【即，所有的产生式都是非终结符推别的】\n二型文法==下推自动机（PDA）\n最右（左）推导：推导的每一步都是替代最右（左）边非终结符的推导\n二义性文法：一个文法，如果存在某个句子有不止一棵分析树与之对应，则称这个文法是二义的\n一个二义性的文法是可能将其改写，得到一个等价的无二义性的文法的。这种变换不总是可行的，具体看看第三章ppt吧\n自上而下、自下而上：\n对任何输入串，试图用一切可能的方法，从文法开始符号出发，自上而下、从左到右地为输入串建立分析树，即为输入串寻找最左推导。 消除左递归 左递归的分类 直接左递归：直接见诸于产生式的左递 一般的左递归：如S→Aa|b A→Ac|Sd| 直接左递归消除： 一般左递归消除： 文法G满足一定条件可以唯一的选择产生式继续推导\nFIRST集：令文法G是不含左递归的文法，对G的非终结符的候选α，定义它的开始符号（终结首符）集合\n求FIRST集：\n直接收取：对形如U－a…的产生式（其中a是终结符），把a收入到First(U)中 反复传送：对形入U－P…的产生式（其中P是非终结符），应把First(P)中的全部内容传送到First(U)中。 在不含左递归且每个非终结符所有选择的FIRST集都两两不相交的条件下（互相求∩为空集），存在可能构造专门的递归下降分析器\nFOLLOW集合：Follow集合是针对非终结符而言的，Follow(U)所表达的是句型中非终结符U所有可能的后随终结符号的集合（U后面可能跟着的终结符的集合）\n求FOLLOW集：例，求FOLLOW(U)\n（1）将 $ 放到follow（S）中，其中S是文法的开始符号。\n（2）如果存、A→αBβ，那么first（β）中除ε之外加入follow（B）中。 【 follow(B)是求跟在B后的终结符或$组成的集合，因此对于跟在B后的β，它的first集合就是follow(B)的子集 】\n（3）如果存在A→αB，或A→αBβ， 且first（β）包含ε， follow（A）加入follow（B）中。 【 对于A→αBβ,且β多步推导出ε ，那么可以用αB替换A, B后面紧跟的字符就是A后面紧跟的字符】\n预测分析表\n例，上面那个的预测分析表：\n非递归预测分析\nLL(1)文法：\nhttps://leiblog.wang/%E7%BC%96%E8%AF%91%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B%EF%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ https://blog.csdn.net/m0_67877471/article/details/124776449\nhttps://www.cnblogs.com/henuliulei/\n编译原理FOLLOW集的求法_follow集合怎么求-CSDN博客\n","date":"2023-12-15T17:37:48+08:00","permalink":"http://localhost:1313/p/%E7%BC%96%E8%AF%91%E7%A8%8B%E5%BA%8F%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B/","title":"编译程序高级教程"},{"content":"为了避免跨平台的问题，直接用choco在windows上安装openssl 3.1.1\nchoco install openssl\n首先生成私钥\n1 openssl genrsa -aes256 -out private.pem 4096 其中\ngenrsa是openssl的一个命令，用于生成RSA私钥。\n-aes256表示在输出私钥之前，使用AES 256加密\n-out private.pem 表示将生成的私钥输出到名为private.pem的文件中\n4096表示生成的私钥的位数，即私钥的长度为4096位\n然后使用私钥生成证书\n1 openssl req -new -x509 -days 365 -key .\\private.pem -out cacert.crt -config .\\smime.cnf -extensions smime 其中\nreq是openssl的一个命令，用于创建和处理PKCS#10格式的证书请求。\n-new表示创建一个新的证书请求。\n-x509表示生成一个自签名的证书，而不是生成一个证书请求。\n-days 365表示生成的证书的有效期为365天。\n-key .\\private.pem表示使用名为private.pem的文件中的私钥来签署证书。\n-out cacert.crt表示将生成的证书输出到名为cacert.crt的文件中。\n-config .\\smime.cnf表示使用名为smime.cnf的文件作为配置文件。\n-extensions smime表示应该包含配置文件中名为smime的部分中指定的扩展。\nsmime的部分为\n1 2 3 4 5 6 7 [smime] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = emailProtection subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always, issuer subjectAltName = email:copy basicConstraints = CA:FALSE指定证书不能用作CA（证书颁发机构）\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment指定证书的公钥可以用于哪些用途。这个证书可以用于非否认（nonRepudiation）、数字签名（digitalSignature）和密钥封装（keyEncipherment）\nextendedKeyUsage = emailProtection用于电子邮件保护（emailProtection）\nsubjectKeyIdentifier = hash用公钥的hash值唯一地标识证书中的公钥。\nauthorityKeyIdentifier = keyid:always, issuer用于标识签署此证书的CA的公钥。这个扩展通常包含CA公钥的keyid（一个唯一标识符），以及CA的名称（issuer）。keyid:always表示总是包含keyid，无论是否需要\nsubjectAltName = email:copy用于指定证书的主题可选名称（Subject Alternative Name）。主题可选名称是电子邮件地址，该地址从证书的主题名称字段中复制\n生成的时候国家地区公司啥的都不重要，我直接敲回车按默认了。邮箱写自己的就行了。\n直接用windwos自带的证书查看器查看这个证书。\n版本v3是指我们使用了x.509第3版本，然后序列号是有证书生成算法生成的，唯一的指定这个证书，像身份证号似的。签名算法和哈希算法是一个声明，颁发者是我们刚才在生成证书时写的。有效期由我们刚才的 -days 参数指明，使用者和颁发者一样。\n公钥直接在证书文件里保存。公钥参数0500表示NULL，这是因为RSA的公钥的参数（模数和公开指数）已经在公钥字段中给出，所以不需要在公钥参数字段中再给出，如果是其他的加密算法，可能会包含其他信息。\n基本约束\nSubject Type=End Entity ：这表示该证书是一个终端实体证书，而不是CA（证书颁发机构）证书。也就是说这个证书不能用于签发/创建其他证书。Path Length Constraint=None ：这表示路径长度没有设置，准许其签发多级的数字证书。然而，由于Subject Type=End Entity，这个证书不能用于签发其他证书，所以这个设置在这种情况下没有意义。这是由于我们使用了 basicConstraints = CA:FALSE的选项。\n下面的其他拓展都在-extension部分说过了，这里就不多赘述了。\n然后安装这个证书，并且选择保存路径为受信任的根证书颁发机构\n我使用的客户端是outlook。使用的邮箱服务是qq邮箱。\n在outlook里添加我的证书\n这里outlook只支持导入pfx，所以我们需要把生成的证书格式转换\n1 openssl pkcs12 -export -out cacert.pfx -inkey .\\private.pem -in .\\cacert.crt 接下来是导入助教的证书，首先在outlook里新建一个联系人\n导入，这里又只支持.cer了，我的windows下的openssl好像缺了库没法转，所以用wsl里的openssl转了一下\n1 openssl pkcs12 -in limengjie22\\@mails.ucas.ac.cn.pfx -nokeys -out output.cer openssl pkcs12 -nokeys命令用于从PKCS#12文件（通常具有.pfx或.p12扩展名）中提取证书，-nokeys指定不包含私钥，这样生成的output.cer不能做任何需要私钥的操作（我们也没有要用私钥的操作）\nimport password即使提供的私钥.txt的内容。然后就可以成功导入了\n然后在发送邮件的时候，在选项里把加密和签署都点了\n","date":"2023-11-29T19:56:28+08:00","permalink":"http://localhost:1313/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/","title":"网络认证技术作业三"},{"content":"导语 复现这篇2023NDSS论文，他大开源在https://github.com/fuchuanpu/HyperVision\n环境配置 我是在home下做的，如果想在别的地方搞稍微换下路径就行\n先拉git的代码 git clone https://github.com/fuchuanpu/HyperVision.git\n依照作者在readme里说，在纯净的ubuntu22.04上运行他的脚本即可。用docker装一个纯净的ubuntu22.04\n1 2 3 4 5 6 # 下载镜像 docker pull homebrew/ubuntu22.04:latest # 启动，并且把~/HyperVision 和容器内的/root/HyperVision连起来 docker run -td --name hypervision -v \u0026#34;$HOME/HyperVision\u0026#34;:/root/HyperVision homebrew/ubuntu22.04:latest # 进入 docker exec hypervision -it bash 接下来的操作在docker里了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sudo su cd /root/HyperVision sudo ./env/install_all.sh # 这里最好先换个国内的源，会把需要装的都装了 wget https://hypervision-publish.s3.cn-north-1.amazonaws.com.cn/hypervision-dataset.tar.gz # 下载数据集，有6G，走的cdn，裸连速度就还不错 tar -xzf hypervision-dataset.tar.gz # 我也不知道为什么他写了个-xxf 如果想删掉原来的就删吧，不删也没关系 ./script/rebuild.sh ./script/expand.sh cd build \u0026amp;\u0026amp; ../script/run_all_brute.sh \u0026amp;\u0026amp; cd .. cd ./result_analyze ./batch_analyzer.py -g brute cat ./log/brute/*.log | grep AU_ROC 在https://www.bilibili.com/video/BV1zj411e7xC/可以看到复现视频\n","date":"2023-10-25T10:25:43+08:00","permalink":"http://localhost:1313/p/%E5%A4%8D%E7%8E%B0detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/","title":"复现Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis"},{"content":"导语 ucas 2023秋 网络认证技术\n作业2：任选一个标准（口令鉴别协议），书写阅读报告。报告内容要求描述基本原理，解决了什么问题，可能存在什么问题。\n概述 我选择阅读RFC 7296，该标准是互联网密钥交换 （IKE） 协议的第二个版本。本标准使RFC 5996废弃， IKEv2是当前的互联网标准。\n解决了什么问题 IKEv2（Internet Key Exchange version 2）是一种用于建立虚拟专用网络（VPN）连接的协议，它解决了许多与安全通信和远程访问有关的问题。包括：\n安全性：IKEv2提供了强大的安全性，通过使用加密算法来保护数据的机密性和完整性。它还允许身份验证，以确保通信双方是合法的，并可以抵御各种网络攻击，如中间人攻击和数据篡改。 移动性：IKEv2支持移动设备的连接，允许用户从一个网络切换到另一个网络时保持连接的连续性。这对于移动工作人员或在不同网络环境中工作的人员非常有用。 多平台兼容性：IKEv2是一种通用的VPN协议，支持多种操作系统和设备，包括Windows、macOS、iOS、Android和Linux。这使得它成为广泛使用的VPN协议，能够在不同平台之间建立安全的连接。 快速重新连接：IKEv2具有快速重新连接的能力，可以在断开连接后快速重新建立连接，而不需要用户手动干预。这对于移动设备或不稳定的网络连接非常有用。 支持IPv6：随着IPv6的推广，IKEv2也提供了对IPv6的良好支持，使其适用于新一代互联网协议。 NAT穿透：IKEv2能够穿越网络地址转换（NAT）设备，这使得它在各种网络环境中都能够正常工作，包括家庭网络和企业网络。 在基本原理-1.1节也简述了IKEv2在特定场景下解决了什么问题。\n基本原理 1.1 使用场景 IP 安全性 （IPsec） 为 IP 数据报提供机密性、数据完整性、访问控制和数据源身份验证，这些服务是通过维护 IP 数据报的源和接收方之间的共享状态来提供的。以手动方式建立此共享状态不能很好地扩展。IKEv2正是这样一个动态建立此状态的协议。IKE 在双方之间执行相互身份验证，并建立 IKE 安全关联 （SA），该关联包含共享机密信息，可用于高效建立用于封装安全有效负载 （ESP） [ESP] 或身份验证标头 （AH） [AH] 的 SA，以及一组加密算法，供 SA 用于保护其承载的流量。IKE 用于在许多不同的场景中协商 ESP 或 AH SA，每种方案都有自己的特殊要求。\n1.1.1 隧道模式下的安全网关到安全网关 在此方案中，IP 连接的两个endpoint都不实现 IPsec，但它们之间的网络节点会保护部分方式的流量。保护对endpoint是透明的，并且依赖于普通路由通过隧道终结点发送数据包进行处理。每个endpoint将宣布其后subnet的地址集，数据包将以隧道模式发送，其中内部 IP 标头将包含实际端点的 IP 地址。\n1.1.2 端点到端点传输模式 在此方案中，IP 连接的两个终结点都实现 IPsec，这是 [IPSECARCH] 中主机的要求。该模式通常使用没有内部 IP 标头。将协商一对地址，以便此 SA 保护数据包。这些endpoint可以根据参与者的 IPsec 身份验证身份实现应用层访问控制。此方案实现了端到端安全性。虽然此场景可能不完全适用于 IPv4 公网，但已在使用 IKEv1 的内网内的特定场景中成功部署。在向 IPv6 过渡期间和采用 IKEv2 期间，应该更广泛地启用它。\n在这种情况下，一个或两个受保护的端点可能位于网络地址转换 （NAT） 节点后面，在这种情况下，必须对隧道数据包进行 UDP 封装，以便 UDP 标头中的端口号可用于标识 NAT “后面”的各个endpoint。\n1.1.3隧道模式下的端点到安全网关 在此方案中，受保护的endpoint（通常是便携式计算机）通过受 IPsec保护的隧道连接回其企业网络。它可能仅使用此隧道访问公司网络上的信息，或者可能通过公司网络将其所有流量通过隧道传输回，以便利用公司防火墙提供的针对基于 Internet 的攻击的保护。在任一情况下，受保护端点都需要一个与安全网关关联的 IP 地址，以便返回到该网关的数据包将转到安全网关并用隧道传回。此 IP 地址可以是静态的，也可以由安全网关动态分配。为了支持后一种情况，IKEv2 包括一种机制（即配置有效负载），发起方请求安全网关拥有的 IP 地址，以便在其 SA 期间使用。\n在这种情况下，数据包将使用隧道模式。在来自受保护endpoint的每个数据包上，外部 IP 标头将包含与其当前位置关联的源 IP 地址（即，将流量直接路由到端点的地址），而内部 IP 标头将包含安全网关分配的源 IP 地址（即，将流量路由到安全网关以转发到端点的地址）。外部目标地址将始终是安全网关的地址，而内部目标地址将是数据包的最终目标。\n在这种情况下，受保护的终结点可能位于 NAT 后面。在这种情况下，安全网关看到的 IP 地址将与受保护端点发送的 IP 地址不同，并且必须对数据包进行 UDP 封装才能正确路由。\n1.2初始交换 使用 IKE 的通信始终从IKE_SA_INIT和IKE_AUTH交换开始（在 IKEv1 中称为阶段 1）。这些初始交换通常由四条消息组成，但在某些情况下，该数字可能会增长。使用 IKE 的所有通信都由请求/响应对组成。我们将首先描述基础交换，然后是变体。第一对消息 （IKE_SA_INIT） 协商加密算法、交换随机数并进行 Diffie-Hellman 交换 [DH]。\n第二对消息 （IKE_AUTH） 对以前的消息进行身份验证，交换身份和证书，并建立第一个子 SA。这些消息的某些部分使用通过IKE_SA_INIT交换建立的密钥进行加密和完整性保护，因此身份对窃听者隐藏，并且所有消息中的所有字段都经过身份验证。有关如何生成加密密钥的信息，请参阅第 2.14 节。（无法完成IKE_AUTH交换的中间人攻击者仍可以看到发起者的身份。\n初始交换后的所有消息都使用在IKE_SA_INIT交换中协商的加密算法和密钥进行加密保护。这些后续消息使用第 3.14 节中描述的加密有效负载的语法，使用第 2.14 节中所述派生的密钥进行加密。所有后续消息都包含加密有效负载，即使它们在文本中称为“空”。对于CREATE_CHILD_SA、IKE_AUTH或信息交换，标头后面的消息是加密的，包含标头的消息是使用为 IKE SA 协商的加密算法进行完整性保护的。\n每个 IKE 消息都包含一个消息 ID 作为其固定标头的一部分。此消息 ID 用于匹配请求和响应，并标识消息的重新传输。\n一些简称如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 Notation Payload ---------------------------------------- AUTH Authentication CERT Certificate CERTREQ Certificate Request CP Configuration D Delete EAP Extensible Authentication HDR IKE header (not a payload) IDi Identification - Initiator IDr Identification - Responder KE Key Exchange Ni, Nr Nonce N Notify SA Security Association SK Encrypted and Authenticated TSi Traffic Selector - Initiator TSr Traffic Selector - Responder V Vendor ID 第 3 节介绍了每个有效负载的内容的详细信息。可能选择显示的有效负载将显示在括号中，例如 [CERTREQ];这表示可以选择包含证书请求有效负载。\n初始交流如下：\n发起方→接收方HDR, SAi1, KEi, Ni。\nHDR 包含安全参数索引 （SPI）、版本号、Exchange 类型、消息 ID 和各种标志。SAi1 有效负载声明发起方为 IKE SA 支持的加密算法。KE 有效负载发送发起方的 Diffie-Hellman 值。Ni是发起者的随机数。\n接收方→发起方HDR, SAr1, KEr, Nr, [CERTREQ]。\n响应方从发起方提供的选择中选择加密套件，并在 SAr1 有效负载中表达该选择，完成与 KEr 有效负载的 Diffie-Hellman 交换，并在 Nr 有效负载中发送其随机数。\n在协商的这一点上，每一方都可以生成一个名为 SKEYSEED 的数量（参见第 2.14 节），该 IKE SA 的所有密钥都从中派生出来。以下消息完全加密和完整性保护，邮件头除外。用于加密和完整性保护的密钥派生自 SKEYSEED，称为SK_e（加密）和SK_a（身份验证，又名完整性保护）;有关密钥派生的详细信息，请参见第 2.13 和 2.14 节。为每个方向计算单独的SK_e和SK_a。除了从 Diffie-Hellman 值派生的用于保护 IKE SA 的密钥SK_e和SK_a之外，还派生了另一个数量SK_d，并用于派生子 SA 的进一步密钥材料。符号 SK { \u0026hellip; } 表示这些有效负载已使用该方向的SK_e和SK_a进行加密和完整性保护。\n发起方→接收方HDR, SK {IDi, [CERT,] [CERTREQ,] [IDr,] AUTH, SAi2, TSi, TSr} 。发起方使用 IDi 有效负载断言其身份，证明与 IDi 对应的密钥的知识，完整性使用 AUTH 有效负载保护第一条消息的内容（请参阅第 2.15 节）。它还可能在 CERT 有效负载中发送其证书，并在 CERTREQ 有效负载中发送其信任锚的列表。如果包含任何 CERT 有效负载，则提供的第一个证书必须包含用于验证 AUTH 字段的公钥。可选的有效负载 IDr 使发起方能够指定要与响应方的哪个身份通信。当运行响应程序的计算机在同一 IP 地址上托管多个标识时，这很有用。如果发起方建议的 IDr 不被响应方接受，则响应方可能会使用其他某个 IDr 来完成交换。如果发起方随后不接受响应方使用的 IDr 与所请求的 IDr 不同的事实，则发起方可以在注意到这一事实后关闭 SA。发起方使用 SAi2 有效负载开始协商子 SA。最终字段（以 SAi2 开头）在CREATE_CHILD_SA交换的描述中描述。\n接收方→发起方HDR, SK {IDr, [CERT,] AUTH, SAr2, TSi, TSr}。响应方使用 IDr 有效负载断言其身份，可以选择发送一个或多个证书（再次使用包含用于验证 AUTH 的公钥的证书首先列出），使用 AUTH 有效负载验证其身份并保护第二条消息的完整性，并使用下面在CREATE_CHILD_SA交换中描述的其他字段完成子 SA 的协商。IKE_AUTH交换双方必须验证所有签名和消息身份验证代码 （MAC） 是否正确计算。如果任何一方使用共享密钥进行身份验证，则 ID 有效负载中的名称必须与用于生成 AUTH 有效负载的密钥相对应。由于发起方在IKE_SA_INIT中发送其 Diffie-Hellman 值，因此它必须猜测响应方将从其支持的组列表中选择的 Diffie-Hellman 组。如果发起方猜错了，响应方将使用类型 INVALID_KE_PAYLOAD 的通知有效负载进行响应，指示所选组。在这种情况下，发起方必须使用更正的 Diffie-Hellman 组重试IKE_SA_INIT。发起方必须再次提出其完整的可接受加密套件集，因为拒绝消息未经身份验证，否则主动攻击者可以诱使端点协商弱的套件。\n如果在IKE_AUTH交换期间创建子 SA 由于某种原因而失败，IKE SA 仍会照常创建。IKE_AUTH交换中不阻止设置 IKE SA 的通知消息类型列表至少包括以下内容：NO_PROPOSAL_CHOSEN、TS_UNACCEPTABLE、SINGLE_PAIR_REQUIRED、INTERNAL_ADDRESS_FAILURE和FAILED_CP_REQUIRED。\n如果失败与创建 IKE SA 有关（例如，返回AUTHENTICATION_FAILED通知错误消息），则不会创建 IKE SA。请注意，尽管IKE_AUTH消息已加密且完整性受到保护，但如果收到此通知错误消息的对等方尚未对另一端进行身份验证（或者如果对等方由于某种原因未能对另一端进行身份验证），则需要谨慎对待这些信息。更准确地说，假设MAC正确验证，则已知错误通知消息的发送方是IKE_SA_INIT交换的响应者，但无法保证发送方的身份。\n请注意，IKE_AUTH消息不包含 KEi/KEr 或 Ni/Nr 有效负载。因此，IKE_AUTH交换中的 SA 有效负载不能包含具有除 NONE 以外的任何值的转换类型 4（Diffie-Hellman 组）。实现应该省略整个转换子结构，而不是发送值 NONE。\n1.3 CREATE_CHILD_SA交换 CREATE_CHILD_SA交换用于创建新的子 SA，并重新生成 IKE SA 和子 SA 的密钥。此交换由单个请求/响应对组成，其某些功能在 IKEv1 中称为第 2 阶段交换。在初始交换完成后，它可以由IKE SA的任何一端发起。\n通过创建新 SA，然后删除旧 SA 来重新生成 SA 的密钥。本节介绍重新生成密钥的第一部分，即创建新 SA;第 2.8 节介绍了重新生成密钥的机制，包括将流量从旧 SA 移动到新 SA 以及删除旧 SA。必须一起阅读这两个部分才能理解重新生成密钥的整个过程。\n任一端点都可能发起CREATE_CHILD_SA交换，因此在本节中，术语发起方是指发起此交换的端点。实现可以拒绝 IKE SA 中的所有CREATE_CHILD_SA请求。\nCREATE_CHILD_SA请求可以选择包含用于额外 Diffie-Hellman 交换的 KE 有效负载，以便为子 SA 提供更强有力的前向保密保证。子 SA 的键控材料是 IKE SA 建立期间建立的SK_d、CREATE_CHILD_SA交换期间交换的随机数和 Diffie-Hellman 值（如果 KE 有效载荷包含在CREATE_CHILD_SA交换中）的函数。\n如果CREATE_CHILD_SA交换包含 KEi 有效载荷，则至少有一个 SA 报价必须包括 KEi 的 Diffie-Hellman 组。KEi的Diffie-Hellman组必须是发起者期望响应者接受的组的一个元素（可以提出其他Diffie-Hellman组）。如果响应方使用不同的 Diffie-Hellman 组（NONE 除外）选择提案，则响应方必须拒绝该请求，并在INVALID_KE_PAYLOAD Notify 有效负载中指示其首选的 Diffie-Hellman 组。有两个八位字节的数据与此通知相关联：接受的 Diffie-Hellman 组号，按大端序排列。在此类拒绝的情况下，CREATE_CHILD_SA交换失败，发起方可能会在响应者在INVALID_KE_PAYLOAD通知有效负载中给出的组中使用 Diffie-Hellman 提案和 KEi 重试交换。\n响应方发送NO_ADDITIONAL_SAS通知，以指示CREATE_CHILD_SA请求不可接受，因为响应方不愿意在此 IKE SA 上接受更多的子 SA。此通知还可用于拒绝 IKE SA 重新生成密钥。一些最小实现可能只接受初始 IKE 交换上下文中的单个子 SA 设置，并拒绝任何后续添加更多设置的尝试。\n1.3.1 通过CREATE_CHILD_SA交换创建新的子 SA 可以通过发送CREATE_CHILD_SA请求来创建子 SA。创建新子 SA 的CREATE_CHILD_SA请求是：\n发起方→接收方 HDR, SK {SA, Ni, [KEi,] TSi, TSr}。\n发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。\n接收方→发起方 HDR, SK {SA, Nr, [KEr,]TSi, TSr}\n如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。\n要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。\nUSE_TRANSPORT_MODE通知可以包含在请求消息中，该消息还包括请求子 SA 的 SA 有效负载。它要求子 SA 对创建的 SA 使用传输模式而不是隧道模式。如果请求被接受，则响应还必须包含类型 USE_TRANSPORT_MODE 的通知。如果响应方拒绝请求，子 SA 将在隧道模式下建立。如果发起方无法接受，则发起方必须删除 SA。注意：除非使用此选项协商传输模式，否则所有子 SA 都将使用隧道模式。\nESP_TFC_PADDING_NOT_SUPPORTED通知断言发送终结点将不接受在正在协商的子 SA 上填充包含流量流机密性 （TFC） 填充的数据包。如果两个终结点都不接受 TFC 填充，则此通知将包含在请求和响应中。如果此通知仅包含在其中一条消息中，则仍可以在另一个方向发送 TFC 填充。\nNON_FIRST_FRAGMENTS_ALSO通知用于碎片控制。有关更全面的解释，请参见 [IPSECARCH]。双方需要同意在任何一方发送非第一个片段之前发送。仅当建议 SA 的请求和接受 SA 的响应中都包含通知NON_FIRST_FRAGMENTS_ALSO才会启用它。如果响应程序不想发送或接收非第一个片段，则它只会从响应中省略NON_FIRST_FRAGMENTS_ALSO通知，但不会拒绝整个子 SA 创建。\n第 2.22 节中涵盖的IPCOMP_SUPPORTED通知也可以包含在交易所中。\n创建子 SA 的失败尝试不应拆除 IKE SA：没有理由丢失为 IKE SA 所做的工作。有关创建子 SA 失败时可能出现的错误消息列表，请参阅第 2.21 节。\n1.3.2 使用CREATE_CHILD_SA交换机重新生成 IKE SA 的密钥 重新生成 IKE SA 密钥的CREATE_CHILD_SA请求是：\n发起方→接收方HDR, SK {SA, Ni, KEi}\n发起方在 SA 有效负载中发送 SA 产品/服务，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值。必须包括 KEi 有效负载。新的发起方 SPI 在 SA 有效负载的 SPI 字段中提供。一旦对等方收到重新生成 IKE SA 密钥的请求或发送重新生成 IKE SA 的请求，它就不应在正在重新生成密钥的 IKE SA 上发起任何新的CREATE_CHILD_SA交换。\n接收方→发起方 HDR, SK {SA, Nr, KEr}\n如果所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。新的响应程序 SPI 在 SA 有效负载的 SPI 字段中提供。\n新的 IKE SA 将其消息计数器设置为 0，无论它们在早期的 IKE SA 中是什么。来自新 IKE SA 上双方的第一个 IKE 请求的消息 ID 为 0。旧的 IKE SA 保留其编号，因此任何进一步的请求（例如，删除 IKE SA）都将具有连续编号。新的 IKE SA 的窗口大小也重置为 1，并且此重新密钥交换中的发起方是新 IKE SA 的新“原始发起方”。\n1.3.3. 使用 CREATE_CHILD_SA 交换重新生成子 SA 的密钥 重新生成子 SA 密钥CREATE_CHILD_SA请求是：\n发起方→接收方 HDR, SK {N(REKEY_SA), SA, Ni, [KEi,] TSi, TSr}\n发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。第 1.3.1 节中描述的通知也可以在重新生成密钥交换中发送。通常，这些通知与原始交换中使用的通知相同;例如，重新生成传输模式 SA 的密钥时，将使用USE_TRANSPORT_MODE通知。如果交换的目的是替换现有的 ESP 或 AH SA，则必须将REKEY_SA通知包含在CREATE_CHILD_SA交换中。正在重新生成密钥的 SA 由通知有效负载中的 SPI 字段标识;这是交换发起方在入站 ESP 或 AH 数据包中期望的 SPI。没有与此通知消息类型关联的数据。REKEY_SA通知的协议 ID 字段设置为与我们要重新生成密钥的 SA 的协议匹配，例如，3 表示 ESP，2 表示 AH。\n重新生成子 SA 密钥CREATE_CHILD_SA响应为：\n接收方→发起方 HDR, SK {SA, Nr, [KEr,] TSi, TSr}\n如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受报价、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。\n要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。\n1.4. 信息交换 在 IKE SA 运行过程中的不同点，对等方可能希望相互传达有关某些事件的错误或通知的控制消息。为了实现这一点，IKE 定义了一个信息交换。信息交换必须仅在初始交换之后进行，并使用协商密钥进行加密保护。请注意，某些信息性消息（而非交换）可以在 IKE SA 的上下文之外发送。第 2.21 节还详细介绍了错误消息。\n与 IKE SA 相关的控制消息必须在该 IKE SA 下发送。与子 SA 相关的控制消息必须在生成它们的 IKE SA（如果 IKE SA 已重新生成密钥，则为其后续消息）的保护下发送。\n信息交换中的消息包含零个或多个通知、删除和配置有效负载。信息交换请求的接收者必须发送一些响应;否则，发送方将假定消息在网络中丢失并重新传输。该响应可能是一条空消息。信息交换中的请求消息也可能不包含有效负载。这是终结点可以要求另一个终结点验证其是否处于活动状态的预期方式。\n信息交换定义为：\n发起方→接收方 HDR, SK {[N,] [D,] [CP,] \u0026hellip;}\n接收方→发起方 HDR, SK {[N,] [D,] [CP,] \u0026hellip;}\n信息交换的处理由其组件有效载荷决定。\n1.4.1. 删除具有信息交换的 SA ESP 和 AH SA 始终成对存在，每个方向上有一个 SA。关闭 SA 时，必须关闭（即删除）对的两个成员。每个终结点必须关闭其传入的 SA，并允许另一个终结点关闭每对中的另一个 SA。要删除 SA，将发送具有一个或多个 Delete 有效负载的信息交换，列出要删除的 SA 的 SPI（正如入站数据包标头中预期的那样）。收件人必须关闭指定的 SA。请注意，从不在单个消息中发送 SA 两端的删除有效负载。如果要同时删除多个 SA，则在信息交换中包括每个 SA 对的入站部分的删除有效负载。\n通常，信息交换中的响应将包含向另一个方向的配对 SA 的删除有效负载。有一个例外。如果一组 SA 的两端偶然独立决定关闭它们，则每个 SA 都可能发送 Delete 有效负载，并且这两个请求可能会在网络中交叉。如果节点收到已发出删除请求的 SA 的删除请求，则必须在处理请求时删除传出 SA，在处理响应时删除传入 SA。在这种情况下，响应不得包含已删除 SA 的删除有效负载，因为这会导致重复删除，并且理论上可能会删除错误的 SA。\n与 ESP 和 AH SA 类似，IKE SA 也通过发送信息交换来删除。删除 IKE SA 会隐式关闭根据该 IKE SA 协商的任何剩余子 SA。对删除 IKE SA 的请求的响应是空的信息响应。\n半闭合 ESP 或 AH 连接是异常的，具有审计功能的节点如果它们仍然存在，则可能应该审计它们的存在。请注意，此规范未指定时间段，因此由各个终结点决定等待多长时间。节点可以拒绝接受半闭合连接上的传入数据，但不得单方面关闭它们并重用 SPI。如果连接状态变得足够混乱，节点可能会关闭 IKE SA，如上所述。然后，它可以在新的 IKE SA 下干净的基础上重建所需的 SA。\n1.5 IKE SA 之外的信息性消息 在某些情况下，节点收到无法处理的数据包，但它可能希望将这种情况通知发送方。\n如果 ESP 或 AH 数据包到达时带有无法识别的 SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。 如果加密的 IKE 请求数据包到达端口 500 或 4500，并且具有无法识别的 IKE SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。 如果 IKE 请求数据包到达时的主版本号高于实现支持的版本号。 在第一种情况下，如果接收节点有一个活动的 IKE SA 到数据包来自的 IP 地址，它可能会在信息交换中通过该 IKE SA 发送任性数据包的INVALID_SPI通知。通知数据包含无效数据包的 SPI。此通知的接收者无法判断 SPI 是针对 AH 还是 ESP，但这并不重要，因为在许多情况下，两者的 SPI 会有所不同。如果不存在合适的 IKE SA，则节点可能会向源 IP 地址发送没有加密保护的信息性消息，如果数据包是 UDP（UDP 封装的 ESP 或 AH），则使用源 UDP 端口作为目标端口。在这种情况下，它应该只被收件人用作可能出错的提示（因为它很容易被伪造）。此消息不是信息交换的一部分，接收节点不得响应它，因为这样做可能会导致消息循环。消息构造如下：没有对此类通知的接收者有意义的 IKE SPI 值;使用零值或随机值都是可以接受的，这是第 3.1 节中禁止零 IKE 发起方 SPI 的规则的例外。发起方标志设置为 1，响应标志设置为 0，版本标志以正常方式设置;这些标志在第 3.1 节中描述。\n在第两种和第三种情况下，消息始终在没有加密保护的情况下发送（在 IKE SA 外部），并且包括INVALID_IKE_SPI或INVALID_MAJOR_VERSION通知（没有通知数据）。该消息是响应消息，因此它被发送到带有相同 IKE SPI 的 IP 地址和端口，并且消息 ID 和交换类型是从请求中复制的。响应标志设置为 1，版本标志以正常方式设置。\n可能存在的问题 参考发表在27th USENIX Security Symposium (USENIX Security 18), 2018的The Dangers of Key Reuse: Practical Attacks on IPsec IKE可IKEv1、v2如果重用密钥可能导致跨协议身份验证绕过，从而使攻击者能够冒充受害者主机或网络。在IKEv1模式下利用Bleichenbacher预言机，其中RSA加密的随机数用于身份验证。利用此漏洞打破了基于 RSA 加密的模式，此外还破坏了 IKEv1 和 IKEv2 中基于 RSA 签名的身份验证。此外，还存在针对基于 PSK（预共享密钥）的 IKE 模式的离线字典攻击，从而涵盖了 IKE 的所有可用身份验证机制。在思科（CVE-2018-0131）、华为（CVE2017-17305）、Clavister（CVE-2018-8753）和合勤科技（CVE-2018-9129）的IKEv1实现中找到了Bleichenbacher预言机。\n","date":"2023-10-19T16:31:55+08:00","permalink":"http://localhost:1313/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/","title":"IKEv2标准阅读"},{"content":"导语 ucas 2023 赵地老师GPU架构与编程课程大作业一\n要求：cuda不调库实现CNN推理 加分：cuda不调库实现CNN训练\n分析 实现CNN推理的话应该只需要用cuda实现CNN的正向传播，训练就都得实现。\n参考一下LeNet5_CNN\n这个项目大概是ece408课程的大作业，就是直接实现了LeNet5\n然后他的Makefile对编译的每个东西都include了一个libgputk，应该是他们课程给的一个用来调试的库，直接全删了依然可以编译。\n然后再按照课程要求，在每个nvcc命令后面加上编译选项 -Xcompiler \u0026quot;-O3 -std=c++14\u0026quot; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true\n源码分析 makefile如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 m2:\tm2.o custom nvcc -o m2 -lm -lcuda -lrt m2.o ece408net.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/layer/custom/*.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true m2.o:\tm2.cc nvcc --compile m2.cc -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true ece408net.o: ece408net.cc nvcc --compile ece408net.cc -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true network.o:\tsrc/network.cc nvcc --compile src/network.cc -o src/network.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true mnist.o:\tsrc/mnist.cc nvcc --compile src/mnist.cc -o src/mnist.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true layer:\tsrc/layer/conv.cc src/layer/ave_pooling.cc src/layer/conv_cpu.cc src/layer/conv_cust.cc src/layer/fully_connected.cc src/layer/max_pooling.cc src/layer/relu.cc src/layer/sigmoid.cc src/layer/softmax.cc nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/conv_cpu.cc -o src/layer/conv_cpu.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/conv_cust.cc -o src/layer/conv_cust.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true custom: nvcc --compile src/layer/custom/cpu-new-forward.cc -o src/layer/custom/cpu-new-forward.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/custom/gpu-utils.cu -o src/layer/custom/gpu-utils.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/layer/custom/gpu-new-forward-optimized.cu -o src/layer/custom/gpu-new-forward-optimized.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true loss: src/loss/cross_entropy_loss.cc src/loss/mse_loss.cc nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -Xcompiler \u0026#34;-O3 -std=c++14\u0026#34; -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -rdc=true clean: rm m2 rm m2.o run: m2 ./m2 1000 最终得到的可执行文件是m2，是m2.o和ece408net.o组成的，先查看一下生成m2.o的源码m2.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #include \u0026#34;ece408net.h\u0026#34; void inference_only(int batch_size) { std::cout\u0026lt;\u0026lt;\u0026#34;Loading fashion-mnist data...\u0026#34;; MNIST dataset(\u0026#34;./data/\u0026#34;); # class MNIST定义在minst.h里 dataset.read_test_data(batch_size); std::cout\u0026lt;\u0026lt;\u0026#34;Done\u0026#34;\u0026lt;\u0026lt;std::endl; std::cout\u0026lt;\u0026lt;\u0026#34;Loading model...\u0026#34;; Network dnn = createNetwork_GPU(); # 通过createNetworkGPU来建立dnn模型 std::cout\u0026lt;\u0026lt;\u0026#34;Done\u0026#34;\u0026lt;\u0026lt;std::endl; dnn.forward(dataset.test_data); # 进行正向传播，得到结果 float acc = compute_accuracy(dnn.output(), dataset.test_labels); std::cout\u0026lt;\u0026lt;std::endl; std::cout\u0026lt;\u0026lt;\u0026#34;Test Accuracy: \u0026#34;\u0026lt;\u0026lt;acc\u0026lt;\u0026lt; std::endl; std::cout\u0026lt;\u0026lt;std::endl; } int main(int argc, char* argv[]) { int batch_size = 10000; if(argc == 2){ batch_size = atoi(argv[1]); } std::cout\u0026lt;\u0026lt;\u0026#34;Test batch size: \u0026#34;\u0026lt;\u0026lt;batch_size\u0026lt;\u0026lt;std::endl; inference_only(batch_size); return 0; } 追踪一下控制流把。\n读取数据集 首先是读取dataset，定义了一个MNIST类，在minst.h和minst.cc里声明和实现\nmnist.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #ifndef SRC_MNIST_H_ #define SRC_MNIST_H_ #include \u0026lt;fstream\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026#34;./utils.h\u0026#34; class MNIST { private: std::string data_dir; public: Matrix train_data; Matrix train_labels; Matrix test_data; Matrix test_labels; void read_mnist_data(std::string filename, Matrix\u0026amp; data, int batch_size=-1); void read_mnist_label(std::string filename, Matrix\u0026amp; labels, int batch_size=-1); explicit MNIST(std::string data_dir) : data_dir(data_dir) {} void read(); void read_test_data(int batch_size); }; #endif // SRC_MNIST_H_ explicit MNIST(std::string data_dir) : data_dir(data_dir) {} 是MNIST类的构造函数，接受一个string类型的data_dir，并使用成员初始化列表将该参数的值赋给 data_dir 成员变量，且构造函数体为空，即构造的时候不做其他操作（cpp苦手），在代码里使用 MNIST dataset('./data')就是给dataset的对象的private变量data_dir赋值为 ./dataexplicit声明是要求cpp不要进行强制类型转换\n成员初始化列表\n1 2 3 4 5 6 7 8 9 10 11 12 class MyClass { public: // 成员初始化列表初始化成员变量 x 和 y MyClass(int a, int b) : x(a), y(b) { // 构造函数体（如果需要） // 可以在这里执行其他操作 } private: int x; int y; }; 语法类似上面，给private赋值\n然后使用MNIST类的read_test_data()方法读取数据集\n1 2 3 4 void MNIST::read_test_data(int batch_size) { read_mnist_data(data_dir + \u0026#34;t10k-86-images-idx3-ubyte\u0026#34;, test_data, batch_size); read_mnist_label(data_dir + \u0026#34;t10k-86-labels-idx1-ubyte\u0026#34;, test_labels, batch_size); } 使用read_mnist_data方法读取ubyte数据集\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 void MNIST::read_mnist_data(std::string filename, Matrix\u0026amp; data, int batch_size) { std::ifstream file(filename, std::ios::binary); // std::cout\u0026lt;\u0026lt;\u0026#34;Reading: \u0026#34;\u0026lt;\u0026lt;filename\u0026lt;\u0026lt;std::endl; if (file.is_open()) { int magic_number = 0; int number_of_images = 0; int n_rows = 0; int n_cols = 0; unsigned char label; file.read((char*)\u0026amp;magic_number, sizeof(magic_number)); file.read((char*)\u0026amp;number_of_images, sizeof(number_of_images)); file.read((char*)\u0026amp;n_rows, sizeof(n_rows)); file.read((char*)\u0026amp;n_cols, sizeof(n_cols)); //magic_number = ReverseInt(magic_number); //number_of_images = ReverseInt(number_of_images); //n_rows = ReverseInt(n_rows); //n_cols = ReverseInt(n_cols); if (batch_size \u0026gt; 0 \u0026amp;\u0026amp; batch_size \u0026lt; number_of_images){ number_of_images = batch_size; } // std::cout\u0026lt;\u0026lt;number_of_images\u0026lt;\u0026lt;\u0026#34;,\u0026#34;\u0026lt;\u0026lt;n_rows\u0026lt;\u0026lt;\u0026#34;,\u0026#34;\u0026lt;\u0026lt;n_cols\u0026lt;\u0026lt;std::endl; data.resize(n_cols * n_rows, number_of_images); for (int i = 0; i \u0026lt; number_of_images; i++) { for (int r = 0; r \u0026lt; n_rows; r++) { for (int c = 0; c \u0026lt; n_cols; c++) { unsigned char image = 0; file.read((char*)\u0026amp;image, sizeof(image)); data(r * n_cols + c, i) = (float)image; } } } } } 这里要说一下MNIST的ubyte数据集的结构\n前4个字节：魔数（Magic Number） - 一个32位整数，用于标识文件的类型。 接下来4个字节：图像数量（Number of Images） - 一个32位整数，指示文件中包含的图像数量。 接下来4个字节：图像的行数（Number of Rows） - 一个32位整数，表示每个图像的行数。 接下来4个字节：图像的列数（Number of Columns） - 一个32位整数，表示每个图像的列数。 接下来的字节：图像数据 - 连续的字节序列，每个字节表示一个像素的灰度值，通常是8位的无符号字符。图像数据以行为主排列，依次为每个图像的每一行。 转为float的是CNN的需求，能够方便范围与归一化、求导和反向传播以及激活函数的非线性需求\n读取模型 Network dnn=createNetwork_GPU();用来读取模型，Network类在network.h和network.cc声明与实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #ifndef SRC_NETWORK_H_ #define SRC_NETWORK_H_ #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026#34;./layer.h\u0026#34; #include \u0026#34;./loss.h\u0026#34; #include \u0026#34;./optimizer.h\u0026#34; #include \u0026#34;./utils.h\u0026#34; class Network { private: std::vector\u0026lt;Layer*\u0026gt; layers; // layer pointers Loss* loss; // loss pointer float BIN_FILE_DELIM = 0xFFFFFFFF; public: Network() : loss(NULL) {} ~Network() { for (int i = 0; i \u0026lt; layers.size(); i ++) { delete layers[i]; } if (loss) { delete loss; } } void add_layer(Layer* layer) { layers.push_back(layer); } void add_loss(Loss* loss_in) { loss = loss_in; } void forward(const Matrix\u0026amp; input); void backward(const Matrix\u0026amp; input, const Matrix\u0026amp; target); void update(Optimizer\u0026amp; opt); const Matrix\u0026amp; output() { return layers.back()-\u0026gt;output(); } float get_loss() { return loss-\u0026gt;output(); } /// Get the serialized layer parameters std::vector\u0026lt;std::vector\u0026lt;float\u0026gt; \u0026gt; get_parameters() const; /// Set the layer parameters void set_parameters(const std::vector\u0026lt; std::vector\u0026lt;float\u0026gt; \u0026gt;\u0026amp; param); /// Get the serialized derivatives of layer parameters std::vector\u0026lt;std::vector\u0026lt;float\u0026gt; \u0026gt; get_derivatives() const; /// Debugging tool to check parameter gradients void check_gradient(const Matrix\u0026amp; input, const Matrix\u0026amp; target, int n_points, int seed = -1); void save_parameters(std::string filename); void load_parameters(std::string filename); }; #endif // SRC_NETWORK_H_ 这里定义了Network的构造函数和析构函数，在创建Network的对象时将private变量loss设置为null，也就是不设置模型的损失函数，用delete将Network对象析构\n而 createNetwork_GPU() 方法在ece408net.cc中实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 Network createNetwork_GPU() { Network dnn; Layer* conv1 = new Conv_Custom(1, 86, 86, 4, 7, 7); Layer* pool1 = new MaxPooling(4, 80, 80, 2, 2, 2); Layer* conv2 = new Conv_Custom(4, 40, 40, 16, 7, 7); Layer* pool2 = new MaxPooling(16, 34, 34, 4, 4, 4); Layer* fc3 = new FullyConnected(pool2-\u0026gt;output_dim(), 32); Layer* fc4 = new FullyConnected(32, 10); Layer* relu1 = new ReLU; Layer* relu2 = new ReLU; Layer* relu3 = new ReLU; Layer* softmax = new Softmax; dnn.add_layer(conv1); dnn.add_layer(relu1); dnn.add_layer(pool1); dnn.add_layer(conv2); dnn.add_layer(relu2); dnn.add_layer(pool2); dnn.add_layer(fc3); dnn.add_layer(relu3); dnn.add_layer(fc4); dnn.add_layer(softmax); // loss Loss* loss = new CrossEntropy; dnn.add_loss(loss); //load weights dnn.load_parameters(\u0026#34;./build/weights-86.bin\u0026#34;); return dnn; } ","date":"2023-10-17T10:31:05+08:00","permalink":"http://localhost:1313/p/cuda%E5%AE%9E%E7%8E%B0cnn%E6%8E%A8%E7%90%86%E4%B8%8E%E5%8D%B7%E7%A7%AF/","title":"Cuda实现CNN推理与卷积"},{"content":"导语 doi is here\nAbstract 问题引入 目前互联网上的流量已被广泛加密，同时流量加密总是被攻击者滥用以隐藏其恶意行为，现有的加密恶意流量检测方法受到监督，它们依赖于已知攻击（例如，标记数据集）的先验知识。\n提出方法 提出了HyperVision，一种基于实时无监督机器学习的恶意流量检测系统。\n能够利用基于流量模式构建的紧凑内存图来检测加密恶意流量的未知模式。该图捕获由图结构特征表示的流交互模式，而不是特定已知攻击的特征。 我们开发了一种无监督图学习方法，通过分析图的连接性、稀疏性和统计特征来检测异常交互模式 建立了一个信息论模型来证明图保存的信息接近理想的理论边界。 Introduction 现有方法 深度数据包检测（DPI）的传统基于签名的方法在加密有效载荷的攻击下无效，加密流量具有与良性流量相似的特征，因此也可以逃避现有的基于机器学习。特别是，现有的加密流量检测方法受到监督，即依赖于已知攻击的先验知识，并且只能检测具有已知流量模式的攻击。此外，这些方法无法检测使用和不使用加密流量构建的攻击，并且由于加密和非加密攻击流量的特征显着不同，因此无法实现通用检测\n简而言之，现有方法无法实现无监督检测，也无法检测具有未知模式的加密恶意流量。特别是，加密的恶意流量具有隐蔽行为，这些方法无法捕获这些行为，这些方法根据单个流的模式检测攻击。但是，检测此类攻击流量仍然是可行的，因为即使攻击的单个流与良性攻击流相似，这些攻击涉及攻击者和受害者之间具有不同流交互的多个攻击步骤与良性流交互模式不同。\nHyperVision，这是一个实时检测系统，旨在通过分析流之间的交互模式来捕获加密恶意流量的足迹。特别是，它可以通过识别异常流交互（即不同于良性的交互模式）来检测具有未知足迹的加密恶意流。\n但是，构建用于实时检测的图形具有挑战性。我们不能简单地使用 IP 地址作为顶点，而传统的四元组流（源目的ip，源目的port）作为边来构建图，因为生成的密集图无法维持各种流之间的交互模式，例如，引起依赖爆炸问题 。\n收到流量尺寸分布的研究的启发，互联网上的大多数流都是短流，而大多数数据包与长流相关联，我们利用两种策略来记录不同大小的流，并在图中分别处理短流和长流的交互模式。\n我们设计了一种四步 轻量级 无监督 图学习方法，通过利用图上维护的丰富流交互信息来检测加密的恶意流量。\n首先，我们通过提取连通分量来分析图的连通性，并通过对高层次统计特征进行聚类来识别异常分量。通过排除良性分量，我们还显著减少了学习开销。 其次，我们根据在边特征中观察到的局部邻接关系对边进行预聚类。预聚类操作显著降低了特征处理开销，并确保了实时检测。 第三，我们使用Z3 SMT solver求解顶点覆盖问题来提取关键顶点，以最大程度地减少聚类的数量。 最后，根据每个临界顶点的连接边进行聚类，这些边位于预聚类产生的簇的中心，从而得到指示加密恶意流量的异常边。 此外，为了量化HyperVision基于图的流量记录相对于现有方法的优势，我们开发了一个流量记录熵模型，这是一个基于信息论的框架，从理论上分析恶意流量检测系统的现有数据源保留的信息量。这个框架表明NetFlow [19]和Zeek [86]无法保留高保真流量信息，而HyperVision中的图捕获了接近最优的流量信息，并且图中维护的信息量接近理想化数据源的理论上界。（这么屌啊？）此外，分析结果表明，HyperVision中的图形实现了比所有现有数据源更高的信息密度（即每单位存储的流量信息量），这是准确高效检测的基础。\n过两天读R. Zamir, “A proof of the fisher information inequality via a data processing argument,” IEEE Trans. Inf. Theory, vol. 44, no. 3, pp. 12461250, 1998.\n平台和数据集 我们使用英特尔的数据平面开发套件 （DPDK） [37] 对 HyperVision进行原型设计。为了广泛评估原型的性能，我们重放了92个攻击数据集，其中包括在我们的虚拟私有云 （VPC）中收集的80个新数据集，其中包含 1,500 多个实例。在 VPC 中，我们收集了 48 个典型的加密恶意流量，包括 （i） 加密泛洪流量，例如泛洪目标链路 [41];（ii） 网络攻击，例如利用网络漏洞 [64];（iii） 恶意软件活动，包括连接测试、依赖项更新和下载。\n此外，HyperVision 的平均检测吞吐量超过 100 Gb/s，平均检测延迟为 0.83 秒。\n省流 • 我们提出了 HyperVision，这是首个使用流交互图实现对未知模式的加密恶意流量进行实时无监督检测的方法。 • 我们开发了多种算法来构建内存中的图，使我们能够准确捕获不同流之间的交互模式。 • 我们设计了一种轻量级的无监督图形学习方法，通过图形特征来检测加密流量。 • 我们开发了一个由信息论建立的理论分析框架，以展示该图形捕获了接近最优的流量交互信息。 • 我们原型化了 HyperVision，并进行了广泛的实验，使用各种真实世界的加密恶意流量来验证其准确性和效率。\n名词解释 连通分量：在图论中，连通分量是一个图中的一个子图，其中任意两个顶点都可以通过边相连的路径相互访问。\nZ3 SMT solver：3（Z3 SMT solver）是由微软研究院开发的一个高性能的SMT（Satisfiability Modulo Theories）求解器。SMT 求解器是一种自动化工具，用于解决布尔公式、一阶逻辑公式和其他数学理论的判定问题。Z3 在各种计算机科学和工程领域都有广泛的应用，包括软件验证、形式化方法、人工智能、编译器优化和硬件验证等。\n英特尔的数据平面开发套件 （DPDK）：旨在优化数据包处理性能。它专注于高性能网络应用程序和数据平面开发，使开发人员能够在通用服务器硬件上实现高吞吐量和低延迟的数据包处理。它通过绕过操作系统内核，并在用户空间中实现网络协议栈，从而提供极低的延迟和高吞吐量。支持多核处理器，允许并行处理大量数据包。利用支持硬件加速的网络接口卡（NIC）来进一步提高性能。DPDK 是一个开源项目，开发人员可以根据其需求进行自定义和扩展。DPDK 通常用于构建高性能网络应用程序，如网络功能虚拟化（NFV）、防火墙、负载均衡、数据包过滤和路由等。它还用于云计算、边缘计算和网络设备。\nHyperVision 首先HyperVision以镜像来的路由器流量作为输入，确保不会干扰流量转发。在识别加密的恶意流量后，它可以与现有的中间恶意流量防御配合，以限制检测到的流量。重点检测使用加密流量构建的主动攻击。不考虑不会为受害者带来流量的被动攻击，例如流量窃听和被动流量分析\nHyperVision的设计目标如下：首先，它应该能够实现通用检测，即检测使用加密或非加密流量构建的攻击，从而确保攻击无法逃避流量加密的检测。其次，它能够实现实时高速流量处理，这意味着它可以识别通过加密流量是否是恶意的，同时产生低检测延迟。第三，HyperVision 执行的检测是不受监督的，这意味着它不需要任何加密恶意流量的先验知识。\n图构造 将流分为短流和长流，并分别记录它们的相互作用模式，以降低图的密度。\n使用不同的地址作为顶点，分别连接与短流和长流关联的边。聚合大量相似的短流，为一组短流构建一条边，从而减少维护流交互模式的开销。拟合长流中数据包特征的分布，构建与长流相关的边缘，从而保证了高保真记录的流交互模式，同时解决了传统方法中粗粒度流特征的问题。\n预处理图 通过提取连通分量来减少图的开销，并使用高级统计信息进行聚类。其中，聚类可以准确地检测出只有良性交互模式的组件，从而对这些良性组件进行过滤，减小图的规模。此外，我们进行了预聚类，并使用生成的聚类中心来表示图像中的识别的集群的边缘。（第五节详细讲）\n基于图的恶意流量检测 通过分析图特征来实现无监督加密恶意流量检测。\n图构造 流的分类 为了避免图构建过程中流之间的依赖爆炸，把流分成长流和短流，并且降低密度。下图显示了显示了2020年1月MAWI互联网流量数据集的流完成时间和流长度的分布，纵轴PDF是概率密度函数，可以看到不论是长流还是短流都在分布短时间、长长度更多。 利用短流合并后，图的稠密度显著下降 获取每个数据包的信息，并获取其源、目标地址、端口号和每个数据包的功能，包括协议、长度和到达间隔。我们开发了一种流量分类算法来对流量进行分类（附录A中的算法1）简单来说就是维护一个哈希表，键是hash(src,dest,src_post,dest_port)，值是流的所有数据包特征的序列(协议、数据包长度、到达间隔)，然后用一个定时器TIME_NOW，每隔JUDGE_INTERVAL检查一下，如果在这个interval里流发了多个数据包，就算他是长流，否则就说他是短流）【q，这个interval怎么设置？为什么后面说ssh暴力破解都是短流？这不是应该是短期发好多包吗？】\n短流聚合 我们观察到，大多数短流具有几乎相同的每个数据包的特征序列。我们设计了一种聚合短流的算法（附录A中的算法2）。当满足以下所有要求时，可以聚合一组流\n流具有相同的源和/或目标地址（为啥不是哈希表的键值一样） 流具有相同的协议类型 流的数量足够大，即当短流量的数量达到阈值AGG_LINE 我们为短流构建一条边，为所有流及其四个元组保留一个特征序列（即协议、数据包长度和到达间隔）\n长流的特征分布拟合 由于长流中的特征是集中分布的，我们使用直方图来表示长流中每个数据包特征的频率分布。直方图的每个条目表示一个数据包特征的频率，从而避免保留其长的每个数据包特征序列。具体来说，我们为每个长流中的每个数据包特征序列构建直方图，然后维护一个哈希表， 键为数据包特征序列，值为直方图。我们将数据包长度和到达间隔的桶宽度分别设置为 10 字节和 1 毫秒，以在拟合精度和开销之间进行权衡。 下图显示了数据集中的长流中已用桶的数量和最大桶的大小，可以看是集中分布的，即长流中的大多数数据包具有相似的包长度和到达间隔。长度拟合平均用11个桶，每个桶平均200个数据包；到达间隔拟合平均用121个桶，每个桶平均71个数据包。 ","date":"2023-10-16T20:34:22+08:00","permalink":"http://localhost:1313/p/detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/","title":"Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis"},{"content":"导语 果壳的校园网不给直接内网连远程桌面，感觉是因为划的子网之间不能互相通信，不知道是深澜故意的还是不小心的。todesk自然是可以，但是感觉免费的todesk画质略输一筹的同时延迟也有点小高。于是想到用frp的内网穿透来搞p2p的远程桌面。\n操作 实际上还是挺简单的，为数不多的坑是网上的教程都还是ini格式，但是在现在的版本里已经转为了toml、yaml等，而且参数好像也有些变化。\n去github下载frp的releas\n服务端 在你的服务器上装frps，并且配置toml文件，下载的frps.toml已经基本上写好了，基本啥也不用改，只要把最后的插件注释掉（或者你也可以把插件装了用）\n1 2 3 4 5 6 7 8 9 10 11 #[[httpPlugins]] #name = \u0026#34;user-manager\u0026#34; #addr = \u0026#34;127.0.0.1:9000\u0026#34; #path = \u0026#34;/handler\u0026#34; #ops = [\u0026#34;Login\u0026#34;] #[[httpPlugins]] #name = \u0026#34;port-manager\u0026#34; #addr = \u0026#34;127.0.0.1:9001\u0026#34; #path = \u0026#34;/handler\u0026#34; #ops = [\u0026#34;NewProxy\u0026#34;] 再改一下auth.token，这个token是你的frpc也要配置成一样的，相当于server对client的认证。\n1 auth.token = \u0026#34;hsijdfhsjdhf\u0026#34; # 这个感觉可以随便写，多复杂都行，反正你能连上你的服务器就能查 然后再改一改web界面的用户名密码端口啥的或者直接把web也注释了\n1 2 3 4 webServer.addr = \u0026#34;0.0.0.0\u0026#34; webServer.port = 7500 webServer.user = \u0026#34;dgsdgfsdfs\u0026#34; webServer.password = \u0026#34;dweqweas\u0026#34; 然后启动时一定要用-c指定toml配置文件，否则我也不知道他默认找的哪里的配置文件\n客户端 这里使用了xtcp的代理协议来进行p2p的内网穿透，如果想用其他方法可以参考官方文档（？是吗）\n在被控端和控制端都装上对应平台的frpc，并且配置frpc.toml\n配置frps的地址端口和token\n1 2 3 serverAddr = \u0026#34;1.1.1.1\u0026#34; serverPort = 7000 auth.token = \u0026#34;asfggsaddasd\u0026#34; # 这里要和服务端配的一样 把下面哪些示例配置全都注释掉，然后写上下面的内容\n1 2 3 4 5 6 7 [[proxies]] name = \u0026#34;rdesk\u0026#34; type = \u0026#34;xtcp\u0026#34; localIP = \u0026#34;127.0.0.1\u0026#34; # 本机 localPort = 3389 # 远程桌面连接 role = \u0026#34;server\u0026#34; secretKey = \u0026#34;akjndsghnkjadsfjh\u0026#34; 如果你的被控端的frpc设置开了web，那你应该可以再web界面看到你的xtcp的连接\n对了你还可以再你的frps和frpc里都指定一个user，这样你的proxy的name就会变成user.name的形式，这也就使你可以在server端配置多用户（指直接管name叫做aaa.xxx而不配置frps的user）\n同时在控制的机器那边也装上frpc，配置frps的地址端口和token\n1 2 3 serverAddr = \u0026#34;1.1.1.1\u0026#34; serverPort = 7000 auth.token = \u0026#34;asfggsaddasd\u0026#34; # 这里要和服务端配的一样 然后再加上\n1 2 3 4 5 6 7 [[visitors]] name = \u0026#34;rdesk_visitor\u0026#34; type = \u0026#34;xtcp\u0026#34; serverName = \u0026#34;rdesk\u0026#34; # 这里要和上面的name一致 secretKey = \u0026#34;akjndsghnkjadsfjh\u0026#34; # 这里要和上面的secretkey一致 bindAddr = \u0026#34;127.0.0.1\u0026#34; # 本机的ip地址 bindPort = 8000 然后使用frpc的同时也要用-c来指定配置文件\n./frpc.exe -c ./frpc.toml\nvisitors是在web里看不见的，看不到不要觉得奇怪。\n如果在server上可以看到都连上了，直接mstsc连就行了，连127.0.0.1:8000（也就是你在visitor里设置的地址端口）（这里是把log指向了console，所以可以直接看）\nwindwos防火墙会弹，同意了就完事了\n使用systemd让frps挂在后台 直接参考https://gofrp.org/zh-cn/docs/setup/systemd/\n","date":"2023-10-16T14:10:06+08:00","permalink":"http://localhost:1313/p/frp%E5%AE%9E%E7%8E%B0%E9%9A%A7%E9%81%93%E7%A9%BF%E9%80%8F%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/","title":"Frp实现隧道穿透远程桌面"},{"content":"First take 旧的博客忘了同步，导致换了电脑之后source找不到了（哭哭 于是整了个新博客儿，采用小号githubpage+hugo的解决方法 当成一个新的云笔记吧，不过其实也搞了本地md+github的方案，但是肯定没有静态博客看着爽了，整，都可以整！ 顺便给旧的hexo博客指路https://blog.pillar.fun\n","date":"2023-10-03T17:54:20+08:00","permalink":"http://localhost:1313/p/first_blog/","title":"First_blog"},{"content":".+++ title = \u0026lsquo;机器学习笔记\u0026rsquo; date = 2023-12-27T19:02:35+08:00 draft = true +++\n导语 ucas2023秋 周晓飞机器学习课程笔记\n这门课说是会给题库，但是知识还是学给自己比较好。\n笔记 题库知识点 分类/回归：x-\u0026gt;y的映射，分类问题y为离散值，回归问题y为连续值\n分类强调依据类别标签 y 对样本 x 空间的划分，回归强调 x 与回归值的拟合。\n聚类：学习 x 到类簇 y 的映射\n\u0026ldquo;类簇\u0026quot;通常指的是一组相似的数据点的集合。这种相似性通常是通过某种度量（例如欧几里得距离或余弦相似性）来确定的。\n最后一节课 ","date":"0001-01-01T00:00:00Z","permalink":"http://localhost:1313/p/","title":""}]