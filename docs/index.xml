<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>π1l4r_のblog</title>
        <link>https://pillar23.github.io/</link>
        <description>Recent content on π1l4r_のblog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>pill4r</copyright>
        <lastBuildDate>Fri, 16 Feb 2024 16:28:03 +0800</lastBuildDate><atom:link href="https://pillar23.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>TFE GNN</title>
        <link>https://pillar23.github.io/p/tfe-gnn/</link>
        <pubDate>Fri, 16 Feb 2024 16:28:03 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/tfe-gnn/</guid>
        <description>&lt;h1 id=&#34;abstract&#34;&gt;abstract&lt;/h1&gt;
&lt;p&gt;目的：实现加密流量[VPN、tor]的分类&lt;/p&gt;
&lt;p&gt;现有局限性：只能提取低级别特征，基于统计的方法对&lt;strong&gt;短流&lt;/strong&gt;无效，对header和payload采取&lt;strong&gt;不平等&lt;/strong&gt;的处理，难以挖掘字节之间的潜在相关性。&lt;/p&gt;
&lt;p&gt;提出方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于逐点互信息(PMI)的字节级流量图构建方法&lt;/li&gt;
&lt;li&gt;基于图神经网络(TFE-GNN)进行特征提取的时序融合编码器模型&lt;/li&gt;
&lt;li&gt;引入了一个双嵌入层、一个基于GNN的流量图编码器以及一个交叉门控特征融合机制。[分别嵌入header和payload，然后通过融合实现数据增强]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;结果：两个真实数据集（WWT和ISCX）优于SOTA&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708314808397.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708314808397&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;introduction&lt;/h1&gt;
&lt;p&gt;加密流量保存用户隐私同时也给了攻击者藏身的机会。&lt;/p&gt;
&lt;p&gt;传统的数据包检测（DPI）挖掘数据包中的潜在模式或关键词，面对加密数据包时&lt;strong&gt;耗时&lt;/strong&gt;且准确性低。&lt;/p&gt;
&lt;p&gt;由于动态端口的应用，基于端口的工作不再有效。&lt;/p&gt;
&lt;p&gt;通过数据流的统计特征（e.g.数据包长度的平均值）采用机器学习分类器（e.g.随机森林）来实现分类的方法，需要手工制作的特征工程，并且在某些情况下可能会由于不可靠/不稳定的fow级统计信息而失败。与长流相比，短流的统计特征有更大的偏差（e.g.长度通常服从长尾分布）意味着不可靠的统计特征普遍存在。我们使用&lt;strong&gt;数据包字节&lt;/strong&gt;而非统计特征。&lt;/p&gt;
&lt;p&gt;GNN 可以识别图中隐含的特定拓扑模式，以便我们可以用预测标签对每个图进行分类。目前大多数GNN根据数据包之间的相关性来构建图，这实际上是统计特征的另一种使用形式，导致上述问题。&lt;/p&gt;
&lt;p&gt;用了数据包字节的方法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;平等地对待header和payload，忽略了它们之间的含义差异。&lt;/li&gt;
&lt;li&gt;原始字节利用不足，只是将数据包视为节点，将原始字节作为节点特征，不能充分利用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文提出了一种基于逐点互信息（PMI）的字节级图构建方法，一种基于图神经网络(TFE-GNN)进行特征提取的时序融合编码器模型。通过挖掘字节之间的相关性来构建流量图，用作TFE-GNN的输入。&lt;/p&gt;
&lt;p&gt;TFE-GNN由三大子模块组成（即双嵌入、流量图编码器和交叉门控特征融合机制）。双嵌入层分别嵌入header和payload；图编码器将图编码为高维图向量；交叉门控特征融合机制对header和payload的图向量融合，得到数据包的整体表示向量。&lt;/p&gt;
&lt;p&gt;使用端到端训练（从输入数据到最终输出直接进行训练，而无需将任务分解为多个独立的阶段或模块），采用时间序列模型，获得下游任务的预测结果。&lt;/p&gt;
&lt;p&gt;实验使用了自收集的WWT（WhatsApp、WeChat、Telegram）和公开的ISCX数据集，与十几个baseline比较得出TFE-GNN效果最好。&lt;/p&gt;
&lt;h1 id=&#34;preliminary&#34;&gt;preliminary&lt;/h1&gt;
&lt;p&gt;1、图的定义&lt;/p&gt;
&lt;p&gt;G = { $V,\varepsilon,X$}表示一个图，V是节点集合，$\varepsilon$是边集，X是节点的初始特征矩阵（每个节点的特征向量拼起来）&lt;/p&gt;
&lt;p&gt;$A$是大小为$\lvert V \lvert * \lvert V \lvert$图的邻接矩阵&lt;/p&gt;
&lt;p&gt;$N(v)$是节点v相邻的节点&lt;/p&gt;
&lt;p&gt;$d_l$是第l层的嵌入维度&lt;/p&gt;
&lt;p&gt;TS(traffic segment)=[$P_{t_1},P_{t_2}&amp;hellip;P_{t_n}$]是一段时间内的数据包的集合。$P_{t_i}$是时间戳为$t_i$的数据包，n是流量序列的长度。$t_1,t_2$是流量序列的开始和结束时间。&lt;/p&gt;
&lt;p&gt;2、加密流量分类&lt;/p&gt;
&lt;p&gt;M是训练样本数量&lt;/p&gt;
&lt;p&gt;N是分类类别&lt;/p&gt;
&lt;p&gt;$bs^j_i=[b^{ij}_1, b^{ij}_2,&amp;hellip;,b^{ij}_m]$，m是字节序列长度，$b^{ij}_k$是第i个流量样本第j个字节序列的第k个字节&lt;/p&gt;
&lt;p&gt;$s_i=[bs^i_1,bs^i_2,&amp;hellip;,bs^i_n)]$，n是序列长度$bs^i_j$为第i个样本的第j个字节序列，可以理解为就是TS&lt;/p&gt;
&lt;p&gt;3、 MP-GNN&lt;/p&gt;
&lt;p&gt;MP-GNN 是 Message Passing Graph Neural Network（消息传递图神经网络）的简称，节点嵌入向量可以通过特定的聚合策略将节点的嵌入向量集成到邻域中，从而迭代更新节点嵌入向量。&lt;/p&gt;
&lt;p&gt;第l层 MP-GNN 可以形式化为两个过程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708310294876.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708310294876&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其中$h^{(l)}_u,h^{(l)}_v$是节点u和v在第l层的嵌入向量，$m^{(l)}_u$是l层中节点u的计算信息，$MSG^{(l)}$是消息计算函数，$AGG^{(l)}$是消息聚合函数，$\theta$是他们对应的参数&lt;/p&gt;
&lt;h1 id=&#34;methodology&#34;&gt;methodology&lt;/h1&gt;
&lt;h2 id=&#34;字节级流量图构造-byte-level-traffic-graph-construction&#34;&gt;字节级流量图构造 Byte-level Traffic Graph Construction&lt;/h2&gt;
&lt;p&gt;节点：某一个字节，注意相同的字节值共享同一个节点，因此节点个数不会超过256，这样能够保持图在一定的规模下，不会太大。&lt;/p&gt;
&lt;p&gt;字节之间的相关性表示：采用点互信息（PMI）来建模两个字节之间的相似性，字节i和字节j的相似性用$PMI(i,j)$表示。&lt;/p&gt;
&lt;p&gt;边：根据PMI值来构造边，PMI值为正：表示字节之间的语义相关性高；而PMI值为零或负值：表示字节之间的语义相关性很小或没有。因此，我们只在PMI值为正的两个字节之间创建一条边。&lt;/p&gt;
&lt;p&gt;节点特征：每个节点的初始特征为字节的值，维度为1，范围为[0,255]&lt;/p&gt;
&lt;p&gt;图构建：由于$PMI(i,j)=PMI(j,i)$，因此该图是个无向图。&lt;/p&gt;
&lt;h3 id=&#34;pmi&#34;&gt;PMI&lt;/h3&gt;
&lt;p&gt;PMI：是一种用于衡量两个事件之间相关性的统计量。&lt;/p&gt;
&lt;p&gt;$$
PMI(A, B) = \log \frac{P(A, B)}{P(A) \cdot P(B)}
$$&lt;/p&gt;
&lt;p&gt;值大于零，则表示 A 和 B 之间有正相关性；如果值等于零，则表示它们之间没有关联；如果值小于零，则表示它们之间有负相关性。&lt;/p&gt;
&lt;h2 id=&#34;双嵌入-dual-embedding&#34;&gt;双嵌入 dual embedding&lt;/h2&gt;
&lt;p&gt;原因：字节值通常用作进一步向量嵌入的初始特征。具有不同值的两个字节对应两个不同的嵌入向量。然而，字节的含义不仅随字节值本身而变化，还随它所在的字节序列的部分而变化。换句话说，在数据包的header和payload中，具有相同值的两个字节的表示含义可能完全不同。对于header和payload，使用&lt;strong&gt;两个不共享参数的嵌入层&lt;/strong&gt;的双嵌入，嵌入矩阵分别是$E_{header}$和$E_{payload}$&lt;/p&gt;
&lt;h2 id=&#34;交叉门控特征融合的流量图编码器-traffic-graph-encoder-with-cross-gated-feature-fusion&#34;&gt;交叉门控特征融合的流量图编码器 Traffic Graph Encoder with Cross-gated Feature Fusion&lt;/h2&gt;
&lt;p&gt;因为要double embedding，所以encoder也要两个。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708314903093.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708314903093&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这里堆叠了N个GraphSAGE&lt;/p&gt;
&lt;h3 id=&#34;graphsage&#34;&gt;GraphSAGE&lt;/h3&gt;
&lt;p&gt;对于图 G 中的每个节点 v，GraphSAGE 通过使用节点 v 的度数归一化其嵌入向量，计算来自每个相邻节点的消息。&lt;/p&gt;
&lt;p&gt;通过逐元素均值运算（element-wise mean operation）计算所有相邻节点$N(v)$的整体消息，并通过串联运算聚合整体消息以及节点v的嵌入向量&lt;/p&gt;
&lt;p&gt;对节点A的嵌入向量进行非线性变换，完成一个GraphSAGE层的正向过程&lt;/p&gt;
&lt;p&gt;GraphSAGE的消息聚合和计算可以描述为&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708315615539.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708315615539&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;$m^{(l)}_{N(v)}$是将v节点所有临接节点的上一层嵌入向量求平均的结果&lt;/p&gt;
&lt;p&gt;$h_v^{(l)}$是本层v节点的嵌入向量，$\sigma(\cdot)$是激活函数，$CONCAT(\cdot)$是连接函数。然后通过BatchNorm对h进行批量归一化&lt;/p&gt;
&lt;p&gt;激活函数选择PReLU，将每个负元素值按不同因子缩放，不但引入了非线性，还由于每个负元素的缩放因子的不同而起到类似于注意力机制的作用。&lt;/p&gt;
&lt;p&gt;由于深度GNN模型中的过度平滑问题，我们最多只堆叠GraphSAGE4层，并将输出拼接起来。这与跳转知识网络（JKL）相类似&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708316388852.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708316388852&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;最后，通过对每个节点的最终嵌入使用meanpooling来得到图嵌入&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708316889596.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708316889596&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;用$g_h,g_p$表示header和payload得到的图嵌入&lt;/p&gt;
&lt;h2 id=&#34;cross-gated-feature-fusion&#34;&gt;Cross-gated Feature Fusion&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708316997715.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708316997715&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;交叉门控特征融合，这个模块的目标是将$g_h,g_p$融合，获取门控矢量$s_h,s_p$。&lt;/p&gt;
&lt;p&gt;如上图，我们用了两个filter，每个filter的组成都是线性层、PReLU、线性层、Sigmoid。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708317405446.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708317405446&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其中w和b分别是线性层的weight和bias。z 是数据包字节的整体表示向量。&lt;/p&gt;
&lt;h2 id=&#34;端到端的下游任务训练-end-to-end-training-on-downstream-tasks&#34;&gt;端到端的下游任务训练 End-to-End Training on Downstream Tasks&lt;/h2&gt;
&lt;p&gt;由于我们已经将流量段中每个数据包的原始字节编码为表示向量z，因此可以将段级分类任务视为时间序列预测任务。&lt;/p&gt;
&lt;p&gt;这里我们使用双层Bi-LSTM作为baseline，他的输出喂给一个带PReLU的两层线性分类器，使用交叉熵作为损失函数&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708318165707.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708318165707&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;n是长度，CE是交叉熵，y是ground truth（标注数据的分类标签）&lt;/p&gt;
&lt;p&gt;实验部分还有一个用transformer的&lt;/p&gt;
&lt;h1 id=&#34;experiments&#34;&gt;experiments&lt;/h1&gt;
&lt;p&gt;介绍了实验设置，在很多数据集和baseline上做了对比实验，&lt;strong&gt;做了消融实验（good）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;还分析了TFE-GNN的灵敏度，回答了&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个组件的功能&lt;/li&gt;
&lt;li&gt;哪个GNN架构效果最好&lt;/li&gt;
&lt;li&gt;TFE-GNN的复杂度如何&lt;/li&gt;
&lt;li&gt;超参数的变化在多大程度上会影响TFE-GNN的有效性&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;实验设置&#34;&gt;实验设置&lt;/h2&gt;
&lt;p&gt;数据集：ISCX VPN-nonVPN , ISCX Tor-nonTor , self-collected WWT datasets.&lt;/p&gt;
&lt;p&gt;预处理：对于每个数据集，筛除&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;空流或空段：所有数据包都没有有效负载（用于establish 连接）&lt;/li&gt;
&lt;li&gt;超长流或超长段：长度（即数据包数）大于 10000&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;然后对于每个数据包，删掉以太网标头，源 IP 地址和目标 IP 地址以及端口号都将被删除，以消除IP地址和端口号的敏感信息的干扰##&lt;/p&gt;
&lt;p&gt;细节：一个样本的最大数据包数设置为50，最大有效负载字节长度和最大标头字节长度分别设置为 150 和 40，PMI 窗口大小设置为 5&lt;/p&gt;
&lt;p&gt;epoch设置为120，lr为1e-2，用Adam优化器分512批次将lr从1e-2衰减到1e-4。warmup为0.1，droupout为0.2.\&lt;/p&gt;
&lt;p&gt;运行了10次实验。&lt;/p&gt;
&lt;p&gt;用AC、PR、RC和F1做评估&lt;/p&gt;
&lt;p&gt;和基于传统特征工程的方法（即 AppScanner [31]、CUMUL [23]、K-FP （K-Fingerprinting） [8]、FlowPrint [32]、GRAIN [43]、FAAR [19]、ETC-PS [40]）、基于深度学习的方法（即 FS-Net [18]、 EDC [16]、FFB [44]、MVML [4]、DF [30]、ET-BERT [17]）和基于图神经网络的方法（即 GraphDApp [29]、ECD-GNN [11]）做比较。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708319331300.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708319331300&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708319345719.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708319345719&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;消融实验&#34;&gt;消融实验&lt;/h2&gt;
&lt;p&gt;在 ISCX-VPN 和 ISCX-Tor 数据集上对 TFE-GNN 进行了消融实验，分别将头、有效载荷、双嵌入模块、跳跃知识网络式串联、交叉门控特征融合和激活函数以及批量归一化分别表示为 &amp;lsquo;H&amp;rsquo;、&amp;lsquo;P&amp;rsquo;、&amp;lsquo;DUAL&amp;rsquo;、&amp;lsquo;JKN&amp;rsquo;、&amp;lsquo;CGFF&amp;rsquo; 和 &amp;lsquo;A&amp;amp;N&amp;rsquo;。不仅验证了TFE-GNN中每个组件的有效性，而且还测试了一些替代模块或操作的影响，用sum、max替换mean，用GRU、transformer替换LSTM&lt;/p&gt;
&lt;p&gt;（只用H或P就不需要DUAL和CGFF）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708319828833.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708319828833&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;得出结论&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据包标头在分类中起着比数据包有效载荷更重要的作用，不同的数据集具有不同级别的标头和有效载荷重要性&lt;/li&gt;
&lt;li&gt;使用双嵌入使 f1 分数分别提高了 3.63% 和 0.95%，这表明其总体有效性。JKN样串联和交叉门控特征融合在两个数据集上都以相似的幅度增强了TFE-GNN的性能。&lt;/li&gt;
&lt;li&gt;缺少激活函数和批量归一化在两个数据集上都可以看到显著的性能下降，证明了其必要性&lt;/li&gt;
&lt;li&gt;用sum替换mean在两个数据集上分别差了11.1%和29.64%，用max替换mean在VPN上差很多在tor上只差一点&lt;/li&gt;
&lt;li&gt;用GRU替换LSTM导致两个都差10%左右，transformer替换LSTM导致VPN差了40%左右，tor上只差一点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;换GNN架构为GAT, GIN, GCN and SGC，还是GraphSAGE最好&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708321015036.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708321015036&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;此外，通过TFE-GNN可以快捷的拓展一个segment级的全局特征&lt;/p&gt;
&lt;p&gt;复杂度&lt;/p&gt;
&lt;p&gt;TFE-GNN 在模型复杂度相对较小的情况下，在公共数据集上实现了最显着的改进。虽然ET-BERT在ISCX-nonVPN数据集上达到了可比的结果，但ET-BERT的FLOP大约是TFEGNN的五倍，模型参数的数量也增加了一倍，这通常表明模型推理时间更长，需要更多的计算资源。此外，ETBERT的预训练阶段非常耗时，由于预训练期间有大量的额外数据，并且模型复杂度高，因此成本很高。相比之下，TFE-GNN可以实现更高的精度，同时降低训练或推理成本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708321171217.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708321171217&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;双重嵌入维度的影响。为了研究双嵌入层隐藏维度的影响，我们进行了灵敏度实验，结果如图a所示。正如我们所看到的，当嵌入维度低于 100 时，f1 分数会迅速增加。在此之后，随着维度的变化，模型性能趋于稳定。为了减少计算消耗，选取50作为默认维度&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1708321219227.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1708321219227&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;PMI窗口大小的影响。从图 b 中可以看出，较小的窗口大小通常会导致更好的 f1 分数。窗口越大图中添加的边就越多，由于图太密集，模型将更难分类。&lt;/p&gt;
&lt;p&gt;段长的影响。从图 c 中，我们可以得出一个结论，即用于训练的短段长度通常会使性能更好。&lt;/p&gt;
&lt;h1 id=&#34;conclusion-and-future-work&#34;&gt;conclusion and future work&lt;/h1&gt;
&lt;p&gt;我们提出了一种构建字节级流量图的方法和一个名为 TFE-GNN 的模型，用于加密流量分类。字节级流量图构造方法可以挖掘原始字节之间的潜在相关性并生成判别性流量图。TFE-GNN 旨在从构建的流量图中提取高维特征。最后，TFE-GNN可以将每个数据包编码为一个整体表示向量，该向量可用于一些下游任务，如流量分类。选择了几个基线来评估 TFE-GNN 的有效性。实验结果表明，所提模型全面超越了WWT和ISCX数据集上的所有基线。精心设计的实验进一步证明了TFE-GNN具有很强的有效性。&lt;/p&gt;
&lt;p&gt;将来，我们将尝试在以下限制方面改进 TFE-GNN。（1）有限的图构建方法。所提模型的图拓扑结构是在训练过程之前确定的，这可能会导致非最佳性能。此外，TFE-GNN无法应对每个数据包的原始字节中隐含的字节级噪声。（2） 字节序列中隐含的未使用的时间信息。字节级 trafc 图的构造没有引入字节序列的显式时间特征。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>网络认证技术笔记</title>
        <link>https://pillar23.github.io/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 27 Dec 2023 19:07:10 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;ucas2023秋 林璟锵、马存庆 网络认证技术笔记&lt;/p&gt;
&lt;p&gt;说是开课以来从未有过挂科选手，但是想得不错的分数还是要努努力，进自己脑子的知识才是最好的知识&lt;/p&gt;
&lt;h1 id=&#34;笔记&#34;&gt;笔记&lt;/h1&gt;
&lt;p&gt;网络认证技术 ≈ 密码学+计算机网络&lt;/p&gt;
&lt;p&gt;网络认证：在信息系统/网络环境中，实现身份的确认。目标：在不可信的网络环境中确认主体是谁，有什么属性、权限、能力&lt;/p&gt;
&lt;p&gt;身份确认的主体：人、设备、软件服务……&lt;/p&gt;
&lt;p&gt;PKI：公钥基础设施&lt;/p&gt;
&lt;p&gt;CA：认证中心，生成数字证书&lt;/p&gt;
&lt;p&gt;CA是PKI的核心组成成分，但是在很多地方把CA和PKI混用了。&lt;/p&gt;
&lt;h1 id=&#34;考试用&#34;&gt;考试用&lt;/h1&gt;
&lt;p&gt;两个半小时&lt;/p&gt;
&lt;p&gt;题型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 2分&lt;/li&gt;
&lt;li&gt;简答 5-8分
&lt;ol&gt;
&lt;li&gt;建议不要空这&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;关键是原理之类的要记住的&lt;/p&gt;
&lt;h2 id=&#34;01-导言意义不大&#34;&gt;01 导言（意义不大）&lt;/h2&gt;
&lt;p&gt;相关概念&lt;/p&gt;
&lt;h2 id=&#34;02-密码学基础会涉及题目需要复习复习&#34;&gt;02 密码学基础（会涉及题目，需要复习复习）&lt;/h2&gt;
&lt;p&gt;对称：加解密&lt;/p&gt;
&lt;p&gt;非对称：签名&lt;/p&gt;
&lt;p&gt;光看密钥长度不能知道强度，RSA1024bits=ECC160bits。短密钥可以达到高强度&lt;/p&gt;
&lt;p&gt;哈希：验证&lt;/p&gt;
&lt;p&gt;消息鉴别码：MAC=C(K,M)，K为密钥M为消息，把密钥跟着一块哈希了&lt;/p&gt;
&lt;p&gt;可鉴别加密CCM、GCM、AEAD（简单了解）&lt;/p&gt;
&lt;p&gt;国外的密码基本原理不细说了&lt;/p&gt;
&lt;p&gt;国产的了解一下&lt;/p&gt;
&lt;p&gt;SM2 非对称 ECC &amp;mdash; 椭圆曲线 知道基于椭圆曲线域上的离散对数困难问题。 替换RSA&lt;/p&gt;
&lt;p&gt;SM3 哈希 256bit和sha256差不多 分组长度512bit，摘要值长度256bit&lt;/p&gt;
&lt;p&gt;SM4 分组工作模式&lt;/p&gt;
&lt;p&gt;ECB：对每个块独立加密：明文同样的块会加密成同样的密文&lt;/p&gt;
&lt;p&gt;CBC：明文先与上一个密文异或在加密，需要初始化向量&lt;/p&gt;
&lt;p&gt;OFB：将块密码转为流密码，生成密钥流的块&lt;/p&gt;
&lt;p&gt;CTR（ICM、SIC）：将块密码变为流密码，通过递增加密计数器产生密钥流&lt;/p&gt;
&lt;p&gt;ZUC 流密码&lt;/p&gt;
&lt;p&gt;128位的初始密钥key和128位的初始向量iv来作为输入。每个时钟周期能生成32bit&lt;/p&gt;
&lt;p&gt;共享密钥问题-&amp;gt;为什么要有非对称的原因-&amp;gt;数字签名&lt;/p&gt;
&lt;h2 id=&#34;03-口令鉴别&#34;&gt;03 口令鉴别&lt;/h2&gt;
&lt;p&gt;client 用复杂口令，不要告诉别人，次数限制&lt;/p&gt;
&lt;p&gt;传输 使用已被验证的安全信道&lt;/p&gt;
&lt;p&gt;server 存储，验证&lt;/p&gt;
&lt;h2 id=&#34;04-基于密码技术的鉴别&#34;&gt;04 基于密码技术的鉴别&lt;/h2&gt;
&lt;p&gt;两大类：&lt;/p&gt;
&lt;p&gt;对称： 有没有密钥&lt;/p&gt;
&lt;p&gt;提一个协议框架，让你看有没有什么错，一些参数有什么用【用什么方式可以抵抗什么攻击】&lt;/p&gt;
&lt;p&gt;replay attack（重放攻击）：通过加一个nonce抵抗&lt;/p&gt;
&lt;p&gt;oracle session attack（就是攻击者使另一方帮自己来计算）：让u和v不同。比如u为加密，v为解密，被挑战方只能加密，就不能被当成解密服务器了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705143946346.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705143946346&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Parallel Session attack：p(), q()与方向有关。从而攻击者不能利用服务器的计算。比如发起者会加一个xor，被挑战者会加一个左移&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705144131279.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144131279&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;offset attack：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705144243068.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144243068&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;把返回的东西改为E(f()#E(g))&lt;/p&gt;
&lt;p&gt;可信第三方，kobras&lt;/p&gt;
&lt;p&gt;非对称：数字签名和验证&lt;/p&gt;
&lt;p&gt;单向（带一个时间戳之类的约定好的东西）、双向（A发给B后B还要发给A）&lt;/p&gt;
&lt;p&gt;PPT标红好好看看&lt;/p&gt;
&lt;h2 id=&#34;0506-pki技术&#34;&gt;05+06 PKI技术&lt;/h2&gt;
&lt;p&gt;CA：认证机构，权威第三方，公钥（证书）可信发布[根CA、子CA]&lt;/p&gt;
&lt;p&gt;RA：注册机构，审查信息，防止CA职能太多导致一个出问题导出都出问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705144615935.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705144615935&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;repository（存数据的吧）&lt;/p&gt;
&lt;p&gt;CRL（Certificate Revocation List，证书撤销列表）&lt;/p&gt;
&lt;p&gt;Online Certificate Status Protocol（OCSP）一种通信协议，专门用于检查证书是否已经被撤销 相应的服务器称为OCSP Server-&amp;gt;（证书有三种状态）Good、Revoked、Unknown ：未撤销、已经撤销、未知&lt;/p&gt;
&lt;p&gt;ASN.1-基本数据类型-DER编码-sequence-implicit/explicit tag  稍微看一下&lt;/p&gt;
&lt;h2 id=&#34;07-证书拓展&#34;&gt;07 证书拓展&lt;/h2&gt;
&lt;p&gt;证书基本域&lt;/p&gt;
&lt;p&gt;证书扩展域 X.509版本3 18种，了解功能即可&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704713547641.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704713547641&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;拓展有关键和非关键，如果关键出了错（识别不出来），直接认定证书非法。非关键出错则忽略拓展&lt;/p&gt;
&lt;p&gt;Basic Constraints：区分是否是CA证书（能否签发其他证书）以及路径的深度（说明CA可以有多少层次的下级）&lt;/p&gt;
&lt;p&gt;Authority Key Identifier：证书链中可能有多个公钥，这个确定哪个是用来验证证书的颁发者（CA）的公钥&lt;/p&gt;
&lt;p&gt;Subject Key Identifier：证书链中可能有多个公钥，确定哪个是证书自己的公钥&lt;/p&gt;
&lt;p&gt;Key Usage：密钥的用途。7种+2种辅助用途&lt;/p&gt;
&lt;p&gt;Private Key Usage Period：给出证书有效的开始到结束的时间&lt;/p&gt;
&lt;p&gt;Issuer Alternative Name：放置签发者（CA）的消息（DN存放CA信息，子CN没法用DN，就用这来放）&lt;/p&gt;
&lt;p&gt;Subject Alternative Name：放置证书拥有者的消息&lt;/p&gt;
&lt;p&gt;Subject Directory Attributes：可加入任何与Subject有关的信息，例如，民族、生日等&lt;/p&gt;
&lt;p&gt;Name Constraints：限制下级CA所能够签发证书的订户的名字空间（只在下级CA中有用）&lt;/p&gt;
&lt;p&gt;Certificate Policies（CP）：区分不同证书的安全等级&lt;/p&gt;
&lt;p&gt;Inhibit Any-Policy：（CP的Any-Policy指对于该CA所签发的订户证书的CP没有限制），值是整数N，表示：在证书路径中，本证书之下的N个证书可带有Any-Policy的证书&lt;/p&gt;
&lt;p&gt;Policy Mappings：说明了不同CA域之间的CP等级的相互映射关系&lt;/p&gt;
&lt;p&gt;Policy Constraints：对于证书认证路径的策略映射过程中，有关CP的处理，进行限制。N：在N个证书后，不允许再进行策略映射；M：在M个证书后，就必须要有认识的、明确的CP&lt;/p&gt;
&lt;p&gt;Extended Key Usage：证书/密钥可用的用途（拓展）&lt;/p&gt;
&lt;p&gt;CRL Distribution Points：和应用系统约定在哪儿获取CRL&lt;/p&gt;
&lt;p&gt;Freshest CRL：增量CRL情况下，获取最新的增量CRL的地址&lt;/p&gt;
&lt;p&gt;Authority Information Access：如何在Internet上面，访问一些CA的信息（目前只有 1、上级CA的情况 2、OCSP服务器的情况两个信息）&lt;/p&gt;
&lt;p&gt;Subject Information Access： l如何在Internet上面，访问一些用户的信息 （目前只有 1、资料库的地址（针对CA）2、TSA服务地址（针对TSA服务器））&lt;/p&gt;
&lt;h2 id=&#34;08-pki信任体系&#34;&gt;08 PKI信任体系&lt;/h2&gt;
&lt;p&gt;信任模型&lt;/p&gt;
&lt;p&gt;单根CA&lt;/p&gt;
&lt;p&gt;多根CA 根之间要互相通信-&lt;strong&gt;CTL&lt;/strong&gt;（用户自主+权威发布）&lt;em&gt;&lt;strong&gt;沟通方式、原理、优缺点，应用&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CTL（信任锚）由权威机构统一地发布1个可信的信任锚列表（Certificate Trust List）包括多个根CA证书文件的HASH结果和受信任CA对其签名&lt;/p&gt;
&lt;p&gt;【信任锚里有根CA证书的hash、其他CA证书、CRL、信任策略和规则等。然后由一个我信任的CA对CTL签名，一般不用CTL里信任的CA来签名。】&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705156671337.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705156671337&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;方式：1、不同PKI域用同一个CTL 2、加一个ACA的信任锚说明哪些根CA是可以信任的&lt;/p&gt;
&lt;p&gt;交叉认证-网状mesh-&amp;gt;桥CA&lt;/p&gt;
&lt;p&gt;相当于将对方CA认作是我的子CA。&lt;/p&gt;
&lt;p&gt;mesh-&amp;gt;信任链变成信任网&lt;/p&gt;
&lt;p&gt;桥CA-&amp;gt;不同域之间的证书传递&lt;/p&gt;
&lt;h2 id=&#34;09-证书撤销&#34;&gt;09 证书撤销&lt;/h2&gt;
&lt;p&gt;验证签名-验证有效期-验证撤销状态&lt;/p&gt;
&lt;p&gt;撤销状态 CRL、OCSP、CRT 原理&lt;/p&gt;
&lt;p&gt;CA/CRL Issuer定期地签发CRL CRL，certificate revocation list&lt;/p&gt;
&lt;p&gt;完全CRL－Complete CRL：所有CRL信息一次发布&lt;/p&gt;
&lt;p&gt;增量CRL－Delta CRL：发布新增的CRL信息发布&lt;/p&gt;
&lt;p&gt;直接CRL－Direct CRL：证书签发者签发CRL&lt;/p&gt;
&lt;p&gt;间接CRL－Indirect CRL：使用CRL issuer签发CRL&lt;/p&gt;
&lt;p&gt;OCSP在线证书状态协议 Online Certificate Status Protocol 在线服务器&lt;/p&gt;
&lt;p&gt;CRT：证书撤销树，对于各证书序列号进行一定的结构化，形成了HASH链&lt;/p&gt;
&lt;p&gt;使用了merkle hash tree【区块链信任算法】&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705164608101.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705164608101&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;拿加粗的子哈希算哈希，就可以推出根hash，验证起来需要更少的那啥&lt;/p&gt;
&lt;h2 id=&#34;10-tls&#34;&gt;10 TLS&lt;/h2&gt;
&lt;p&gt;handshake怎么shake的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705163142817.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705163142817&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;增加一个server对client的鉴别&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705208493971.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705208493971&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如果server证书只能签名不能加密，则要生成一个临时公钥，签名后发给client【ServerKeyExchange】&lt;/p&gt;
&lt;p&gt;两张图里的消息有什么含义 1.3和1.2的区别&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705208812117.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705208812117&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;直接在client hello中发了选择算法的key（用server 公钥加密）&lt;/p&gt;
&lt;h2 id=&#34;105wifi认证&#34;&gt;10.5wifi认证&lt;/h2&gt;
&lt;p&gt;WPA-PSK共享口令 （路由器上做）&lt;/p&gt;
&lt;p&gt;WPA-802.1X 基于账号的身份鉴别 （身份鉴别server）&lt;/p&gt;
&lt;p&gt;客户端或网页 （微信、短信）&lt;/p&gt;
&lt;h2 id=&#34;11-不考&#34;&gt;11 不考&lt;/h2&gt;
&lt;h2 id=&#34;12-pki安全增强&#34;&gt;12 PKI安全增强&lt;/h2&gt;
&lt;h3 id=&#34;入侵容忍-解决了什么问题怎么解决的-原理&#34;&gt;入侵容忍 解决了什么问题？怎么解决的 原理&lt;/h3&gt;
&lt;p&gt;解决了在入侵场景下的高可用。黑客侵入了其中一个PKI节点无法获利，同时PKI系统任然保持可用性&lt;/p&gt;
&lt;p&gt;【门限密码学：把密钥分成L份，当有其中f+1份时可以解密，否则解密不了】&lt;/p&gt;
&lt;p&gt;eg. Shamir&amp;rsquo;s Secret Sharing 基于拉格朗日插值法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705065190379.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065190379&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;eg2. ITTC 基于离散对数的子密钥分配&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705065815902.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065815902&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705065831688.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705065831688&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;server上用密码算，CA来整合&lt;/p&gt;
&lt;p&gt;也就是说黑客就算攻入了一个节点，他仍然无法获取PKI用来签名证书的私钥，同时其他节点还能继续工作。&lt;/p&gt;
&lt;h3 id=&#34;信任增强-解决方式原理&#34;&gt;信任增强 解决方式原理&lt;/h3&gt;
&lt;p&gt;信任机制基本假设：1、CA行为不会出错，证书中的信息不会出错【只有可能是错误操作导致的签发给错误的人】 2、无限制权利&lt;/p&gt;
&lt;p&gt;三个思路：&lt;/p&gt;
&lt;p&gt;1、 浏览器端实施检测：&lt;/p&gt;
&lt;p&gt;（1）浏览器维护证书信息&lt;/p&gt;
&lt;p&gt;（2）多个会话之间互相比较&lt;/p&gt;
&lt;p&gt;2、限制CA权利&lt;/p&gt;
&lt;p&gt;（1）假定server只会向同一个国家的CA申请证书&lt;/p&gt;
&lt;p&gt;（2）限定CA能签发的顶级域名范围&lt;/p&gt;
&lt;p&gt;（3）域名拥有者可以控制哪个CA给他签发证书&lt;/p&gt;
&lt;p&gt;（4）server再次确认机制：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705067041133.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705067041133&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;server在多一个sovereign Key的公私钥对挂在timeline上，浏览器看到timeline上有sovereign Key，会要求server再次拿sovereign Key私钥签名，黑客控制了CA，却无法获取server的sovereign Key私钥，因此仍然无法伪造身份&lt;/p&gt;
&lt;p&gt;3、证书透明化：&lt;/p&gt;
&lt;p&gt;假定CA也会出错，审计CA&lt;/p&gt;
&lt;h2 id=&#34;13-证书透明化&#34;&gt;13 证书透明化&lt;/h2&gt;
&lt;p&gt;虚假证书：证书可以被严格验证通过，但是证书对应的私钥并不被证书主体拥有，而是被其他人拥有（CA被人黑了，一顿乱发）&lt;/p&gt;
&lt;p&gt;透明化增加哪些步骤SCT相关特点弄清楚一点&lt;/p&gt;
&lt;p&gt;增加&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;公开日志服务器（Public Log Server）：保存和维护记录证书的公开日志（Public Log）&lt;/p&gt;
&lt;p&gt;收到证书并验证通过后，公开日志服务器会向提交者返回一个凭据（SCT）Signed Certificate Timestamp。（有可能多个公开日志服务器，就会返回多个SCT）用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705068113028.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705068113028&#34;
	
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;用户不仅需要验证证书，还需要验证相应的SCT（将SCT放入证书中，作为证书扩展）

怎么获得SCT呢？

1.从X.509证书扩展项获得SCT

2.从连接建立时TLS扩展项获得SCT -&amp;gt;TLS客户端要支持

3.从OCSP stapling的扩展项获得SCT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面不重要&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;监视员（Monitor）：周期性的访问公开日志服务器，寻找和发现可疑的证书&lt;/li&gt;
&lt;li&gt;审计员（Auditor）：审计公开日志的行为&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;14-隐式证书&#34;&gt;14 隐式证书&lt;/h2&gt;
&lt;p&gt;传统和隐式的结构和使用的区别&lt;/p&gt;
&lt;p&gt;在带宽、计算能力、存储资源有限制的环境下，隐式证书是传统X.509证书的一种高效替代&lt;/p&gt;
&lt;p&gt;X.509证书基本内容：订户身份信息+公钥数据+CA数字签名&lt;/p&gt;
&lt;p&gt;隐式证书基本内容：中间公钥数据$P_U$  + 订户身份标识。 最终公钥 P=$P_{CA}$+$P_U$以及身份信息也有关  $P_{CA}$：CA证书公钥&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705069930310.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705069930310&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705070190100.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705070190100&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705070203597.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705070203597&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;X.509： 需要对订户证书进行CA签名的验证&lt;/p&gt;
&lt;p&gt;隐式证书：需要重构出订户公钥，在对消息的验签时同时完成对证书本身的验证&lt;/p&gt;
&lt;p&gt;隐式证书中，没有对CA数字签名的验证，取而代之的是，重构公钥的计算，后者的计算量较小。&lt;/p&gt;
&lt;p&gt;假名证书不考&lt;/p&gt;
&lt;h2 id=&#34;15-kerberos&#34;&gt;15 kerberos&lt;/h2&gt;
&lt;p&gt;可信第三方TTP，基于对称密码，也支持在某些过程使用非对称&lt;/p&gt;
&lt;p&gt;获得一个TGT，用TGT和要访问的目，请求问kerberos服务器，来获取访问目标的票据（不是TGT，TGT只是告诉kerberos我已经被验证过了）&lt;/p&gt;
&lt;p&gt;kerberos票据流程&lt;/p&gt;
&lt;p&gt;长期密钥（主密钥）Long-term Key/Master Key： 长期保持不变的密钥。被长期密钥（主密钥）加密的数据尽量不在网络上传输。（防止暴力破解、分析）&lt;/p&gt;
&lt;p&gt;短期密钥（会话密钥）Short-term Key/Session Key： 加密需要进行网络传输的数据。只在一段时间内有效，即使被加密的数据包被黑客截获并破解成功后，这个Key早就已经过期了。&lt;/p&gt;
&lt;p&gt;KDC（Key Distribution Center）：kerberos server作为可信第三方，维护所有帐户（client、server）的注册信息、用户名、口令、用户主密钥、服务器主密钥&lt;/p&gt;
&lt;p&gt;Server 与Client之间基于共享秘密短期密钥key实现身份鉴别&lt;/p&gt;
&lt;p&gt;KDC仅仅是允许进入应用系统，至于有什么权限、由应用系统自主决定&lt;/p&gt;
&lt;p&gt;获取TGT：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705080938952.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705080938952&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;client发请求，KDC用client的master key加密一个会话密钥$S_{KDC-Client}$，用KDC的master key加密TGT，TGT里包含会话密钥和client信息（让client 鉴别KDC是KDC而非被伪造）&lt;/p&gt;
&lt;p&gt;获取ST：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705139628762.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705139628762&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;这个图有问题，KDC还给client的不是用clinet的master key，而是用session key。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当client要访问server的时候，给KDC自己的TGT和要访问的server。&lt;/p&gt;
&lt;p&gt;KDC根据TGT来对client进行认证，生成$S_{Server-Client}$和ST(session ticket)&lt;/p&gt;
&lt;p&gt;$S_{Server-Client}$：用client的主密钥加密一个会话密钥，&lt;/p&gt;
&lt;p&gt;ST：用server的主密钥加密，ST包含会话密钥和client的信息。&lt;/p&gt;
&lt;p&gt;将这两个被加密的Copy一并发送给Client&lt;/p&gt;
&lt;p&gt;client得到会话密钥后，用session key解密，创建Authenticator（Client Info + Timestamp）并用会话密钥加密&lt;/p&gt;
&lt;p&gt;client将ST和Authenticator访问server，server用自己的主密钥解密ST得到会话密钥，在用会话密钥解密Authenticator，比较Authenticator里的client info和ST里的client info来确定client就是client&lt;/p&gt;
&lt;p&gt;那如果TGT没过期，session key过期了呢？可以用TGT再申请一个，因为TGT用KDC的master key加密，KDC可以得到旧的session key和client info，进而再发一个session key。由于session key是TGT的一部分，这其实也就相当于重新申请了TGT&lt;/p&gt;
&lt;p&gt;client鉴别server（双向鉴别）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705080767000.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705080767000&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在Authenticator里在加一个flag要求server自证。&lt;/p&gt;
&lt;p&gt;server看到后，用ST里得到的会话密钥解密Authenticator，把里面的timestamp用会话密钥加密发给client&lt;/p&gt;
&lt;h2 id=&#34;16-oauthoidc&#34;&gt;16 OAuth&amp;amp;OIDC&lt;/h2&gt;
&lt;p&gt;单点登录(Single Sign on)在某个地方认证了之后，在整个域里都不用再认证了。&lt;/p&gt;
&lt;p&gt;SSO 口令记录器-&amp;gt;保存在edge/chrome&lt;/p&gt;
&lt;p&gt;OAuth 协议流程图，理解认证的流程，有那几个角色，分别做了什么&lt;/p&gt;
&lt;p&gt;OIDC 协议流程图，理解认证的流程&lt;/p&gt;
&lt;h2 id=&#34;17-fido&#34;&gt;17 FIDO&lt;/h2&gt;
&lt;p&gt;在服务器端将用户与移动终端的可信环境进行身份绑定
将用户与服务器之间的直接鉴别转变为两段式鉴别
1 移动终端鉴别用户主要是靠生物特征
2 服务器端鉴别移动终端主要是靠数字签名&lt;/p&gt;
</description>
        </item>
        <item>
        <title>自然语言处理笔记</title>
        <link>https://pillar23.github.io/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 27 Dec 2023 19:07:00 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;ucas 2023秋 自然语言处理基础 胡玥、曹亚男&lt;/p&gt;
&lt;p&gt;期末全是开放问题，因此弄清楚各种模型的优劣非常有必要。&lt;/p&gt;
&lt;h1 id=&#34;笔记&#34;&gt;笔记&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/aialgorithm/AiPy/blob/master/Ai%E7%9F%A5%E8%AF%86%E5%9B%BE%E5%86%8C/Ai_Roadmap/nlp.png?raw=true&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;nlp.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;model&lt;/h2&gt;
&lt;p&gt;常见的模型有DNN、CNN、RNN、GNN、LSTM、&lt;/p&gt;
&lt;h2 id=&#34;task&#34;&gt;task&lt;/h2&gt;
&lt;p&gt;NLP的经典问题有&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704274435391.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704274435391&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在课程中，我们主要学习了&lt;/p&gt;
&lt;h4 id=&#34;属性抽取ae&#34;&gt;属性抽取（AE）&lt;/h4&gt;
&lt;p&gt;opinion target和aspect的区别：opinion target是被评价对象，aspect是对象的属性&lt;/p&gt;
&lt;p&gt;eg&amp;quot;这个手机的摄像头很出色，但电池寿命较短。&amp;ldquo;手机是opinion target，而摄像头和电池寿命是手机的两个aspect。&lt;/p&gt;
&lt;p&gt;目标：抽取对象。eg：华为技术遥遥领先！-&amp;gt; 抽取出“华为”&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/51189078&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Aspect Term Extraction 论文阅读（一） - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704948448294.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704948448294&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;THA是用了attention机制的LSTM，用于获得上文（单向）已经标注过的aspect的信息，来指导当前aspect标注。&lt;/p&gt;
&lt;p&gt;STN是LSTM，用于获得opinion的摘要信息。首先，STN单元获得基于给定aspect的opinion的表示，接下来利用attention机制来获得基于全局的opinion的表示。自此就可以获得基于当前aspect的opinion摘要。将aspect的表示和opinion摘要拼接作为特征，用于标注。&lt;/p&gt;
&lt;p&gt;（表示就是一个框里三个圆圆）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704954212132.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704954212132&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;将ATE形式化为一个seq2seq的学习任务。在这个任务中，源序列和目标序列分别由单词和标签组成。为了使Seq2Seq学习更适合ATE,作者设计了门控单元网络和位置感知注意力机制。门控单元网络用于将相应的单词表示融入解码器，而位置感知注意力机制则用于更多地关注目标词的相邻词。&lt;/p&gt;
&lt;p&gt;decoder包含一个门控单元，用于控制编码器和解码器产生的隐状态。当解码标签时，这个门控单元可以自动的整合来自编码器和解码器隐状态的信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704955929276.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704955929276&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;masked seq2seq。首先，对输入句子的连续几个词进行掩码处理。然后，encoder接收部分掩码的句子及其标签序列作为输入，decoder尝试根据编码上下文和标签信息重建句子原文。要求保持opinion target位置不变&lt;/p&gt;
&lt;h4 id=&#34;观点抽取oe&#34;&gt;观点抽取（OE）&lt;/h4&gt;
&lt;p&gt;一般都是先抽取aspect，在对aspect进行情感预测的流水线方式&lt;/p&gt;
&lt;p&gt;IMN使用非流水线方式。与传统的多任务学习方法依赖于学习不同任务的共同特征不同，IMN引入了一种消息传递体系结构，通过一组共享的潜在变量将信息迭代地传递给不同的任务&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704957048177.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704957048177&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;它接受一系列tokens{x1，…，xn}作为特征提取组件fθs的输入，该组件在所有任务之间共享。该组件由单词嵌入层和几个特征提取层（好多个CNN）组成。输出所有任务共享的潜在向量{hs1，hs2，…，hsn}的序列。该潜在向量序列会根据来自不同任务组件传播来的信息来更新。&lt;/p&gt;
&lt;p&gt;$hi^{s(T)}$ 表示为t轮消息传递后Xi对应的共享潜在向量的值。&lt;/p&gt;
&lt;p&gt;共享潜在向量序列用作不同任务特定组件的输入。每个特定于任务的组件都有自己的潜在变量和输出变量集。输出变量对应于序列标签任务中的标签序列；在AE中，我们为每个令牌分配一个标签，表明它是否属于任何aspect或opinion，而在AS中，我们为每个单词加上它的情感标签。在分类任务中，输出对应于输入实例的标签：情感分类任务(DS)的文档的情感，以及领域分类任务(DD)的文档域。在每次迭代中，适当的信息被传递回共享的潜在向量以进行组合；这可以是输出变量的值，也可以是潜在变量的值，具体取决于任务。 此外，我们还允许在每次迭代中在组件之间传递消息（opinion transmission）。&lt;/p&gt;
&lt;p&gt;感觉有点训练词向量的感觉，像是预处理一下得到向量序列来方便其他任务。&lt;/p&gt;
&lt;p&gt;【超，好像这些都不是考试重点】&lt;/p&gt;
&lt;h2 id=&#34;属性级情感分类&#34;&gt;属性级情感分类&lt;/h2&gt;
&lt;h1 id=&#34;for-exam&#34;&gt;For exam&lt;/h1&gt;
&lt;p&gt;试卷题型：简答题 40 分（5*8）好多个问号（内容为胡老师讲的基础部分）+ 综合题 60 分（内容为曹老师讲的核心应用部分）&lt;/p&gt;
&lt;p&gt;简答题重点章节：&lt;/p&gt;
&lt;p&gt;什么是语言模型、神经网络语言模型、几种、特点（优点）&lt;/p&gt;
&lt;p&gt;概念性的简答题， 不难+&lt;/p&gt;
&lt;p&gt;第4章 语言模型+词向量 （要求掌握：语言模型概念，神经网络语言模型 ）&lt;/p&gt;
&lt;p&gt;第 5章 NLP中的注意力机制 （全部要求掌握）概念、用处&lt;/p&gt;
&lt;p&gt;第 7 章 预训练语言模型（全部要求掌握）[主要掌握GPT，BERT 是 怎么训练的，与下游任务是如何对接的]prompt，inconcert learning，思维链【建模的几种范式】&lt;/p&gt;
&lt;p&gt;主观题重点章节： 设计东西&lt;/p&gt;
&lt;p&gt;第9章 情感分析（要求掌握：方面级情感分析基本方法原理）&lt;/p&gt;
&lt;p&gt;第10章 信息抽取 （要求掌握：实体和关系联合抽取基本方法原理）&lt;/p&gt;
&lt;p&gt;第 11章 问答系统（要求掌握：检索式问答系统基本方法原理）&lt;/p&gt;
&lt;h2 id=&#34;语言模型概念&#34;&gt;语言模型概念&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704349075208.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349075208&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704349189011.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349189011&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704349255995.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704349255995&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型&lt;/h2&gt;
&lt;p&gt;统计的方法使用最大似然估计，需要数据平滑否则会出现0概率问题。&lt;/p&gt;
&lt;p&gt;神经网络使用DNN和RNN&lt;/p&gt;
&lt;p&gt;利用RNN 语言模型可以解决以上概率语言模型问题，在神经网络一般用RNN语言模型&lt;/p&gt;
&lt;h3 id=&#34;一些我不会的背景知识&#34;&gt;一些（我不会的）背景知识&lt;/h3&gt;
&lt;h4 id=&#34;梯度下降算法&#34;&gt;梯度下降算法&lt;/h4&gt;
&lt;p&gt;梯度下降法是一种常用的优化算法，主要用于找到函数的局部最小值。它的基本思想是：在每一步迭代过程中，选择函数在当前点的负梯度（即函数在该点下降最快的方向）作为搜索方向，然后按照一定的步长向该方向更新当前点，不断迭代，直到满足停止准则。&lt;/p&gt;
&lt;p&gt;具体来说，假设我们要最小化一个可微函数$f(x)$，我们首先随机选择一个初始点$x_0$，然后按照以下规则更新$x$：&lt;/p&gt;
&lt;p&gt;$$
x_{n+1} = x_n - \alpha \nabla f(x_n)
$$&lt;/p&gt;
&lt;p&gt;其中，$\nabla f(x_n)$是函数$f$在点$x_n$处的梯度，$\alpha$是步长（也称为学习率），控制着每一步更新的幅度。&lt;/p&gt;
&lt;p&gt;梯度下降法只能保证找到局部最小值&lt;/p&gt;
&lt;h4 id=&#34;双曲正切函数&#34;&gt;双曲正切函数&lt;/h4&gt;
&lt;p&gt;它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在&lt;/p&gt;
&lt;p&gt;$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$&lt;/p&gt;
&lt;h4 id=&#34;bp算法&#34;&gt;BP算法&lt;/h4&gt;
&lt;p&gt;反向传播算法，当正向传播得到的结果和预期不符，则反向传播，修改权重&lt;/p&gt;
&lt;h4 id=&#34;bptt算法&#34;&gt;BPTT算法&lt;/h4&gt;
&lt;p&gt;是BP算法的拓展，可以处理具有时间序列结构的数据，用于训练RNN&lt;/p&gt;
&lt;p&gt;BPTT的工作原理如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;正向传播&lt;/strong&gt; ：在每个时间步，网络会读取输入并计算输出。这个过程会持续进行，直到处理完所有的输入序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt; ：一旦完成所有的正向传播步骤，网络就会计算最后一个时间步的误差（即网络的预测与实际值之间的差距），然后将这个误差反向传播到前一个时间步。这个过程会持续进行，直到误差被传播回第一个时间步。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;参数更新&lt;/strong&gt; ：在误差反向传播的过程中，网络会计算误差关于每个参数的梯度。然后，这些梯度会被用来更新网络的参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;one-hot编码&#34;&gt;one-hot编码&lt;/h4&gt;
&lt;p&gt;独热编码是一种将离散的分类标签转换为二进制向量的方法&lt;/p&gt;
&lt;p&gt;假设我们要做一个分类任务，总共有3个类别，分别是猫、狗、人。那这三个类别就是一种离散的分类：它们之间互相独立，不存在谁比谁大、谁比谁先、谁比谁后的关系。&lt;/p&gt;
&lt;p&gt;在神经网络中，需要一种数学的表示方法，来表示猫、狗、人的分类。最容易想到的，便是以 0 代表猫，以 1 代表狗，以 2 代表人这种简单粗暴的方式。但这样会导致分类标签之间出现了不对等的情况。（2比1大……）&lt;/p&gt;
&lt;p&gt;而进行如下的编码的话就可以解决这个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;猫：[1, 0, 0]&lt;/li&gt;
&lt;li&gt;狗：[0, 1, 0]&lt;/li&gt;
&lt;li&gt;人：[0, 0, 1]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这就是独热码&lt;/p&gt;
&lt;h4 id=&#34;pairwise&#34;&gt;pairwise&lt;/h4&gt;
&lt;p&gt;&amp;ldquo;Pairwise&amp;quot;是一种常用于排序和推荐系统的方法。它的主要思想是将排序问题转换为二元分类问题。每次取一对样本，预估这一对样本的先后顺序，不断重复预估一对对样本，从而得到某条查询下完整的排序。如果文档A的相关性高于文档B，则赋值+1，反之则赋值-1。这样，我们就得到了二元分类器训练所需的训练样本&lt;/p&gt;
&lt;p&gt;Pairwise方法也有其缺点。例如，它只考虑了两篇文档的相对顺序，没有考虑他们出现在搜索结果列表中的位置。&lt;/p&gt;
&lt;p&gt;除了Pairwise，还有其他的方法如Pointwise和Listwise。Pointwise方法每次仅仅考虑一个样本，预估的是每一条和查询的相关性，基于此进行排序。而Listwise方法则同时考虑多个样本，找到最优顺序。这些方法各有优缺点，选择哪种方法取决于具体的应用场景和需求。&lt;/p&gt;
&lt;h4 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h4&gt;
&lt;p&gt;&amp;ldquo;Zero-shot learning&amp;rdquo;（零样本学习）是一种机器学习范式，它允许模型在没有先前训练过相关数据集的情况下，对不包含在训练数据中的类别或任务进行准确的预测或推断。这种能力是由先进的深度学习模型和迁移学习方法得以实现的。&lt;/p&gt;
&lt;p&gt;举个例子，假设我们的模型已经能够识别马，老虎和熊猫了，现在需要该模型也识别斑马，那么我们需要告诉模型，怎样的对象才是斑马，但是并不能直接让模型看见斑马。所以模型需要知道的信息是马的样本、老虎的样本、熊猫的样本和样本的标签，以及关于前三种动物和斑马的描述。&lt;/p&gt;
&lt;p&gt;这种方法的优点是可以极大地节省标注量。不需要增加样本，只需要增加描述即可。&lt;/p&gt;
&lt;h4 id=&#34;ppo&#34;&gt;PPO&lt;/h4&gt;
&lt;p&gt;PPO（Proximal Policy Optimization，近端策略优化）是一种强化学习算法，由OpenAI在2017年提出。PPO算法的目标是解决深度强化学习中策略优化的问题。&lt;/p&gt;
&lt;p&gt;PPO算法的核心在于更新策略梯度，主流方法有两种，分别是KL散度做penalty，另一种是Clip剪裁，它们的主要作用都是限制策略梯度更新的幅度，从而推导出不同的神经网络参数更新方式。&lt;/p&gt;
&lt;p&gt;PPO算法具备Policy Gradient、TRPO的部分优点，采样数据和使用随机梯度上升方法优化代替目标函数之间交替进行，虽然标准的策略梯度方法对每个数据样本执行一次梯度更新，但PPO提出新目标函数，可以实现小批量更新。&lt;/p&gt;
&lt;h3 id=&#34;dnnnnlm&#34;&gt;DNN（NNLM）&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704359720351.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704359720351&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-grambigram&#34;&gt;2-gram（bigram）&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704364780556.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704364780556&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704364894689.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704364894689&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其中$\theta$就是训练过程中要学习的参数，有了这些参数就可以直接的到 $p(w_i|w_{i-1})$， 找到一组足够好的参数，就能让得到的$p(w_i|w_{i-1})$最接近训练语料库&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704365950174.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704365950174&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这里的最大化就是损失函数最小（最接近0），因为P永远小于1，所以log永远是负数，他们加起来永远小于0，让log最大，也就是让log最接近0&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704366274757.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366274757&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;n-gram&#34;&gt;n-gram&lt;/h4&gt;
&lt;p&gt;拓展一下罢了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704366342703.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366342703&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704366354123.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366354123&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704366370789.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704366370789&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;rnnrnnlm&#34;&gt;RNN（RNNLM）&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704367534089.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367534089&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704367561457.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367561457&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704367582922.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704367582922&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704368432414.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704368432414&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;词向量&#34;&gt;词向量&lt;/h2&gt;
&lt;p&gt;自然语言问题要用计算机处理时，第一步要找一种方法把这些符号数字化，成为计算机方便处理的形式化表示&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NNLM模型词向量&lt;/li&gt;
&lt;li&gt;RNNLM模型词向量&lt;/li&gt;
&lt;li&gt;C&amp;amp;W 模型词向量&lt;/li&gt;
&lt;li&gt;CBOW 模型词向量&lt;/li&gt;
&lt;li&gt;Skip-gram模型词向量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不同模型的词向量之间的主要区别在于它们捕获和编码词义和上下文信息的方式。以下是一些常见模型的词向量特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;神经网络语言模型（NNLM）&lt;/strong&gt; ：NNLM通过学习预测下一个词的任务来生成词向量。这种方法可以捕获词义和词之间的关系，但是它通常无法捕获长距离的依赖关系，因为它只考虑了固定大小的上下文。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;循环神经网络语言模型（RNNLM）&lt;/strong&gt; ：RNNLM使用循环神经网络结构，可以处理变长的输入序列，并能捕获长距离的依赖关系。因此，RNNLM生成的词向量可以包含更丰富的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt; ：Word2Vec是一种预训练词向量的方法，它包括两种模型：Skip-gram和CBOW。Skip-gram模型通过一个词预测其上下文，而CBOW模型则通过上下文预测一个词。Word2Vec生成的词向量可以捕获词义和词之间的各种关系，如同义词、反义词、类比关系等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GloVe&lt;/strong&gt; ：GloVe（Global Vectors for Word Representation）是另一种预训练词向量的方法，它通过对词-词共现矩阵进行分解来生成词向量。GloVe生成的词向量可以捕获词义和词之间的线性关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; ：BERT（Bidirectional Encoder Representations from Transformers）使用Transformer模型结构，并通过预训练任务（如Masked Language Model和Next Sentence Prediction）来生成词向量。BERT生成的词向量是上下文相关的，也就是说，同一个词在不同的上下文中可能有不同的词向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，不同模型的词向量之间的区别主要在于它们捕获和编码词义和上下文信息的方式。选择哪种词向量取决于具体的任务需求和计算资源。&lt;/p&gt;
&lt;h3 id=&#34;nnlm的词向量&#34;&gt;NNLM的词向量&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704369375532.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369375532&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;解决办法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704369411606.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369411606&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;通过一个|D| * |V|的矩阵，额可以将one-shot的编码转为D维的稠密的词向量，所以管他叫lookup表&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704369517877.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369517877&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704369568799.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369568799&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;NNLM 语言模型在训练语言模型同时也训练了词向量&lt;/p&gt;
&lt;h3 id=&#34;rnnlm的词向量&#34;&gt;RNNLM的词向量&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704369951464.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704369951464&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704370827646.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704370827646&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;cw&#34;&gt;C&amp;amp;W&lt;/h3&gt;
&lt;p&gt;C&amp;amp;W模型是靠两边猜中间的一种模型，输入层是wi上下文的词向量&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704371807694.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704371807694&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;score是wi中间这个word在这个位置有多合理，越高越合理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704372099728.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372099728&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;正样本通常是指在实际语料库中出现过的词语及其上下文。负样本则是人为构造的，通常是将一个词与一个随机的上下文配对。&lt;/p&gt;
&lt;p&gt;Pairwise方法在训练C&amp;amp;W词向量时，主要是通过比较一对词的上下文来进行训练的。具体来说，对于每一对词（一个正样本和一个负样本），我们都会计算它们的词向量，并通过比较这两个词向量的相似度来更新我们的模型。&lt;/p&gt;
&lt;p&gt;在训练过程中，我们首先需要选择一个损失函数，这里是修改后的HingeLoss&lt;/p&gt;
&lt;p&gt;然后，我们会使用一种优化算法来最小化这个损失函数，这里是梯度下降，在每一次迭代中，我们都会根据当前的损失来更新我们的词向量。&lt;/p&gt;
&lt;p&gt;训练的目标是在正样本中的score高，负样本的score低，然后score差的越大效果越好&lt;/p&gt;
&lt;h3 id=&#34;cbow&#34;&gt;CBOW&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704372201199.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372201199&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;CBOW也是靠两边猜中间，输入层是wi上下文词向量的平均值，目标是最小化（最收敛与0）上下文词的平均与目标词之间的距离。输出是&lt;/p&gt;
&lt;h3 id=&#34;skip-gram&#34;&gt;skip-gram&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704372268757.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704372268757&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;skip-gram是知道中间猜两边，训练最小化（最收敛于0）目标词与上下文词之间的距离。&lt;/p&gt;
&lt;h2 id=&#34;注意力机制&#34;&gt;注意力机制&lt;/h2&gt;
&lt;h3 id=&#34;概述&#34;&gt;概述&lt;/h3&gt;
&lt;p&gt;在注意力机制中，Q、K、V 分别代表查询（Query）、键（Key）和值（Value）。&lt;/p&gt;
&lt;p&gt;注意力机制的工作过程可以简单概括为：对于每一个查询，计算它与所有键的匹配程度（通常使用点积），然后对这些匹配程度进行归一化（通常使用 softmax 函数），得到每个键对应的&lt;strong&gt;权重&lt;/strong&gt;。最后，用这些权重对所有的值进行加权求和，得到最终的输出。&lt;/p&gt;
&lt;p&gt;这种机制允许模型在处理一个元素时，考虑到其他相关元素的信息，从而捕捉输入元素之间的依赖关系。在自然语言处理、计算机视觉等领域，注意力机制已经被广泛应用，并取得了显著的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704978816700.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978816700&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704978826803.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978826803&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704978837242.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704978837242&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;W是权重，都是学来的。&lt;/p&gt;
&lt;p&gt;参考&lt;/p&gt;
&lt;p&gt;KQV矩阵： &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;amp;vd_source=2fbfeabcc6cdd857dcd6247eb0154d83&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;vd_source=2fbfeabcc6cdd857dcd6247eb0154d83&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Attention机制： &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1YA411G7Ep&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1YA411G7Ep&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704455161185.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704455161185&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704455261000.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704455261000&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;K、V都是经过线性变换的词向量集合（矩阵）&lt;/p&gt;
&lt;p&gt;Q是隐藏状态（隐藏向量）&lt;/p&gt;
&lt;p&gt;A是一个注意力值，就是我们设置的这个字的注意力值&lt;/p&gt;
&lt;p&gt;通过attention的学习，可以得到a1、a2……，这些就是K中各个向量对Q的权重&lt;/p&gt;
&lt;p&gt;步骤1：计算 f ( Q ,Ki )&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704467445387.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467445387&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;步骤2：计算对于Q 各个 Ki 的权重&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704467647363.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467647363&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;步骤3：计算输出 Att-V值（各 Ki 乘以自己的权重，然后求和 ）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704467679320.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704467679320&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;举例1-seq2seqrnn2rnn的机器翻译中&#34;&gt;举例1， seq2seq（RNN2RNN）的机器翻译中&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704543763032.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704543763032&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;seq2seq做机器翻译的过程，需要大量的两种语言的平行语料，就是意思相同的语言的一一对应的关系。&lt;/p&gt;
&lt;p&gt;其中x为词向量，A为权重矩阵，h为隐藏状态（隐藏向量）。&lt;/p&gt;
&lt;p&gt;RNN是用预训练的词向量，然后通过学习权重矩阵A来微调，得到隐藏状态，可以理解为隐藏状态是带了上下文的更加符合RNN的词的向量表示。&lt;/p&gt;
&lt;p&gt;每一个时间步中，A都被微调， 因此x1、x2、x3的A可能都是不一样的。在大量预料的训练下会获得表现比较好的A和A&#39;&lt;/p&gt;
&lt;p&gt;h可以表示为$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$其中$h_t$ 是当前时间步的隐藏状态，$h_{t-1}$ 是前一时间步的隐藏状态，$x_t$ 是当前时间步的输入，$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b_h$ 是偏置项，$f$ 是激活函数&lt;/p&gt;
&lt;p&gt;不加注意力机智的seq2seq模型，encoder是RNN，decoder也是RNN，在encoder接受了$x_1$到$x_m$的词向量序列后，得到最终的隐藏状态$h_m$， 也就是$s_0$，作为decoder的初始状态。&lt;/p&gt;
&lt;p&gt;如果不加注意力机制，decoder那边也就是靠隐藏状态、x和参数（A矩阵，偏置值b）来继续进行RNN的步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704550430429.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704550430429&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;现在我们引入注意力机制，也就是图上的$c_0$，权重的计算按照上面所说的KQV计算方法，这里K是词向量集合x1,x2&amp;hellip; Q是隐藏状态。也就是对于每一个隐藏状态，都可以求一个关于词向量序列的权重值$\alpha$。&lt;/p&gt;
&lt;p&gt;通过求出这一系列的$\alpha$，就可以加权求出上下文矩阵$c$，c知道当前隐藏状态和词向量矩阵的全部关系。&lt;/p&gt;
&lt;p&gt;加了注意力机制之后，decoder的各个隐藏状态求解过程就会向之前提到的那样变得更复杂&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704551152595.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704551152595&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;而每一个步骤的c都不一样，比如&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704550965949.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704550965949&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;c0是s0对于x1,x2&amp;hellip;的att-V，也就是hm对于x1,x2&amp;hellip;的att-V；c1是隐藏状态s1对于x1,x2&amp;hellip;的att-V，c2是隐藏状态s2对于x1,x2&amp;hellip;的att-V这些c都需要花算力来算&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704551282267.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704551282267&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;注意力机制的问题是时间复杂度太大了。如果是简单的RNN2RNN，如果encoder词向量矩阵大小为m，decoder词向量矩阵大小为n，所需的时间复杂度为O(m+n)，而使用注意力机制之后就会变成O(mn)，还是打分函数比较简单的情况下。&lt;/p&gt;
&lt;h3 id=&#34;注意力编码机制&#34;&gt;注意力编码机制&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704470189498.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470189498&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704470407421.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470407421&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704470394991.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704470394991&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;attention机制还可以将不同序列融合编码（将多个序列经过某种处理或嵌入方式，转换为一个固定长度的向量或表示形式。）&lt;/p&gt;
&lt;p&gt;就是给每个词向量乘个权重加起来，被称作注意力池化（Attention Pooling）或加权求和（Weighted Sum）。这个操作的含义是将注意力权重分配给输入序列中的不同部分，从而形成一个汇聚了注意力的向量表示。&lt;/p&gt;
&lt;p&gt;这个操作的效果是聚焦于输入序列中具有更高注意力权重的部分，形成一个综合的表示，其中对于重要的部分有更大的贡献。这对于处理序列数据中的上下文信息，关注重要元素，以及实现对不同部分不同程度的关注都非常有用，特别是在自然语言处理中的任务中。&lt;/p&gt;
&lt;h2 id=&#34;预训练语言模型&#34;&gt;预训练语言模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704705278999.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705278999&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;迁移学习&#34;&gt;迁移学习&lt;/h4&gt;
&lt;p&gt;迁移学习（Transfer Learning）是一种机器学习方法，其核心思想是利用已有的知识来辅助学习新的知识。例如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#。&lt;/p&gt;
&lt;p&gt;迁移学习通常会关注有一个源域（源任务） $D_ {s}$ 和一个目标域（目标任务） $D_ {t}$ 的情况.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704705772371.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705772371&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;迁移方式分为两种&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704705819455.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704705819455&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;几个范式&#34;&gt;几个范式&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704782526810.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704782526810&#34;
	
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;第三范式预训练-精调范式&#34;&gt;第三范式：预训练-精调范式&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704782261195.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704782261195&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704785636077.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704785636077&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;自回归：预测序列的下一个或者上一个&lt;/p&gt;
&lt;p&gt;自编码：预测序列中的某一个或某几个&lt;/p&gt;
&lt;p&gt;广义自回归：和自回归主要区别在于他们处理输入数据的方式。自回归预训练语言模型在生成序列时，会一个接一个地生成新的词，每个新词都依赖于前面的词。如GPT，而广义自回归预训练语言模型则更为灵活，它们可以在生成序列时考虑更多的上下文信息，模型不仅可以查看前面的词，还可以查看后面的词或者整个序列。如XLNet&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704787153210.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704787153210&#34;
	
	
&gt;&lt;/p&gt;
&lt;h6 id=&#34;gpt训练和对接&#34;&gt;GPT训练和对接&lt;/h6&gt;
&lt;p&gt;GPT 采用了 Transformer 的 Decoder 部分，并且每个子层只有一个 Masked Multi Self-Attention（768 维向量和 12 个 Attention Head）和一个FeedForward （无普通transformer解码器层的编码器-解码器注意力子层），模型共叠加使用了 12 层的 Decoder。使用了从左向右的单向注意力机制&lt;/p&gt;
&lt;p&gt;Masked Multi Self-Attention的768维向量和12个attention head： 意思是12个独立的attention组件，每个组件的参数都独立，然后每个attention的Q向量都是768维，也可以理解为一个词在模型中的向量（或者说词嵌入）是768维]&lt;/p&gt;
&lt;p&gt;feedforward： 作用是提取更深层次的特征。在每个序列的位置单独应用一个全连接前馈网络，由两个线性层和一个激活函数组成。线性层将每个位置的表示扩展，为学习更复杂的特征提供可能性，激活函数帮助模型学习更复杂的非线性特征，第二个线性层将每个位置的表示压缩回原始维度。这样，位置特征敏感的部分就会被表达出来，提供给后续网络学习。&lt;/p&gt;
&lt;p&gt;就是十二个下图这样的小东西&lt;/p&gt;
&lt;p&gt;transformer输入有token embedding和position embedding&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704860957530.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704860957530&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对比一下transformer，transformer的decoder是6个右边的，少了一层multi-head attention的encoder-decoder注意力子层（cross-attention的那个子模块）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704861830326.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704861830326&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;6层attention堆叠就是六个encoder就是个小的encoder，每个encoder里都有attention机制，上图N=6的意思。&lt;/p&gt;
&lt;p&gt;训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704865198754.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865198754&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;maximize负数=近0最小化&lt;/p&gt;
&lt;p&gt;与下游任务对接：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704865491544.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865491544&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;把多序列通过一些特定的规则拼成一个单序列。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704865594321.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704865594321&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;微调：&lt;/p&gt;
&lt;p&gt;任务微调有2种方式 ：① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务&lt;/p&gt;
&lt;p&gt;举例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704866344121.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866344121&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704866727437.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866727437&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这儿的$L_1(C)$是上面提到的预训练过程中的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704866802266.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704866802266&#34;
	
	
&gt;&lt;/p&gt;
&lt;h6 id=&#34;bert训练和对接&#34;&gt;BERT训练和对接&lt;/h6&gt;
&lt;p&gt;用了transformer的encoder再加FFN（前馈神经网络，FFN 层有助于学习序列中的非线性关系和模式）层&lt;/p&gt;
&lt;p&gt;【但是transformer的encoder不是带FeedForward吗？】FFN仅在MLM过程中有用，而BERT的最终输出是模型在整个预训练过程中学到的表示的某种组合。这些表示在后续的任务中可以进一步微调或者用作特征。（BYD，原来只是训练过程中的一个b东西）&lt;/p&gt;
&lt;p&gt;下图中一个trm是一个子层，&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704867651617.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704867651617&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704867919696.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704867919696&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在BERT模型中，输入的每个单词都会通过三种嵌入（embedding）进行编码&lt;/p&gt;
&lt;p&gt;Token Embedding：是将每个单词或者词片映射到一个向量，这个向量能够捕获该单词的语义信息。在BERT中，使用了WordPiece标记化，其中输入句子的每个单词都被分解成子词标记。这些标记的嵌入是随机初始化的，然后通过梯度下降进行训练。&lt;/p&gt;
&lt;p&gt;Segment Embedding：是用来区分不同的句子的。在处理两个句子的任务（如自然语言推理）时，BERT需要知道每个单词属于哪个句子。&lt;/p&gt;
&lt;p&gt;Position Embedding：由于Transformer模型并没有像循环神经网络那样的顺序性，因此需要显式地向模型添加位置信息，以保留句子中单词的顺序信息&lt;/p&gt;
&lt;p&gt;训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704871798829.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704871798829&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;MLM：把一个序列的几个word给mask了让模型猜的训练方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704880632493.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880632493&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;(2).句子顺序模型训练&lt;/p&gt;
&lt;p&gt;凑一些下一句不是下一句的负样本来训练预训练模型对句子顺序的敏感。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704871959170.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704871959170&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对接：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704872111654.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704872111654&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;微调同样有两种① 只调任务参数 ② 任务参数和预训练模型参数一起调，这样可以让预训练模型更加适配任务&lt;/p&gt;
&lt;h6 id=&#34;其他&#34;&gt;其他&lt;/h6&gt;
&lt;p&gt;RoBERTa：把BERT使用Adam默认的参数改为使用更大的batches，训练时把静态mask改为动态mask。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704873163103.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704873163103&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;BART：GPT只用了transformer的decoder，BERT只用了transformer的encoder。导致&lt;/p&gt;
&lt;p&gt;BERT具备双向语言理解能力的却不具备做生成任务的能力。GPT拥有自回归特性的却不能更好的从双向理解语言.&lt;/p&gt;
&lt;p&gt;（模型的&amp;quot;自回归&amp;quot;特性指的是，当前的观察值是过去观察值的加权平均和一个随机项）&lt;/p&gt;
&lt;p&gt;BART使用标准的Transformer结构为基础，吸纳BERT和GPT的优点，使用&lt;strong&gt;多种噪声破坏原文本&lt;/strong&gt;，再将残缺文本通过序列到序列的任务重新复原（降噪自监督）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704880759669.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880759669&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;BERT在预测时加了额外的FFN, 而BART没使用FFN.&lt;/p&gt;
&lt;p&gt;（还记得这个Beyond吗）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704880695071.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704880695071&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;T5&lt;/p&gt;
&lt;p&gt;给整个 NLP 预训练模型领域提供了一个通用框架，把所有NLP任务都转化成一种形式(Text-to-Text)，通过这样的方式可以用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务。以后的各种NLP任务，只需针对一个超大预训练模型，考虑怎么把任转换成合适的文本输入输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704881664053.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881664053&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704881694268.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881694268&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704881712622.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881712622&#34;
	
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;第四范式预训练提示预测范式pre-trainpromptpredict&#34;&gt;第四范式：预训练，提示，预测范式（Pre-train,Prompt,Predict）&lt;/h5&gt;
&lt;p&gt;prompt挖掘工程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704881803249.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704881803249&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;特点：不通过目标工程使预训练的语言模型（LM）适应下游任务，而是将下游任务建模的方式重新定义（Reformulate），通过利用合适prompt实现不对预训练语言模型改动太多，尽量在原始 LM上解决任务的问题。&lt;/p&gt;
&lt;p&gt;实现方法eg：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882346953.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882346953&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882361576.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882361576&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882378196.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882378196&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882674432.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882674432&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882723263.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882723263&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;要素：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882803241.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882803241&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;输入端&lt;/p&gt;
&lt;p&gt;prompt工程&lt;/p&gt;
&lt;p&gt;完形填空和前缀提示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882845802.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882845802&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704882862203.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704882862203&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;模板创建&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704883318434.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883318434&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;输出端&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704883343584.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883343584&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704883381023.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883381023&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;微调&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704883499101.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704883499101&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;生成类任务用法与第五范式相同&lt;/p&gt;
&lt;h5 id=&#34;第五范式大模型&#34;&gt;第五范式：大模型&lt;/h5&gt;
&lt;p&gt;大语言模型 (Large Language Model，LLM) 通常指由大量参数（通常数十亿个权重或更多）组成的人工神经网络预训练语言模型，使用大量的计算资源在海量数据上进行训练。&lt;/p&gt;
&lt;p&gt;大型语言模型是通用的模型，在广泛的任务（例如情感分析、命名实体识别或数学推理）中表现出色，具有与人类认证对齐的特点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704890488901.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704890488901&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704890734795.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704890734795&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704891371789.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891371789&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704891495297.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891495297&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704891515801.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891515801&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704891548168.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704891548168&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;不需要任务模型的意思是只要有预训练就行&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892005089.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892005089&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892027838.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892027838&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;（我靠，这要传统注意力算死了）&lt;/p&gt;
&lt;p&gt;学习方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892066074.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892066074&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;因为上下文学习，在使用的时候也可以用zero-shot, one-shot和few-shot。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892306373.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892306373&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892321288.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892321288&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;chain-of-thought&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892359407.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892359407&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892375228.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892375228&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892402637.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892402637&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892464014.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892464014&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892496826.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892496826&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;与人类对齐：RLHF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704892569477.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704892569477&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;简而言之：1、在人工标注数据上SFT（有监督微调）模型&lt;/p&gt;
&lt;p&gt;2、多模型给标注人员做排序，用来训练奖励模型（RM）&lt;/p&gt;
&lt;p&gt;3、使用强化学习PPO算法，交互地优化模型参数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894207867.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894207867&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;文本分类在各个范式上的例子&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894268859.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894268859&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894283289.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894283289&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894294431.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894294431&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894305898.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894305898&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894318376.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894318376&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894335472.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894335472&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704894354621.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704894354621&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;方面级情感分类&#34;&gt;方面级情感分类&lt;/h2&gt;
&lt;p&gt;方面级情感分类（Aspect-Level Sentiment Classification）是自然语言处理（NLP）中的一个任务，它的目标是识别文本中特定方面的情感倾向。例如，在产品评论中，“这款手机的电池寿命很长，但屏幕质量差。”这句话中，“电池寿命”这个方面的情感是积极的，而“屏幕质量”这个方面的情感是消极的。所以，方面级情感分类不仅要识别出文本中的各个方面，还要判断这些方面的情感倾向。这个任务在许多领域都有应用，比如产品评论分析、社交媒体监控等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704946258919.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946258919&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704946290972.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946290972&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;问题定义&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704946620060.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946620060&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704946709801.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946709801&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704946746917.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946746917&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;基本方法原理&#34;&gt;基本方法、原理&lt;/h3&gt;
&lt;p&gt;子任务等：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704946917658.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704946917658&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity/Target&lt;/strong&gt;：评论的对象或者物品是什么，例如某个餐厅，某款手机。&amp;ldquo;Target&amp;quot;这个词用的比较模糊，其既可以被当作Entity，又可以当作Aspect Term。和在AE里提到的opinion target是一个意思。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Aspect&lt;/strong&gt;：隶属于某个Entity的属性。在这里其因为学者提出的任务类型不同，又分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Aspect Term&lt;/strong&gt;：存在在句子中的Aspect。例如例句中的”拍照“、”电池“、”外观“。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aspect Category&lt;/strong&gt;：预先给定的Aspect。例如，我们想知道评论对”华为手机“的”外观“、”售后服务“、”便携性“三个aspect的情感极性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;lstm&#34;&gt;LSTM&lt;/h4&gt;
&lt;p&gt;LSTM 方法先将所有变长的句子均表示为一种固定长度的向量，具体做法是将最后一个word对应的计算得到的 hidden vector 作为整句话的表示（sentence vector）。之后，将最后得到的这个 sentence vector 送入一个 linear layer，使其输出为一个维度为情绪种类个数。最后对 linear layer 得出的结果做 softmax 并依次为依据选出该句（同时也是 target）的情绪分类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704968549919.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704968549919&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;td-lstm&#34;&gt;TD-LSTM&lt;/h4&gt;
&lt;p&gt;将输入的句子根据 aspect 分为两部分，两边都朝着 aspect 的方向分别同时把 words 送入两个 LSTM 中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704968574703.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704968574703&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;tc-lstm&#34;&gt;TC-LSTM&lt;/h4&gt;
&lt;p&gt;与 TD-LSTM 唯一的不同就是在 input 时在每个 word embedding vector 后面拼接上 aspect vector（如果 aspect 中有多个 word，则取平均）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704963182156.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;img&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;at-lstm&#34;&gt;AT-LSTM&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704970432154.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704970432154&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对隐藏状态h和aspect的词嵌入后施加attention&lt;/p&gt;
&lt;h4 id=&#34;atae-lstm&#34;&gt;ATAE-LSTM&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704970484908.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704970484908&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在LSTM的输入方面在concat一个aspect的词向量，说明aspect的重要性&lt;/p&gt;
&lt;h4 id=&#34;ian&#34;&gt;IAN&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704971332861.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704971332861&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;IAN 模型由两部分组成，两部分分别对 Target 和 Context 进行建模。每一部分都以词嵌入作为输入，再通过 LSTM 获取每个词的隐藏状态，最后取所有隐藏向量的平均值，用它来监督另一部分注意力向量的生成。attention学习隐藏状态和对应词向量序列的相关性。&lt;/p&gt;
&lt;p&gt;attention部分是$h_t^i$&amp;amp;$avg(h_c)$在target上做注意力，$h_c^i$&amp;amp;$avg(h_t)$在context上做注意力&lt;/p&gt;
&lt;h2 id=&#34;实体和关系联合抽取&#34;&gt;实体和关系联合抽取&lt;/h2&gt;
&lt;p&gt;信息抽取：从自然语言文本中抽取指定类型的实体、 关系、 事件等事实信息，并形成结构化数据输出的文本处理技术。一般情况下信息抽取别是知识抽取等其他任务的基础。主要在对无结构数据的抽取出现问题&lt;/p&gt;
&lt;h3 id=&#34;基本方法原理-1&#34;&gt;基本方法原理&lt;/h3&gt;
&lt;h4 id=&#34;名词解释&#34;&gt;名词解释&lt;/h4&gt;
&lt;p&gt;span：指的是文本中的一段连续的子串，这段子串对应于某个&lt;em&gt;&lt;strong&gt;实体&lt;/strong&gt;&lt;/em&gt;或者&lt;em&gt;&lt;strong&gt;关系&lt;/strong&gt;&lt;/em&gt;的具体文本表述。&lt;/p&gt;
&lt;h4 id=&#34;dygie&#34;&gt;DyGIE&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704981462026.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704981462026&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;问题定义：&lt;/p&gt;
&lt;p&gt;输入：所有句中可能的spans序列集合。&lt;/p&gt;
&lt;p&gt;输出三种信息：实体类型，关系分类（同一句），指代链接（跨句）；&lt;/p&gt;
&lt;p&gt;Token Representation Layer（Token表示层）：BiLSTM&lt;/p&gt;
&lt;p&gt;Span Representation Layer（span表示层）： 初始化来自BiLSTM输出联合起来，加入基于注意力模型。&lt;/p&gt;
&lt;p&gt;Coreference Propagation Layer（指代传播层）：N次传播处理，跨span共享上下文信息&lt;/p&gt;
&lt;p&gt;Relation Propagation Layer（关系传播层）：与指代传播层相似&lt;/p&gt;
&lt;p&gt;Final Prediction Layer（最终预测层）：去预测任务—实体任务，关系任务&lt;/p&gt;
&lt;h4 id=&#34;oneie&#34;&gt;OneIE&lt;/h4&gt;
&lt;p&gt;任务定义：给定一个输入的句子，输出一个图，图中节点(含节点类型)代表实体提及或者触发词，图中的边表示表示节点之间的关系&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704982012782.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982012782&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704982419293.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982419293&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;条件随机场（Conditional Random Field，CRF）是一种在自然语言处理（NLP）中广泛使用的模型。CRF的主要作用是解决序列数据的标注问题，它能够考虑整个序列的上下文信息，以做出更准确的预测。&lt;/p&gt;
&lt;p&gt;Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。Beam Search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。&lt;/p&gt;
&lt;p&gt;在这里只保留最好的&lt;/p&gt;
&lt;h4 id=&#34;uie&#34;&gt;UIE&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704982684822.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982684822&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704982702205.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982702205&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;uniex&#34;&gt;UniEX&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704982730929.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982730929&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1704982749870.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1704982749870&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;检索式问答系统&#34;&gt;检索式问答系统&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705210507003.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210507003&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;1、问题分析模块：问题分类和关键词提取&lt;/p&gt;
&lt;p&gt;问题分类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705210654986.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210654986&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;关键词提取：根据问题分类，用&lt;strong&gt;序列标注法&lt;/strong&gt;抽取相应类别的&lt;strong&gt;实体&lt;/strong&gt;做为检索关键词&lt;/p&gt;
&lt;p&gt;2、检索模块：检索问题答案所在文档与段落&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705210788393.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210788393&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;3、 答案抽取模块：在相关片段中抽取备选答案，并对备选答案进行排序&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705210839822.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210839822&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;实现方法：&lt;/p&gt;
&lt;h3 id=&#34;流水线方式&#34;&gt;流水线方式&lt;/h3&gt;
&lt;p&gt;Document Retriever + Reading Comprehension Reader框架&lt;/p&gt;
&lt;h4 id=&#34;drqa&#34;&gt;DrQA&lt;/h4&gt;
&lt;p&gt;TF-IDF：（Term Frequency-Inverse Document Frequency，词频-逆文件频率）是一种用于信息检索和数据挖掘的常用加权技术。它是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705210960856.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210960856&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用TF-IDF获取与问题topK相关的文档&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705210980785.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705210980785&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;然后将对topK使用抽取式阅读理解，从原文中抽取出可以回答的文本&lt;/p&gt;
&lt;h4 id=&#34;evidence-aggregation-for-answer-re-ranking-in-open-domain-question-answering&#34;&gt;Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering&lt;/h4&gt;
&lt;p&gt;有些问题需要来自不同来源的证据相结合才能正确回答。解决方法：strength-based re-ranker&amp;amp;coverage-based re-ranker&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235444754.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235444754&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;strength-based re-ranker的基本思想是，正确的答案通常会被更多的段落反复提及&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235457636.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235457636&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;coverage-based re-ranker考虑每个答案在覆盖不同证据方面的能力，这里用一个BiLSTM来计算答案支撑片段的相似表征【指一个答案和它的支撑片段在表征空间中的相似度】，在垮文本上的相似表征很很高说明这个答案更可靠&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235473330.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235473330&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235487967.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235487967&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;端到端方式&#34;&gt;端到端方式&lt;/h3&gt;
&lt;h4 id=&#34;retriever-reader的联合学习&#34;&gt;Retriever-Reader的联合学习&lt;/h4&gt;
&lt;h5 id=&#34;orqa-open-retriever-question-answering&#34;&gt;ORQA: Open-Retriever Question Answering&lt;/h5&gt;
&lt;p&gt;问题引入：&lt;/p&gt;
&lt;p&gt;1）需要具有强监督的支持证据：监督数据难以获得&lt;/p&gt;
&lt;p&gt;2）利用IR（信息检索）系统检索候选证据：QA与IR存在一定差异性，IR更关注词法或语义相似性，QA对于语言理解层次更丰富&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235654530.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235654530&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;就是一个S是打分函数。评价retrieval和评价reader是两个不同的，$s_{retr}$是评价这个block和问题的相关性的，$S_{read}$是评价块儿里的文本和q的相关性的。这个里面的bert是用来理解retrieval和question的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235732678.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235732678&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每个块通过BERT和权重矩阵b生成隐藏向量h，问题通过BERT和权重矩阵q生成隐藏向量h，通过点积判断相关性&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705235784735.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705235784735&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;BERT_R+MLP生成s，给S_read来评分&lt;/p&gt;
&lt;p&gt;有监督训练，需要手标与a有关的s&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705239164034.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705239164034&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;有挑战，但是懒得管了&lt;/p&gt;
&lt;h4 id=&#34;基于预训练的retriever-free方法&#34;&gt;基于预训练的Retriever-Free方法&lt;/h4&gt;
&lt;p&gt;对预训练模型进行微调，使其能够在没有任何外部上下文或知识的情况下回答问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705240830964.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705240830964&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用span corruption来预训练&lt;/p&gt;
&lt;p&gt;Span Corruption是T5模型预训练任务中的一种方法。它将完整的句子根据随机的span进行掩码。例如，原句：“Thank you for inviting me to your party last week”，Span Corruption之后可能得到输入：“Thank you [X] me to your party [Y] week”，目标：“[X] for inviting [Y] last [Z]”。其中[X]等一系列辅助编码称为sentinels。&lt;/p&gt;
&lt;p&gt;这种方法的目标是让模型学习如何从被打乱或被掩码的句子中恢复出原始的句子。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1705234885580.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1705234885580&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;LLM在问答任务上与有监督微调效果不相上下&lt;/p&gt;
&lt;p&gt;LLM在计数、多跳推理、日期、因果等类型上的性能较弱&lt;/p&gt;
&lt;h1 id=&#34;最后一节课&#34;&gt;最后一节课&lt;/h1&gt;
&lt;p&gt;讲了一节课的对话系统（不考）&lt;/p&gt;
&lt;p&gt;参考：https://blog.csdn.net/ld326/article/details/112802292&lt;/p&gt;
</description>
        </item>
        <item>
        <title>网络认证技术作业三</title>
        <link>https://pillar23.github.io/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/</link>
        <pubDate>Wed, 29 Nov 2023 19:56:28 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/%E7%BD%91%E7%BB%9C%E8%AE%A4%E8%AF%81%E6%8A%80%E6%9C%AF%E4%BD%9C%E4%B8%9A%E4%B8%89/</guid>
        <description>&lt;p&gt;为了避免跨平台的问题，直接用choco在windows上安装openssl 3.1.1&lt;/p&gt;
&lt;p&gt;&lt;code&gt;choco install openssl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;首先生成私钥&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;openssl genrsa -aes256 -out private.pem &lt;span class=&#34;m&#34;&gt;4096&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;genrsa&lt;/code&gt;是openssl的一个命令，用于生成RSA私钥。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-aes256&lt;/code&gt;表示在输出私钥之前，使用AES 256加密&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-out private.pem&lt;/code&gt; 表示将生成的私钥输出到名为private.pem的文件中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;4096&lt;/code&gt;表示生成的私钥的位数，即私钥的长度为4096位&lt;/p&gt;
&lt;p&gt;然后使用私钥生成证书&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;openssl req -new -x509 -days &lt;span class=&#34;m&#34;&gt;365&lt;/span&gt; -key .&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;rivate.pem -out cacert.crt -config .&lt;span class=&#34;se&#34;&gt;\s&lt;/span&gt;mime.cnf -extensions smime
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;req&lt;/code&gt;是openssl的一个命令，用于创建和处理PKCS#10格式的证书请求。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-new&lt;/code&gt;表示创建一个新的证书请求。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-x509&lt;/code&gt;表示生成一个自签名的证书，而不是生成一个证书请求。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-days 365&lt;/code&gt;表示生成的证书的有效期为365天。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-key .\private.pem&lt;/code&gt;表示使用名为private.pem的文件中的私钥来签署证书。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-out cacert.crt&lt;/code&gt;表示将生成的证书输出到名为cacert.crt的文件中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-config .\smime.cnf&lt;/code&gt;表示使用名为smime.cnf的文件作为配置文件。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-extensions smime&lt;/code&gt;表示应该包含配置文件中名为smime的部分中指定的扩展。&lt;/p&gt;
&lt;p&gt;smime的部分为&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;smime&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;basicConstraints&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; CA:FALSE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;keyUsage&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; nonRepudiation, digitalSignature, keyEncipherment
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;extendedKeyUsage&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; emailProtection
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;subjectKeyIdentifier&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;hash&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;authorityKeyIdentifier&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; keyid:always, issuer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;subjectAltName&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; email:copy
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;basicConstraints = CA:FALSE&lt;/code&gt;指定证书不能用作CA（证书颁发机构）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;keyUsage = nonRepudiation, digitalSignature, keyEncipherment&lt;/code&gt;指定证书的公钥可以用于哪些用途。这个证书可以用于非否认（nonRepudiation）、数字签名（digitalSignature）和密钥封装（keyEncipherment）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;extendedKeyUsage = emailProtection&lt;/code&gt;用于电子邮件保护（emailProtection）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;subjectKeyIdentifier = hash&lt;/code&gt;用公钥的hash值唯一地标识证书中的公钥。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;authorityKeyIdentifier = keyid:always, issuer&lt;/code&gt;用于标识签署此证书的CA的公钥。这个扩展通常包含CA公钥的keyid（一个唯一标识符），以及CA的名称（issuer）。&lt;code&gt;keyid:always&lt;/code&gt;表示总是包含keyid，无论是否需要&lt;/p&gt;
&lt;p&gt;&lt;code&gt;subjectAltName = email:copy&lt;/code&gt;用于指定证书的主题可选名称（Subject Alternative Name）。主题可选名称是电子邮件地址，该地址从证书的主题名称字段中复制&lt;/p&gt;
&lt;p&gt;生成的时候国家地区公司啥的都不重要，我直接敲回车按默认了。邮箱写自己的就行了。&lt;/p&gt;
&lt;p&gt;直接用windwos自带的证书查看器查看这个证书。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701257323787.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701257323787&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;版本v3是指我们使用了x.509第3版本，然后序列号是有证书生成算法生成的，唯一的指定这个证书，像身份证号似的。签名算法和哈希算法是一个声明，颁发者是我们刚才在生成证书时写的。有效期由我们刚才的 &lt;code&gt;-days&lt;/code&gt; 参数指明，使用者和颁发者一样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701257667795.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701257667795&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;公钥直接在证书文件里保存。公钥参数0500表示NULL，这是因为RSA的公钥的参数（模数和公开指数）已经在公钥字段中给出，所以不需要在公钥参数字段中再给出，如果是其他的加密算法，可能会包含其他信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701257930426.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701257930426&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;基本约束&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subject Type=End Entity&lt;/strong&gt; ：这表示该证书是一个终端实体证书，而不是CA（证书颁发机构）证书。也就是说这个证书不能用于签发/创建其他证书。&lt;strong&gt;Path Length Constraint=None&lt;/strong&gt; ：这表示路径长度没有设置，准许其签发多级的数字证书。然而，由于Subject Type=End Entity，这个证书不能用于签发其他证书，所以这个设置在这种情况下没有意义。这是由于我们使用了 &lt;code&gt;basicConstraints = CA:FALSE&lt;/code&gt;的选项。&lt;/p&gt;
&lt;p&gt;下面的其他拓展都在-extension部分说过了，这里就不多赘述了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701253924073.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701253924073&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;然后安装这个证书，并且选择保存路径为受信任的根证书颁发机构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701254826717.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254826717&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;我使用的客户端是outlook。使用的邮箱服务是qq邮箱。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701253859046.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701253859046&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在outlook里添加我的证书&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701254522179.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254522179&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701254551598.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254551598&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701254581460.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254581460&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701254667232.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701254667232&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这里outlook只支持导入pfx，所以我们需要把生成的证书格式转换&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;openssl pkcs12 -export -out cacert.pfx -inkey .\private.pem -in .\cacert.crt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;接下来是导入助教的证书，首先在outlook里新建一个联系人&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701255095693.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701255095693&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;导入，这里又只支持.cer了，我的windows下的openssl好像缺了库没法转，所以用wsl里的openssl转了一下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;openssl pkcs12 -in limengjie22\@mails.ucas.ac.cn.pfx -nokeys -out output.cer
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;openssl pkcs12 -nokeys&lt;/code&gt;命令用于从PKCS#12文件（通常具有.pfx或.p12扩展名）中提取证书，-nokeys指定不包含私钥，这样生成的output.cer不能做任何需要私钥的操作（我们也没有要用私钥的操作）&lt;/p&gt;
&lt;p&gt;import password即使提供的私钥.txt的内容。然后就可以成功导入了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701255140765.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701255140765&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;然后在发送邮件的时候，在选项里把加密和签署都点了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1701255501352.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1701255501352&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>IKEv2标准阅读</title>
        <link>https://pillar23.github.io/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/</link>
        <pubDate>Thu, 19 Oct 2023 16:31:55 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/ikev2%E6%A0%87%E5%87%86%E9%98%85%E8%AF%BB/</guid>
        <description>&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;ucas 2023秋 网络认证技术&lt;/p&gt;
&lt;p&gt;作业2：任选一个标准（口令鉴别协议），书写阅读报告。报告内容要求描述基本原理，解决了什么问题，可能存在什么问题。&lt;/p&gt;
&lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;
&lt;p&gt;我选择阅读&lt;a class=&#34;link&#34; href=&#34;https://www.rfc-editor.org/rfc/rfc7296.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RFC 7296&lt;/a&gt;，该标准是互联网密钥交换 （IKE） 协议的第二个版本。本标准使RFC 5996废弃， IKEv2是当前的互联网标准。&lt;/p&gt;
&lt;h1 id=&#34;解决了什么问题&#34;&gt;解决了什么问题&lt;/h1&gt;
&lt;p&gt;IKEv2（Internet Key Exchange version 2）是一种用于建立虚拟专用网络（VPN）连接的协议，它解决了许多与安全通信和远程访问有关的问题。包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安全性：IKEv2提供了强大的安全性，通过使用加密算法来保护数据的机密性和完整性。它还允许身份验证，以确保通信双方是合法的，并可以抵御各种网络攻击，如中间人攻击和数据篡改。&lt;/li&gt;
&lt;li&gt;移动性：IKEv2支持移动设备的连接，允许用户从一个网络切换到另一个网络时保持连接的连续性。这对于移动工作人员或在不同网络环境中工作的人员非常有用。&lt;/li&gt;
&lt;li&gt;多平台兼容性：IKEv2是一种通用的VPN协议，支持多种操作系统和设备，包括Windows、macOS、iOS、Android和Linux。这使得它成为广泛使用的VPN协议，能够在不同平台之间建立安全的连接。&lt;/li&gt;
&lt;li&gt;快速重新连接：IKEv2具有快速重新连接的能力，可以在断开连接后快速重新建立连接，而不需要用户手动干预。这对于移动设备或不稳定的网络连接非常有用。&lt;/li&gt;
&lt;li&gt;支持IPv6：随着IPv6的推广，IKEv2也提供了对IPv6的良好支持，使其适用于新一代互联网协议。&lt;/li&gt;
&lt;li&gt;NAT穿透：IKEv2能够穿越网络地址转换（NAT）设备，这使得它在各种网络环境中都能够正常工作，包括家庭网络和企业网络。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在基本原理-1.1节也简述了IKEv2在特定场景下解决了什么问题。&lt;/p&gt;
&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;
&lt;h2 id=&#34;11-使用场景&#34;&gt;1.1 使用场景&lt;/h2&gt;
&lt;p&gt;IP 安全性 （IPsec） 为 IP 数据报提供机密性、数据完整性、访问控制和数据源身份验证，这些服务是通过维护 IP 数据报的源和接收方之间的共享状态来提供的。以手动方式建立此共享状态不能很好地扩展。IKEv2正是这样一个动态建立此状态的协议。IKE 在双方之间执行相互身份验证，并建立 IKE 安全关联 （SA），该关联包含共享机密信息，可用于高效建立用于封装安全有效负载 （ESP） [ESP] 或身份验证标头 （AH） [AH] 的 SA，以及一组加密算法，供 SA 用于保护其承载的流量。IKE 用于在许多不同的场景中协商 ESP 或 AH SA，每种方案都有自己的特殊要求。&lt;/p&gt;
&lt;h3 id=&#34;111-隧道模式下的安全网关到安全网关&#34;&gt;1.1.1 隧道模式下的安全网关到安全网关&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697641620088.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1697641620088&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在此方案中，IP 连接的两个endpoint都不实现 IPsec，但它们之间的网络节点会保护部分方式的流量。保护对endpoint是透明的，并且依赖于普通路由通过隧道终结点发送数据包进行处理。每个endpoint将宣布其后subnet的地址集，数据包将以隧道模式发送，其中内部 IP 标头将包含实际端点的 IP 地址。&lt;/p&gt;
&lt;h3 id=&#34;112-端点到端点传输模式&#34;&gt;1.1.2 端点到端点传输模式&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697642019874.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1697642019874&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在此方案中，IP 连接的两个终结点都实现 IPsec，这是 [IPSECARCH] 中主机的要求。该模式通常使用没有内部 IP 标头。将协商一对地址，以便此 SA 保护数据包。这些endpoint可以根据参与者的 IPsec 身份验证身份实现应用层访问控制。此方案实现了端到端安全性。虽然此场景可能不完全适用于 IPv4 公网，但已在使用 IKEv1 的内网内的特定场景中成功部署。在向 IPv6 过渡期间和采用 IKEv2 期间，应该更广泛地启用它。&lt;/p&gt;
&lt;p&gt;在这种情况下，一个或两个受保护的端点可能位于网络地址转换 （NAT） 节点后面，在这种情况下，必须对隧道数据包进行 UDP 封装，以便 UDP 标头中的端口号可用于标识 NAT “后面”的各个endpoint。&lt;/p&gt;
&lt;h3 id=&#34;113隧道模式下的端点到安全网关&#34;&gt;1.1.3隧道模式下的端点到安全网关&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697642991491.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1697642991491&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在此方案中，受保护的endpoint（通常是便携式计算机）通过受 IPsec保护的隧道连接回其企业网络。它可能仅使用此隧道访问公司网络上的信息，或者可能通过公司网络将其所有流量通过隧道传输回，以便利用公司防火墙提供的针对基于 Internet 的攻击的保护。在任一情况下，受保护端点都需要一个与安全网关关联的 IP 地址，以便返回到该网关的数据包将转到安全网关并用隧道传回。此 IP 地址可以是静态的，也可以由安全网关动态分配。为了支持后一种情况，IKEv2 包括一种机制（即配置有效负载），发起方请求安全网关拥有的 IP 地址，以便在其 SA 期间使用。&lt;/p&gt;
&lt;p&gt;在这种情况下，数据包将使用隧道模式。在来自受保护endpoint的每个数据包上，外部 IP 标头将包含与其当前位置关联的源 IP 地址（即，将流量直接路由到端点的地址），而内部 IP 标头将包含安全网关分配的源 IP 地址（即，将流量路由到安全网关以转发到端点的地址）。外部目标地址将始终是安全网关的地址，而内部目标地址将是数据包的最终目标。&lt;/p&gt;
&lt;p&gt;在这种情况下，受保护的终结点可能位于 NAT 后面。在这种情况下，安全网关看到的 IP 地址将与受保护端点发送的 IP 地址不同，并且必须对数据包进行 UDP 封装才能正确路由。&lt;/p&gt;
&lt;h2 id=&#34;12初始交换&#34;&gt;1.2初始交换&lt;/h2&gt;
&lt;p&gt;使用 IKE 的通信始终从IKE_SA_INIT和IKE_AUTH交换开始（在 IKEv1 中称为阶段 1）。这些初始交换通常由四条消息组成，但在某些情况下，该数字可能会增长。使用 IKE 的所有通信都由请求/响应对组成。我们将首先描述基础交换，然后是变体。第一对消息 （IKE_SA_INIT） 协商加密算法、交换随机数并进行 Diffie-Hellman 交换 [DH]。&lt;/p&gt;
&lt;p&gt;第二对消息 （IKE_AUTH） 对以前的消息进行身份验证，交换身份和证书，并建立第一个子 SA。这些消息的某些部分使用通过IKE_SA_INIT交换建立的密钥进行加密和完整性保护，因此身份对窃听者隐藏，并且所有消息中的所有字段都经过身份验证。有关如何生成加密密钥的信息，请参阅第 2.14 节。（无法完成IKE_AUTH交换的中间人攻击者仍可以看到发起者的身份。&lt;/p&gt;
&lt;p&gt;初始交换后的所有消息都使用在IKE_SA_INIT交换中协商的加密算法和密钥进行加密保护。这些后续消息使用第 3.14 节中描述的加密有效负载的语法，使用第 2.14 节中所述派生的密钥进行加密。所有后续消息都包含加密有效负载，即使它们在文本中称为“空”。对于CREATE_CHILD_SA、IKE_AUTH或信息交换，标头后面的消息是加密的，包含标头的消息是使用为 IKE SA 协商的加密算法进行完整性保护的。&lt;/p&gt;
&lt;p&gt;每个 IKE 消息都包含一个消息 ID 作为其固定标头的一部分。此消息 ID 用于匹配请求和响应，并标识消息的重新传输。&lt;/p&gt;
&lt;p&gt;一些简称如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Notation        Payload 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;----------------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;AUTH            Authentication 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CERT            Certificate 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CERTREQ         Certificate Request 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CP              Configuration 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;D               Delete 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;EAP             Extensible Authentication 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;HDR             IKE header (not a payload) 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;IDi             Identification - Initiator 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;IDr             Identification - Responder 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;KE              Key Exchange 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Ni, Nr          Nonce 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;N               Notify 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;SA              Security Association 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;SK              Encrypted and Authenticated
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;TSi             Traffic Selector - Initiator 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;TSr             Traffic Selector - Responder 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;V               Vendor ID
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;第 3 节介绍了每个有效负载的内容的详细信息。可能选择显示的有效负载将显示在括号中，例如 [CERTREQ];这表示可以选择包含证书请求有效负载。&lt;/p&gt;
&lt;p&gt;初始交流如下：&lt;/p&gt;
&lt;p&gt;发起方→接收方HDR, SAi1, KEi, Ni。&lt;/p&gt;
&lt;p&gt;HDR 包含安全参数索引 （SPI）、版本号、Exchange 类型、消息 ID 和各种标志。SAi1 有效负载声明发起方为 IKE SA 支持的加密算法。KE 有效负载发送发起方的 Diffie-Hellman 值。Ni是发起者的随机数。&lt;/p&gt;
&lt;p&gt;接收方→发起方HDR, SAr1, KEr, Nr, [CERTREQ]。&lt;/p&gt;
&lt;p&gt;响应方从发起方提供的选择中选择加密套件，并在 SAr1 有效负载中表达该选择，完成与 KEr 有效负载的 Diffie-Hellman 交换，并在 Nr 有效负载中发送其随机数。&lt;/p&gt;
&lt;p&gt;在协商的这一点上，每一方都可以生成一个名为 SKEYSEED 的数量（参见第 2.14 节），该 IKE SA 的所有密钥都从中派生出来。以下消息完全加密和完整性保护，邮件头除外。用于加密和完整性保护的密钥派生自 SKEYSEED，称为SK_e（加密）和SK_a（身份验证，又名完整性保护）;有关密钥派生的详细信息，请参见第 2.13 和 2.14 节。为每个方向计算单独的SK_e和SK_a。除了从 Diffie-Hellman 值派生的用于保护 IKE SA 的密钥SK_e和SK_a之外，还派生了另一个数量SK_d，并用于派生子 SA 的进一步密钥材料。符号 SK { &amp;hellip; } 表示这些有效负载已使用该方向的SK_e和SK_a进行加密和完整性保护。&lt;/p&gt;
&lt;p&gt;发起方→接收方HDR, SK {IDi, [CERT,] [CERTREQ,] [IDr,] AUTH, SAi2, TSi, TSr} 。发起方使用 IDi 有效负载断言其身份，证明与 IDi 对应的密钥的知识，完整性使用 AUTH 有效负载保护第一条消息的内容（请参阅第 2.15 节）。它还可能在 CERT 有效负载中发送其证书，并在 CERTREQ 有效负载中发送其信任锚的列表。如果包含任何 CERT 有效负载，则提供的第一个证书必须包含用于验证 AUTH 字段的公钥。可选的有效负载 IDr 使发起方能够指定要与响应方的哪个身份通信。当运行响应程序的计算机在同一 IP 地址上托管多个标识时，这很有用。如果发起方建议的 IDr 不被响应方接受，则响应方可能会使用其他某个 IDr 来完成交换。如果发起方随后不接受响应方使用的 IDr 与所请求的 IDr 不同的事实，则发起方可以在注意到这一事实后关闭 SA。发起方使用 SAi2 有效负载开始协商子 SA。最终字段（以 SAi2 开头）在CREATE_CHILD_SA交换的描述中描述。&lt;/p&gt;
&lt;p&gt;接收方→发起方HDR, SK {IDr, [CERT,] AUTH, SAr2, TSi, TSr}。响应方使用 IDr 有效负载断言其身份，可以选择发送一个或多个证书（再次使用包含用于验证 AUTH 的公钥的证书首先列出），使用 AUTH 有效负载验证其身份并保护第二条消息的完整性，并使用下面在CREATE_CHILD_SA交换中描述的其他字段完成子 SA 的协商。IKE_AUTH交换双方必须验证所有签名和消息身份验证代码 （MAC） 是否正确计算。如果任何一方使用共享密钥进行身份验证，则 ID 有效负载中的名称必须与用于生成 AUTH 有效负载的密钥相对应。由于发起方在IKE_SA_INIT中发送其 Diffie-Hellman 值，因此它必须猜测响应方将从其支持的组列表中选择的 Diffie-Hellman 组。如果发起方猜错了，响应方将使用类型 INVALID_KE_PAYLOAD 的通知有效负载进行响应，指示所选组。在这种情况下，发起方必须使用更正的 Diffie-Hellman 组重试IKE_SA_INIT。发起方必须再次提出其完整的可接受加密套件集，因为拒绝消息未经身份验证，否则主动攻击者可以诱使端点协商弱的套件。&lt;/p&gt;
&lt;p&gt;如果在IKE_AUTH交换期间创建子 SA 由于某种原因而失败，IKE SA 仍会照常创建。IKE_AUTH交换中不阻止设置 IKE SA 的通知消息类型列表至少包括以下内容：NO_PROPOSAL_CHOSEN、TS_UNACCEPTABLE、SINGLE_PAIR_REQUIRED、INTERNAL_ADDRESS_FAILURE和FAILED_CP_REQUIRED。&lt;/p&gt;
&lt;p&gt;如果失败与创建 IKE SA 有关（例如，返回AUTHENTICATION_FAILED通知错误消息），则不会创建 IKE SA。请注意，尽管IKE_AUTH消息已加密且完整性受到保护，但如果收到此通知错误消息的对等方尚未对另一端进行身份验证（或者如果对等方由于某种原因未能对另一端进行身份验证），则需要谨慎对待这些信息。更准确地说，假设MAC正确验证，则已知错误通知消息的发送方是IKE_SA_INIT交换的响应者，但无法保证发送方的身份。&lt;/p&gt;
&lt;p&gt;请注意，IKE_AUTH消息不包含 KEi/KEr 或 Ni/Nr 有效负载。因此，IKE_AUTH交换中的 SA 有效负载不能包含具有除 NONE 以外的任何值的转换类型 4（Diffie-Hellman 组）。实现应该省略整个转换子结构，而不是发送值 NONE。&lt;/p&gt;
&lt;h2 id=&#34;13-create_child_sa交换&#34;&gt;1.3 CREATE_CHILD_SA交换&lt;/h2&gt;
&lt;p&gt;CREATE_CHILD_SA交换用于创建新的子 SA，并重新生成 IKE SA 和子 SA 的密钥。此交换由单个请求/响应对组成，其某些功能在 IKEv1 中称为第 2 阶段交换。在初始交换完成后，它可以由IKE SA的任何一端发起。&lt;/p&gt;
&lt;p&gt;通过创建新 SA，然后删除旧 SA 来重新生成 SA 的密钥。本节介绍重新生成密钥的第一部分，即创建新 SA;第 2.8 节介绍了重新生成密钥的机制，包括将流量从旧 SA 移动到新 SA 以及删除旧 SA。必须一起阅读这两个部分才能理解重新生成密钥的整个过程。&lt;/p&gt;
&lt;p&gt;任一端点都可能发起CREATE_CHILD_SA交换，因此在本节中，术语发起方是指发起此交换的端点。实现可以拒绝 IKE SA 中的所有CREATE_CHILD_SA请求。&lt;/p&gt;
&lt;p&gt;CREATE_CHILD_SA请求可以选择包含用于额外 Diffie-Hellman 交换的 KE 有效负载，以便为子 SA 提供更强有力的前向保密保证。子 SA 的键控材料是 IKE SA 建立期间建立的SK_d、CREATE_CHILD_SA交换期间交换的随机数和 Diffie-Hellman 值（如果 KE 有效载荷包含在CREATE_CHILD_SA交换中）的函数。&lt;/p&gt;
&lt;p&gt;如果CREATE_CHILD_SA交换包含 KEi 有效载荷，则至少有一个 SA 报价必须包括 KEi 的 Diffie-Hellman 组。KEi的Diffie-Hellman组必须是发起者期望响应者接受的组的一个元素（可以提出其他Diffie-Hellman组）。如果响应方使用不同的 Diffie-Hellman 组（NONE 除外）选择提案，则响应方必须拒绝该请求，并在INVALID_KE_PAYLOAD Notify 有效负载中指示其首选的 Diffie-Hellman 组。有两个八位字节的数据与此通知相关联：接受的 Diffie-Hellman 组号，按大端序排列。在此类拒绝的情况下，CREATE_CHILD_SA交换失败，发起方可能会在响应者在INVALID_KE_PAYLOAD通知有效负载中给出的组中使用 Diffie-Hellman 提案和 KEi 重试交换。&lt;/p&gt;
&lt;p&gt;响应方发送NO_ADDITIONAL_SAS通知，以指示CREATE_CHILD_SA请求不可接受，因为响应方不愿意在此 IKE SA 上接受更多的子 SA。此通知还可用于拒绝 IKE SA 重新生成密钥。一些最小实现可能只接受初始 IKE 交换上下文中的单个子 SA 设置，并拒绝任何后续添加更多设置的尝试。&lt;/p&gt;
&lt;h3 id=&#34;131-通过create_child_sa交换创建新的子-sa&#34;&gt;1.3.1 通过CREATE_CHILD_SA交换创建新的子 SA&lt;/h3&gt;
&lt;p&gt;可以通过发送CREATE_CHILD_SA请求来创建子 SA。创建新子 SA 的CREATE_CHILD_SA请求是：&lt;/p&gt;
&lt;p&gt;发起方→接收方 HDR, SK {SA, Ni, [KEi,] TSi, TSr}。&lt;/p&gt;
&lt;p&gt;发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {SA, Nr, [KEr,]TSi, TSr}&lt;/p&gt;
&lt;p&gt;如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。&lt;/p&gt;
&lt;p&gt;要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。&lt;/p&gt;
&lt;p&gt;USE_TRANSPORT_MODE通知可以包含在请求消息中，该消息还包括请求子 SA 的 SA 有效负载。它要求子 SA 对创建的 SA 使用传输模式而不是隧道模式。如果请求被接受，则响应还必须包含类型 USE_TRANSPORT_MODE 的通知。如果响应方拒绝请求，子 SA 将在隧道模式下建立。如果发起方无法接受，则发起方必须删除 SA。注意：除非使用此选项协商传输模式，否则所有子 SA 都将使用隧道模式。&lt;/p&gt;
&lt;p&gt;ESP_TFC_PADDING_NOT_SUPPORTED通知断言发送终结点将不接受在正在协商的子 SA 上填充包含流量流机密性 （TFC） 填充的数据包。如果两个终结点都不接受 TFC 填充，则此通知将包含在请求和响应中。如果此通知仅包含在其中一条消息中，则仍可以在另一个方向发送 TFC 填充。&lt;/p&gt;
&lt;p&gt;NON_FIRST_FRAGMENTS_ALSO通知用于碎片控制。有关更全面的解释，请参见 [IPSECARCH]。双方需要同意在任何一方发送非第一个片段之前发送。仅当建议 SA 的请求和接受 SA 的响应中都包含通知NON_FIRST_FRAGMENTS_ALSO才会启用它。如果响应程序不想发送或接收非第一个片段，则它只会从响应中省略NON_FIRST_FRAGMENTS_ALSO通知，但不会拒绝整个子 SA 创建。&lt;/p&gt;
&lt;p&gt;第 2.22 节中涵盖的IPCOMP_SUPPORTED通知也可以包含在交易所中。&lt;/p&gt;
&lt;p&gt;创建子 SA 的失败尝试不应拆除 IKE SA：没有理由丢失为 IKE SA 所做的工作。有关创建子 SA 失败时可能出现的错误消息列表，请参阅第 2.21 节。&lt;/p&gt;
&lt;h3 id=&#34;132-使用create_child_sa交换机重新生成-ike-sa-的密钥&#34;&gt;1.3.2 使用CREATE_CHILD_SA交换机重新生成 IKE SA 的密钥&lt;/h3&gt;
&lt;p&gt;重新生成 IKE SA 密钥的CREATE_CHILD_SA请求是：&lt;/p&gt;
&lt;p&gt;发起方→接收方HDR, SK {SA, Ni, KEi}&lt;/p&gt;
&lt;p&gt;发起方在 SA 有效负载中发送 SA 产品/服务，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值。必须包括 KEi 有效负载。新的发起方 SPI 在 SA 有效负载的 SPI 字段中提供。一旦对等方收到重新生成 IKE SA 密钥的请求或发送重新生成 IKE SA 的请求，它就不应在正在重新生成密钥的 IKE SA 上发起任何新的CREATE_CHILD_SA交换。&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {SA, Nr, KEr}&lt;/p&gt;
&lt;p&gt;如果所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受产品/服务、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。新的响应程序 SPI 在 SA 有效负载的 SPI 字段中提供。&lt;/p&gt;
&lt;p&gt;新的 IKE SA 将其消息计数器设置为 0，无论它们在早期的 IKE SA 中是什么。来自新 IKE SA 上双方的第一个 IKE 请求的消息 ID 为 0。旧的 IKE SA 保留其编号，因此任何进一步的请求（例如，删除 IKE SA）都将具有连续编号。新的 IKE SA 的窗口大小也重置为 1，并且此重新密钥交换中的发起方是新 IKE SA 的新“原始发起方”。&lt;/p&gt;
&lt;h3 id=&#34;133-使用-create_child_sa-交换重新生成子-sa-的密钥&#34;&gt;1.3.3. 使用 CREATE_CHILD_SA 交换重新生成子 SA 的密钥&lt;/h3&gt;
&lt;p&gt;重新生成子 SA 密钥CREATE_CHILD_SA请求是：&lt;/p&gt;
&lt;p&gt;发起方→接收方 HDR, SK {N(REKEY_SA), SA, Ni, [KEi,] TSi, TSr}&lt;/p&gt;
&lt;p&gt;发起方在 SA 有效负载中发送 SA 选件，在 Ni 有效负载中发送随机数，在 KEi 有效负载中发送 Diffie-Hellman 值（可选），并在 TSi 和 TSr 有效负载中发送建议的子 SA 的建议流量选择器。第 1.3.1 节中描述的通知也可以在重新生成密钥交换中发送。通常，这些通知与原始交换中使用的通知相同;例如，重新生成传输模式 SA 的密钥时，将使用USE_TRANSPORT_MODE通知。如果交换的目的是替换现有的 ESP 或 AH SA，则必须将REKEY_SA通知包含在CREATE_CHILD_SA交换中。正在重新生成密钥的 SA 由通知有效负载中的 SPI 字段标识;这是交换发起方在入站 ESP 或 AH 数据包中期望的 SPI。没有与此通知消息类型关联的数据。REKEY_SA通知的协议 ID 字段设置为与我们要重新生成密钥的 SA 的协议匹配，例如，3 表示 ESP，2 表示 AH。&lt;/p&gt;
&lt;p&gt;重新生成子 SA 密钥CREATE_CHILD_SA响应为：&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {SA, Nr, [KEr,] TSi, TSr}&lt;/p&gt;
&lt;p&gt;如果请求中包含 KEi 并且所选加密套件包含该组，则响应程序使用 SA 有效负载中的已接受报价、Nr 有效负载中的随机数和 KEr 有效负载中的 Diffie-Hellman 值进行回复（使用相同的消息 ID 进行响应）。&lt;/p&gt;
&lt;p&gt;要在该 SA 上发送的流量的流量选择器在响应的 TS 有效负载中指定，这可能是子 SA 的发起方建议的子集。&lt;/p&gt;
&lt;h2 id=&#34;14-信息交换&#34;&gt;1.4. 信息交换&lt;/h2&gt;
&lt;p&gt;在 IKE SA 运行过程中的不同点，对等方可能希望相互传达有关某些事件的错误或通知的控制消息。为了实现这一点，IKE 定义了一个信息交换。信息交换必须仅在初始交换之后进行，并使用协商密钥进行加密保护。请注意，某些信息性消息（而非交换）可以在 IKE SA 的上下文之外发送。第 2.21 节还详细介绍了错误消息。&lt;/p&gt;
&lt;p&gt;与 IKE SA 相关的控制消息必须在该 IKE SA 下发送。与子 SA 相关的控制消息必须在生成它们的 IKE SA（如果 IKE SA 已重新生成密钥，则为其后续消息）的保护下发送。&lt;/p&gt;
&lt;p&gt;信息交换中的消息包含零个或多个通知、删除和配置有效负载。信息交换请求的接收者必须发送一些响应;否则，发送方将假定消息在网络中丢失并重新传输。该响应可能是一条空消息。信息交换中的请求消息也可能不包含有效负载。这是终结点可以要求另一个终结点验证其是否处于活动状态的预期方式。&lt;/p&gt;
&lt;p&gt;信息交换定义为：&lt;/p&gt;
&lt;p&gt;发起方→接收方 HDR, SK {[N,] [D,] [CP,] &amp;hellip;}&lt;/p&gt;
&lt;p&gt;接收方→发起方 HDR, SK {[N,] [D,] [CP,] &amp;hellip;}&lt;/p&gt;
&lt;p&gt;信息交换的处理由其组件有效载荷决定。&lt;/p&gt;
&lt;h3 id=&#34;141-删除具有信息交换的-sa&#34;&gt;1.4.1. 删除具有信息交换的 SA&lt;/h3&gt;
&lt;p&gt;ESP 和 AH SA 始终成对存在，每个方向上有一个 SA。关闭 SA 时，必须关闭（即删除）对的两个成员。每个终结点必须关闭其传入的 SA，并允许另一个终结点关闭每对中的另一个 SA。要删除 SA，将发送具有一个或多个 Delete 有效负载的信息交换，列出要删除的 SA 的 SPI（正如入站数据包标头中预期的那样）。收件人必须关闭指定的 SA。请注意，从不在单个消息中发送 SA 两端的删除有效负载。如果要同时删除多个 SA，则在信息交换中包括每个 SA 对的入站部分的删除有效负载。&lt;/p&gt;
&lt;p&gt;通常，信息交换中的响应将包含向另一个方向的配对 SA 的删除有效负载。有一个例外。如果一组 SA 的两端偶然独立决定关闭它们，则每个 SA 都可能发送 Delete 有效负载，并且这两个请求可能会在网络中交叉。如果节点收到已发出删除请求的 SA 的删除请求，则必须在处理请求时删除传出 SA，在处理响应时删除传入 SA。在这种情况下，响应不得包含已删除 SA 的删除有效负载，因为这会导致重复删除，并且理论上可能会删除错误的 SA。&lt;/p&gt;
&lt;p&gt;与 ESP 和 AH SA 类似，IKE SA 也通过发送信息交换来删除。删除 IKE SA 会隐式关闭根据该 IKE SA 协商的任何剩余子 SA。对删除 IKE SA 的请求的响应是空的信息响应。&lt;/p&gt;
&lt;p&gt;半闭合 ESP 或 AH 连接是异常的，具有审计功能的节点如果它们仍然存在，则可能应该审计它们的存在。请注意，此规范未指定时间段，因此由各个终结点决定等待多长时间。节点可以拒绝接受半闭合连接上的传入数据，但不得单方面关闭它们并重用 SPI。如果连接状态变得足够混乱，节点可能会关闭 IKE SA，如上所述。然后，它可以在新的 IKE SA 下干净的基础上重建所需的 SA。&lt;/p&gt;
&lt;h2 id=&#34;15-ike-sa-之外的信息性消息&#34;&gt;1.5 IKE SA 之外的信息性消息&lt;/h2&gt;
&lt;p&gt;在某些情况下，节点收到无法处理的数据包，但它可能希望将这种情况通知发送方。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 ESP 或 AH 数据包到达时带有无法识别的 SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。&lt;/li&gt;
&lt;li&gt;如果加密的 IKE 请求数据包到达端口 500 或 4500，并且具有无法识别的 IKE SPI。这可能是由于接收节点最近崩溃并丢失状态，或者由于其他一些系统故障或攻击。&lt;/li&gt;
&lt;li&gt;如果 IKE 请求数据包到达时的主版本号高于实现支持的版本号。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在第一种情况下，如果接收节点有一个活动的 IKE SA 到数据包来自的 IP 地址，它可能会在信息交换中通过该 IKE SA 发送任性数据包的INVALID_SPI通知。通知数据包含无效数据包的 SPI。此通知的接收者无法判断 SPI 是针对 AH 还是 ESP，但这并不重要，因为在许多情况下，两者的 SPI 会有所不同。如果不存在合适的 IKE SA，则节点可能会向源 IP 地址发送没有加密保护的信息性消息，如果数据包是 UDP（UDP 封装的 ESP 或 AH），则使用源 UDP 端口作为目标端口。在这种情况下，它应该只被收件人用作可能出错的提示（因为它很容易被伪造）。此消息不是信息交换的一部分，接收节点不得响应它，因为这样做可能会导致消息循环。消息构造如下：没有对此类通知的接收者有意义的 IKE SPI 值;使用零值或随机值都是可以接受的，这是第 3.1 节中禁止零 IKE 发起方 SPI 的规则的例外。发起方标志设置为 1，响应标志设置为 0，版本标志以正常方式设置;这些标志在第 3.1 节中描述。&lt;/p&gt;
&lt;p&gt;在第两种和第三种情况下，消息始终在没有加密保护的情况下发送（在 IKE SA 外部），并且包括INVALID_IKE_SPI或INVALID_MAJOR_VERSION通知（没有通知数据）。该消息是响应消息，因此它被发送到带有相同 IKE SPI 的 IP 地址和端口，并且消息 ID 和交换类型是从请求中复制的。响应标志设置为 1，版本标志以正常方式设置。&lt;/p&gt;
&lt;h1 id=&#34;可能存在的问题&#34;&gt;可能存在的问题&lt;/h1&gt;
&lt;p&gt;参考发表在27th USENIX Security Symposium (USENIX Security 18), 2018的&lt;a class=&#34;link&#34; href=&#34;https://usenix.org/conference/usenixsecurity18/presentation/felsch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Dangers of Key Reuse: Practical Attacks on IPsec IKE&lt;/a&gt;可IKEv1、v2如果重用密钥可能导致跨协议身份验证绕过，从而使攻击者能够冒充受害者主机或网络。在IKEv1模式下利用Bleichenbacher预言机，其中RSA加密的随机数用于身份验证。利用此漏洞打破了基于 RSA 加密的模式，此外还破坏了 IKEv1 和 IKEv2 中基于 RSA 签名的身份验证。此外，还存在针对基于 PSK（预共享密钥）的 IKE 模式的离线字典攻击，从而涵盖了 IKE 的所有可用身份验证机制。在思科（CVE-2018-0131）、华为（CVE2017-17305）、Clavister（CVE-2018-8753）和合勤科技（CVE-2018-9129）的IKEv1实现中找到了Bleichenbacher预言机。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis</title>
        <link>https://pillar23.github.io/p/detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/</link>
        <pubDate>Mon, 16 Oct 2023 20:34:22 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/detecting-unknown-encrypted-malicious-traffic-in-real-time-via-flow-interaction-graph-analysis/</guid>
        <description>&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;doi is &lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/10.1145/3548606.3560604&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;h2 id=&#34;问题引入&#34;&gt;问题引入&lt;/h2&gt;
&lt;p&gt;目前互联网上的流量已被广泛加密，同时流量加密总是被攻击者滥用以隐藏其恶意行为，现有的加密恶意流量检测方法受到监督，它们依赖于已知攻击（例如，标记数据集）的先验知识。&lt;/p&gt;
&lt;h2 id=&#34;提出方法&#34;&gt;提出方法&lt;/h2&gt;
&lt;p&gt;提出了HyperVision，一种基于实时无监督机器学习的恶意流量检测系统。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;能够利用基于流量模式构建的紧凑内存图来检测加密恶意流量的未知模式。该图捕获由图结构特征表示的流交互模式，而不是特定已知攻击的特征。&lt;/li&gt;
&lt;li&gt;我们开发了一种无监督图学习方法，通过分析图的连接性、稀疏性和统计特征来检测异常交互模式&lt;/li&gt;
&lt;li&gt;建立了一个信息论模型来证明图保存的信息接近理想的理论边界。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;现有方法&#34;&gt;现有方法&lt;/h2&gt;
&lt;p&gt;深度数据包检测（DPI）的传统基于签名的方法在加密有效载荷的攻击下无效，加密流量具有与良性流量相似的特征，因此也可以逃避现有的基于机器学习。特别是，现有的加密流量检测方法受到监督，即依赖于已知攻击的先验知识，并且只能检测具有已知流量模式的攻击。此外，这些方法无法检测使用和不使用加密流量构建的攻击，并且由于加密和非加密攻击流量的特征显着不同，因此无法实现通用检测&lt;/p&gt;
&lt;p&gt;简而言之，现有方法无法实现无监督检测，也无法检测具有未知模式的加密恶意流量。特别是，加密的恶意流量具有隐蔽行为，这些方法无法捕获这些行为，这些方法根据单个流的模式检测攻击。但是，检测此类攻击流量仍然是可行的，因为即使攻击的单个流与良性攻击流相似，这些攻击涉及攻击者和受害者之间具有不同流交互的多个攻击步骤与良性流交互模式不同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697463380666.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;img&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;HyperVision，这是一个实时检测系统，旨在通过分析流之间的交互模式来捕获加密恶意流量的足迹。特别是，它可以通过识别异常流交互（即不同于良性的交互模式）来检测具有未知足迹的加密恶意流。&lt;/p&gt;
&lt;p&gt;但是，构建用于实时检测的图形具有挑战性。我们不能简单地使用 IP 地址作为顶点，而传统的四元组流（源目的ip，源目的port）作为边来构建图，因为生成的密集图无法维持各种流之间的交互模式，例如，引起依赖爆炸问题 。&lt;/p&gt;
&lt;p&gt;收到流量尺寸分布的研究的启发，互联网上的大多数流都是短流，而大多数数据包与长流相关联，我们利用两种策略来记录不同大小的流，并在图中分别处理短流和长流的交互模式。&lt;/p&gt;
&lt;p&gt;我们设计了一种四步 轻量级 无监督 图学习方法，通过利用图上维护的丰富流交互信息来检测加密的恶意流量。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先，我们通过提取连通分量来分析图的连通性，并通过对高层次统计特征进行聚类来识别异常分量。通过排除良性分量，我们还显著减少了学习开销。&lt;/li&gt;
&lt;li&gt;其次，我们根据在边特征中观察到的局部邻接关系对边进行预聚类。预聚类操作显著降低了特征处理开销，并确保了实时检测。&lt;/li&gt;
&lt;li&gt;第三，我们使用Z3 SMT solver求解顶点覆盖问题来提取关键顶点，以最大程度地减少聚类的数量。&lt;/li&gt;
&lt;li&gt;最后，根据每个临界顶点的连接边进行聚类，这些边位于预聚类产生的簇的中心，从而得到指示加密恶意流量的异常边。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，为了量化HyperVision基于图的流量记录相对于现有方法的优势，我们开发了一个流量记录熵模型，这是一个基于信息论的框架，从理论上分析恶意流量检测系统的现有数据源保留的信息量。这个框架表明NetFlow [19]和Zeek [86]无法保留高保真流量信息，而HyperVision中的图捕获了接近最优的流量信息，并且图中维护的信息量接近理想化数据源的理论上界。（这么屌啊？）此外，分析结果表明，HyperVision中的图形实现了比所有现有数据源更高的信息密度（即每单位存储的流量信息量），这是准确高效检测的基础。&lt;/p&gt;
&lt;p&gt;过两天读R. Zamir, “A proof of the fisher information inequality via a data processing argument,” IEEE Trans. Inf. Theory, vol. 44, no. 3, pp. 12461250, 1998.&lt;/p&gt;
&lt;h2 id=&#34;平台和数据集&#34;&gt;平台和数据集&lt;/h2&gt;
&lt;p&gt;我们使用英特尔的数据平面开发套件 （DPDK） [37] 对 HyperVision进行原型设计。为了广泛评估原型的性能，我们重放了92个攻击数据集，其中包括在我们的虚拟私有云 （VPC）中收集的80个新数据集，其中包含 1,500 多个实例。在 VPC 中，我们收集了 48 个典型的加密恶意流量，包括 （i） 加密泛洪流量，例如泛洪目标链路 [41];（ii） 网络攻击，例如利用网络漏洞 [64];（iii） 恶意软件活动，包括连接测试、依赖项更新和下载。&lt;/p&gt;
&lt;p&gt;此外，HyperVision 的平均检测吞吐量超过 100 Gb/s，平均检测延迟为 0.83 秒。&lt;/p&gt;
&lt;h2 id=&#34;省流&#34;&gt;省流&lt;/h2&gt;
&lt;p&gt;• 我们提出了 HyperVision，这是首个使用流交互图实现对未知模式的加密恶意流量进行实时无监督检测的方法。
• 我们开发了多种算法来构建内存中的图，使我们能够准确捕获不同流之间的交互模式。
• 我们设计了一种轻量级的无监督图形学习方法，通过图形特征来检测加密流量。
• 我们开发了一个由信息论建立的理论分析框架，以展示该图形捕获了接近最优的流量交互信息。
• 我们原型化了 HyperVision，并进行了广泛的实验，使用各种真实世界的加密恶意流量来验证其准确性和效率。&lt;/p&gt;
&lt;h2 id=&#34;名词解释&#34;&gt;名词解释&lt;/h2&gt;
&lt;p&gt;连通分量：在图论中，连通分量是一个图中的一个子图，其中任意两个顶点都可以通过边相连的路径相互访问。&lt;/p&gt;
&lt;p&gt;Z3 SMT solver：3（Z3 SMT solver）是由微软研究院开发的一个高性能的SMT（Satisfiability Modulo Theories）求解器。SMT 求解器是一种自动化工具，用于解决布尔公式、一阶逻辑公式和其他数学理论的判定问题。Z3 在各种计算机科学和工程领域都有广泛的应用，包括软件验证、形式化方法、人工智能、编译器优化和硬件验证等。&lt;/p&gt;
&lt;p&gt;英特尔的数据平面开发套件 （DPDK）：旨在优化数据包处理性能。它专注于高性能网络应用程序和数据平面开发，使开发人员能够在通用服务器硬件上实现高吞吐量和低延迟的数据包处理。它通过绕过操作系统内核，并在用户空间中实现网络协议栈，从而提供极低的延迟和高吞吐量。支持多核处理器，允许并行处理大量数据包。利用支持硬件加速的网络接口卡（NIC）来进一步提高性能。DPDK 是一个开源项目，开发人员可以根据其需求进行自定义和扩展。DPDK 通常用于构建高性能网络应用程序，如网络功能虚拟化（NFV）、防火墙、负载均衡、数据包过滤和路由等。它还用于云计算、边缘计算和网络设备。&lt;/p&gt;
&lt;h1 id=&#34;hypervision&#34;&gt;HyperVision&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1698218520903.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1698218520903&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;首先HyperVision以镜像来的路由器流量作为输入，确保不会干扰流量转发。在识别加密的恶意流量后，它可以与现有的中间恶意流量防御配合，以限制检测到的流量。重点检测使用加密流量构建的主动攻击。不考虑不会为受害者带来流量的被动攻击，例如流量窃听和被动流量分析&lt;/p&gt;
&lt;p&gt;HyperVision的设计目标如下：首先，它应该能够实现通用检测，即检测使用加密或非加密流量构建的攻击，从而确保攻击无法逃避流量加密的检测。其次，它能够实现实时高速流量处理，这意味着它可以识别通过加密流量是否是恶意的，同时产生低检测延迟。第三，HyperVision 执行的检测是不受监督的，这意味着它不需要任何加密恶意流量的先验知识。&lt;/p&gt;
&lt;h2 id=&#34;图构造&#34;&gt;图构造&lt;/h2&gt;
&lt;p&gt;将流分为短流和长流，并分别记录它们的相互作用模式，以降低图的密度。&lt;/p&gt;
&lt;p&gt;使用不同的地址作为顶点，分别连接与短流和长流关联的边。聚合大量相似的短流，为一组短流构建一条边，从而减少维护流交互模式的开销。拟合长流中数据包特征的分布，构建与长流相关的边缘，从而保证了高保真记录的流交互模式，同时解决了传统方法中粗粒度流特征的问题。&lt;/p&gt;
&lt;h2 id=&#34;预处理图&#34;&gt;预处理图&lt;/h2&gt;
&lt;p&gt;通过提取连通分量来减少图的开销，并使用高级统计信息进行聚类。其中，聚类可以准确地检测出只有良性交互模式的组件，从而对这些良性组件进行过滤，减小图的规模。此外，我们进行了预聚类，并使用生成的聚类中心来表示图像中的识别的集群的边缘。（第五节详细讲）&lt;/p&gt;
&lt;h2 id=&#34;基于图的恶意流量检测&#34;&gt;基于图的恶意流量检测&lt;/h2&gt;
&lt;p&gt;通过分析图特征来实现无监督加密恶意流量检测。&lt;/p&gt;
&lt;h1 id=&#34;图构造-1&#34;&gt;图构造&lt;/h1&gt;
&lt;h2 id=&#34;流的分类&#34;&gt;流的分类&lt;/h2&gt;
&lt;p&gt;为了避免图构建过程中流之间的依赖爆炸，把流分成长流和短流，并且降低密度。下图显示了显示了2020年1月MAWI互联网流量数据集的流完成时间和流长度的分布，纵轴PDF是概率密度函数，可以看到不论是长流还是短流都在分布短时间、长长度更多。
&lt;img src=&#34;https://pillar23.github.io/images/1698331144407.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1698331144407&#34;
	
	
&gt;
利用短流合并后，图的稠密度显著下降
&lt;img src=&#34;https://pillar23.github.io/images/1698332038326.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1698332038326&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;获取每个数据包的信息，并获取其源、目标地址、端口号和每个数据包的功能，包括协议、长度和到达间隔。我们开发了一种流量分类算法来对流量进行分类（附录A中的算法1）简单来说就是维护一个哈希表，键是hash(src,dest,src_post,dest_port)，值是流的所有数据包特征的序列(协议、数据包长度、到达间隔)，然后用一个定时器TIME_NOW，每隔JUDGE_INTERVAL检查一下，如果在这个interval里流发了多个数据包，就算他是长流，否则就说他是短流）【q，这个interval怎么设置？为什么后面说ssh暴力破解都是短流？这不是应该是短期发好多包吗？】&lt;/p&gt;
&lt;h2 id=&#34;短流聚合&#34;&gt;短流聚合&lt;/h2&gt;
&lt;p&gt;我们观察到，大多数短流具有几乎相同的每个数据包的特征序列。我们设计了一种聚合短流的算法（附录A中的算法2）。当满足以下所有要求时，可以聚合一组流&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;流具有相同的源和/或目标地址（为啥不是哈希表的键值一样）&lt;/li&gt;
&lt;li&gt;流具有相同的协议类型&lt;/li&gt;
&lt;li&gt;流的数量足够大，即当短流量的数量达到阈值AGG_LINE&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们为短流构建一条边，为所有流及其四个元组保留一个特征序列（即协议、数据包长度和到达间隔）&lt;/p&gt;
&lt;h2 id=&#34;长流的特征分布拟合&#34;&gt;长流的特征分布拟合&lt;/h2&gt;
&lt;p&gt;由于长流中的特征是集中分布的，我们使用直方图来表示长流中每个数据包特征的频率分布。直方图的每个条目表示一个数据包特征的频率，从而避免保留其长的每个数据包特征序列。具体来说，我们为每个长流中的每个数据包特征序列构建直方图，然后维护一个哈希表，    键为数据包特征序列，值为直方图。我们将数据包长度和到达间隔的桶宽度分别设置为 10 字节和 1 毫秒，以在拟合精度和开销之间进行权衡。
下图显示了数据集中的长流中已用桶的数量和最大桶的大小，可以看是集中分布的，即长流中的大多数数据包具有相似的包长度和到达间隔。长度拟合平均用11个桶，每个桶平均200个数据包；到达间隔拟合平均用121个桶，每个桶平均71个数据包。
&lt;img src=&#34;https://pillar23.github.io/images/1698336076628.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1698336076628&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Frp实现隧道穿透远程桌面</title>
        <link>https://pillar23.github.io/p/frp%E5%AE%9E%E7%8E%B0%E9%9A%A7%E9%81%93%E7%A9%BF%E9%80%8F%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/</link>
        <pubDate>Mon, 16 Oct 2023 14:10:06 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/frp%E5%AE%9E%E7%8E%B0%E9%9A%A7%E9%81%93%E7%A9%BF%E9%80%8F%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/</guid>
        <description>&lt;h1 id=&#34;导语&#34;&gt;导语&lt;/h1&gt;
&lt;p&gt;果壳的校园网不给直接内网连远程桌面，感觉是因为划的子网之间不能互相通信，不知道是深澜故意的还是不小心的。todesk自然是可以，但是感觉免费的todesk画质略输一筹的同时延迟也有点小高。于是想到用frp的内网穿透来搞p2p的远程桌面。&lt;/p&gt;
&lt;h1 id=&#34;操作&#34;&gt;操作&lt;/h1&gt;
&lt;p&gt;实际上还是挺简单的，为数不多的坑是网上的教程都还是ini格式，但是在现在的版本里已经转为了toml、yaml等，而且参数好像也有些变化。&lt;/p&gt;
&lt;p&gt;去&lt;a class=&#34;link&#34; href=&#34;https://github.com/fatedier/frp/releases/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;github&lt;/a&gt;下载frp的releas&lt;/p&gt;
&lt;h2 id=&#34;服务端&#34;&gt;服务端&lt;/h2&gt;
&lt;p&gt;在你的服务器上装frps，并且配置toml文件，下载的frps.toml已经基本上写好了，基本啥也不用改，只要把最后的插件注释掉（或者你也可以把插件装了用）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#[[httpPlugins]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#name = &amp;#34;user-manager&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#addr = &amp;#34;127.0.0.1:9000&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#path = &amp;#34;/handler&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#ops = [&amp;#34;Login&amp;#34;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#[[httpPlugins]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#name = &amp;#34;port-manager&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#addr = &amp;#34;127.0.0.1:9001&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#path = &amp;#34;/handler&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;#ops = [&amp;#34;NewProxy&amp;#34;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;再改一下auth.token，这个token是你的frpc也要配置成一样的，相当于server对client的认证。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;auth&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;token&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hsijdfhsjdhf&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 这个感觉可以随便写，多复杂都行，反正你能连上你的服务器就能查&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后再改一改web界面的用户名密码端口啥的或者直接把web也注释了&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;webServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;addr&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;0.0.0.0&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;webServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;port&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;webServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;user&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;dgsdgfsdfs&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;webServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;password&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;dweqweas&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后启动时一定要用-c指定toml配置文件，否则我也不知道他默认找的哪里的配置文件&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697438389080.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1697438389080&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;客户端&#34;&gt;客户端&lt;/h2&gt;
&lt;p&gt;这里使用了xtcp的代理协议来进行p2p的内网穿透，如果想用其他方法可以参考&lt;a class=&#34;link&#34; href=&#34;https://gofrp.org/zh-cn/docs/reference/proxy/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官方文档&lt;/a&gt;（？是吗）&lt;/p&gt;
&lt;p&gt;在被控端和控制端都装上对应平台的frpc，并且配置frpc.toml&lt;/p&gt;
&lt;p&gt;配置frps的地址端口和token&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;serverAddr&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1.1.1.1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;serverPort&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;auth&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;token&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;asfggsaddasd&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 这里要和服务端配的一样&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;把下面哪些示例配置全都注释掉，然后写上下面的内容&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;proxies&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;rdesk&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;xtcp&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;localIP&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 本机&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;localPort&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3389&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 远程桌面连接&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;role&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;server&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;secretKey&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;akjndsghnkjadsfjh&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;如果你的被控端的frpc设置开了web，那你应该可以再web界面看到你的xtcp的连接&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697439827539.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1697439827539&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对了你还可以再你的frps和frpc里都指定一个user，这样你的proxy的name就会变成user.name的形式，这也就使你可以在server端配置多用户（指直接管name叫做aaa.xxx而不配置frps的user）&lt;/p&gt;
&lt;p&gt;同时在控制的机器那边也装上frpc，配置frps的地址端口和token&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;serverAddr&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1.1.1.1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;serverPort&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;auth&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;token&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;asfggsaddasd&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 这里要和服务端配的一样&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后再加上&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;visitors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;rdesk_visitor&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;xtcp&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;serverName&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;rdesk&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 这里要和上面的name一致&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;secretKey&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;akjndsghnkjadsfjh&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 这里要和上面的secretkey一致&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;bindAddr&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 本机的ip地址&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;bindPort&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后使用frpc的同时也要用-c来指定配置文件&lt;/p&gt;
&lt;p&gt;&lt;code&gt;./frpc.exe -c ./frpc.toml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;visitors是在web里看不见的，看不到不要觉得奇怪。&lt;/p&gt;
&lt;p&gt;如果在server上可以看到都连上了，直接mstsc连就行了，连127.0.0.1:8000（也就是你在visitor里设置的地址端口）（这里是把log指向了console，所以可以直接看）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pillar23.github.io/images/1697440033017.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1697440033017&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;windwos防火墙会弹，同意了就完事了&lt;/p&gt;
&lt;h2 id=&#34;使用systemd让frps挂在后台&#34;&gt;使用systemd让frps挂在后台&lt;/h2&gt;
&lt;p&gt;直接参考https://gofrp.org/zh-cn/docs/setup/systemd/&lt;/p&gt;
</description>
        </item>
        <item>
        <title>First_blog</title>
        <link>https://pillar23.github.io/p/first_blog/</link>
        <pubDate>Tue, 03 Oct 2023 17:54:20 +0800</pubDate>
        
        <guid>https://pillar23.github.io/p/first_blog/</guid>
        <description>&lt;h1 id=&#34;first-take&#34;&gt;First take&lt;/h1&gt;
&lt;p&gt;旧的博客忘了同步，导致换了电脑之后source找不到了（哭哭
于是整了个新博客儿，采用小号githubpage+hugo的解决方法
当成一个新的云笔记吧，不过其实也搞了本地md+github的方案，但是肯定没有静态博客看着爽了，整，都可以整！
顺便给旧的hexo博客指路&lt;a class=&#34;link&#34; href=&#34;https://blog.pillar.fun&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.pillar.fun&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://pillar23.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://pillar23.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Links</title>
        <link>https://pillar23.github.io/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://pillar23.github.io/links/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;links&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ts-logo-128.jpg&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>https://pillar23.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://pillar23.github.io/search/</guid>
        <description></description>
        </item>
        <item>
        <title>关于</title>
        <link>https://pillar23.github.io/%E5%85%B3%E4%BA%8E/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://pillar23.github.io/%E5%85%B3%E4%BA%8E/</guid>
        <description>&lt;p&gt;This is a test page for i18n support.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
